{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semg 数据形状为： torch.Size([26645, 6, 256])\n",
      "angle 数据形状为： torch.Size([26645, 3])\n",
      "Epoch 1, Train Loss: 325654.34644, Test Loss: 290318.56250\n",
      "Epoch 2, Train Loss: 265531.07300, Test Loss: 243791.54688\n",
      "Epoch 3, Train Loss: 224380.09595, Test Loss: 207002.84375\n",
      "Epoch 4, Train Loss: 191052.89697, Test Loss: 176799.70312\n",
      "Epoch 5, Train Loss: 163461.04541, Test Loss: 151842.39062\n",
      "Epoch 6, Train Loss: 140888.51709, Test Loss: 131101.42188\n",
      "Epoch 7, Train Loss: 121787.07764, Test Loss: 113879.53125\n",
      "Epoch 8, Train Loss: 106002.44348, Test Loss: 99544.74219\n",
      "Epoch 9, Train Loss: 92921.92938, Test Loss: 87609.17969\n",
      "Epoch 10, Train Loss: 82021.66486, Test Loss: 77666.82812\n",
      "Epoch 11, Train Loss: 72906.71930, Test Loss: 69379.32812\n",
      "Epoch 12, Train Loss: 65392.47980, Test Loss: 62497.68359\n",
      "Epoch 13, Train Loss: 59130.54102, Test Loss: 56791.44141\n",
      "Epoch 14, Train Loss: 53881.25226, Test Loss: 52072.51172\n",
      "Epoch 15, Train Loss: 49523.92825, Test Loss: 48191.97656\n",
      "Epoch 16, Train Loss: 46033.59262, Test Loss: 45036.14844\n",
      "Epoch 17, Train Loss: 43189.98596, Test Loss: 42474.09375\n",
      "Epoch 18, Train Loss: 40896.71161, Test Loss: 40423.21875\n",
      "Epoch 19, Train Loss: 38999.34985, Test Loss: 38787.48828\n",
      "Epoch 20, Train Loss: 37545.78867, Test Loss: 37508.42578\n",
      "Epoch 21, Train Loss: 36535.19144, Test Loss: 36513.77344\n",
      "Epoch 22, Train Loss: 35762.78851, Test Loss: 35756.80078\n",
      "Epoch 23, Train Loss: 34921.01895, Test Loss: 35181.75391\n",
      "Epoch 24, Train Loss: 34382.00076, Test Loss: 34758.82031\n",
      "Epoch 25, Train Loss: 34117.83667, Test Loss: 34446.91406\n",
      "Epoch 26, Train Loss: 33927.02692, Test Loss: 34226.13281\n",
      "Epoch 27, Train Loss: 33728.65701, Test Loss: 34068.44531\n",
      "Epoch 28, Train Loss: 33576.25601, Test Loss: 33961.39062\n",
      "Epoch 29, Train Loss: 33400.16702, Test Loss: 33889.26172\n",
      "Epoch 30, Train Loss: 33385.54800, Test Loss: 33840.50000\n",
      "Epoch 31, Train Loss: 33270.72015, Test Loss: 33807.82031\n",
      "Epoch 32, Train Loss: 33248.15451, Test Loss: 33787.76953\n",
      "Epoch 33, Train Loss: 33217.83710, Test Loss: 33774.94531\n",
      "Epoch 34, Train Loss: 33346.17258, Test Loss: 33767.92188\n",
      "Epoch 35, Train Loss: 33230.81604, Test Loss: 33762.61719\n",
      "Epoch 36, Train Loss: 33285.27972, Test Loss: 33760.39453\n",
      "Epoch 37, Train Loss: 33245.39224, Test Loss: 33758.69141\n",
      "Epoch 38, Train Loss: 33410.27844, Test Loss: 33757.73828\n",
      "Epoch 39, Train Loss: 33234.32117, Test Loss: 33757.14062\n",
      "Epoch 40, Train Loss: 33330.97754, Test Loss: 33756.89844\n",
      "Epoch 41, Train Loss: 33281.70081, Test Loss: 33756.95703\n",
      "Epoch 42, Train Loss: 33206.18845, Test Loss: 33756.47656\n",
      "Epoch 43, Train Loss: 33062.56094, Test Loss: 31737.50195\n",
      "Epoch 44, Train Loss: 30487.42166, Test Loss: 30525.66406\n",
      "Epoch 45, Train Loss: 29392.74838, Test Loss: 29224.50391\n",
      "Epoch 46, Train Loss: 28242.59038, Test Loss: 28044.39648\n",
      "Epoch 47, Train Loss: 27055.70856, Test Loss: 26987.41797\n",
      "Epoch 48, Train Loss: 25857.84557, Test Loss: 25540.99219\n",
      "Epoch 49, Train Loss: 24456.68086, Test Loss: 24152.55273\n",
      "Epoch 50, Train Loss: 23181.15260, Test Loss: 22906.20703\n",
      "Epoch 51, Train Loss: 21952.21729, Test Loss: 21774.88672\n",
      "Epoch 52, Train Loss: 20807.37341, Test Loss: 20646.34766\n",
      "Epoch 53, Train Loss: 19750.90645, Test Loss: 19659.75977\n",
      "Epoch 54, Train Loss: 18863.74005, Test Loss: 18769.37305\n",
      "Epoch 55, Train Loss: 17974.67957, Test Loss: 17861.85547\n",
      "Epoch 56, Train Loss: 17149.85275, Test Loss: 17101.45703\n",
      "Epoch 57, Train Loss: 16389.98753, Test Loss: 16437.76758\n",
      "Epoch 58, Train Loss: 15670.98248, Test Loss: 15607.01953\n",
      "Epoch 59, Train Loss: 14960.65269, Test Loss: 14971.66699\n",
      "Epoch 60, Train Loss: 14331.40297, Test Loss: 14323.00781\n",
      "Epoch 61, Train Loss: 13776.42218, Test Loss: 13677.81055\n",
      "Epoch 62, Train Loss: 13087.33809, Test Loss: 13040.25293\n",
      "Epoch 63, Train Loss: 12439.15448, Test Loss: 12332.81543\n",
      "Epoch 64, Train Loss: 11845.32478, Test Loss: 11941.15918\n",
      "Epoch 65, Train Loss: 11249.83377, Test Loss: 11178.86719\n",
      "Epoch 66, Train Loss: 10601.60181, Test Loss: 10597.58887\n",
      "Epoch 67, Train Loss: 10038.83509, Test Loss: 9980.15723\n",
      "Epoch 68, Train Loss: 9452.05434, Test Loss: 9502.96582\n",
      "Epoch 69, Train Loss: 8789.66350, Test Loss: 8767.89844\n",
      "Epoch 70, Train Loss: 8187.70972, Test Loss: 8181.79785\n",
      "Epoch 71, Train Loss: 7591.98792, Test Loss: 7539.29102\n",
      "Epoch 72, Train Loss: 7016.55874, Test Loss: 7012.86182\n",
      "Epoch 73, Train Loss: 6539.20493, Test Loss: 6523.53418\n",
      "Epoch 74, Train Loss: 6081.34800, Test Loss: 6098.02881\n",
      "Epoch 75, Train Loss: 5666.32857, Test Loss: 5731.44482\n",
      "Epoch 76, Train Loss: 5288.03222, Test Loss: 5290.15723\n",
      "Epoch 77, Train Loss: 4965.81294, Test Loss: 4985.19922\n",
      "Epoch 78, Train Loss: 4546.10302, Test Loss: 4613.73145\n",
      "Epoch 79, Train Loss: 4274.40620, Test Loss: 4353.67578\n",
      "Epoch 80, Train Loss: 3998.36131, Test Loss: 4070.96924\n",
      "Epoch 81, Train Loss: 3731.52871, Test Loss: 3805.56592\n",
      "Epoch 82, Train Loss: 3505.74972, Test Loss: 3573.37964\n",
      "Epoch 83, Train Loss: 3283.11954, Test Loss: 3423.66016\n",
      "Epoch 84, Train Loss: 3070.80661, Test Loss: 3188.97852\n",
      "Epoch 85, Train Loss: 2940.38590, Test Loss: 3100.66748\n",
      "Epoch 86, Train Loss: 2778.83954, Test Loss: 2897.80713\n",
      "Epoch 87, Train Loss: 2601.89716, Test Loss: 2660.28613\n",
      "Epoch 88, Train Loss: 2434.21114, Test Loss: 2509.42139\n",
      "Epoch 89, Train Loss: 2296.79971, Test Loss: 2421.35303\n",
      "Epoch 90, Train Loss: 2144.93883, Test Loss: 2262.33496\n",
      "Epoch 91, Train Loss: 2041.76917, Test Loss: 2156.39990\n",
      "Epoch 92, Train Loss: 1916.03010, Test Loss: 2033.02454\n",
      "Epoch 93, Train Loss: 1808.44592, Test Loss: 1879.37854\n",
      "Epoch 94, Train Loss: 1712.74877, Test Loss: 1769.62341\n",
      "Epoch 95, Train Loss: 1597.34369, Test Loss: 1642.18188\n",
      "Epoch 96, Train Loss: 1534.77053, Test Loss: 1567.31995\n",
      "Epoch 97, Train Loss: 1527.47834, Test Loss: 1531.91113\n",
      "Epoch 98, Train Loss: 1370.29045, Test Loss: 1397.49512\n",
      "Epoch 99, Train Loss: 1282.16291, Test Loss: 1328.47302\n",
      "Epoch 100, Train Loss: 1210.03956, Test Loss: 1251.44141\n",
      "Epoch 101, Train Loss: 1136.36420, Test Loss: 1179.41504\n",
      "Epoch 102, Train Loss: 1038.53357, Test Loss: 1074.32129\n",
      "Epoch 103, Train Loss: 993.70726, Test Loss: 1116.78296\n",
      "Epoch 104, Train Loss: 963.62072, Test Loss: 994.60547\n",
      "Epoch 105, Train Loss: 915.61217, Test Loss: 951.21692\n",
      "Epoch 106, Train Loss: 884.89690, Test Loss: 914.95648\n",
      "Epoch 107, Train Loss: 813.33050, Test Loss: 829.15698\n",
      "Epoch 108, Train Loss: 792.06273, Test Loss: 821.91931\n",
      "Epoch 109, Train Loss: 708.88149, Test Loss: 758.16199\n",
      "Epoch 110, Train Loss: 677.64018, Test Loss: 727.08838\n",
      "Epoch 111, Train Loss: 651.89977, Test Loss: 716.94958\n",
      "Epoch 112, Train Loss: 751.83677, Test Loss: 674.01019\n",
      "Epoch 113, Train Loss: 588.73102, Test Loss: 721.39575\n",
      "Epoch 114, Train Loss: 578.13140, Test Loss: 577.14465\n",
      "Epoch 115, Train Loss: 531.60635, Test Loss: 573.72559\n",
      "Epoch 116, Train Loss: 508.70692, Test Loss: 596.48151\n",
      "Epoch 117, Train Loss: 497.46474, Test Loss: 552.18335\n",
      "Epoch 118, Train Loss: 483.76075, Test Loss: 588.51123\n",
      "Epoch 119, Train Loss: 460.52934, Test Loss: 511.73157\n",
      "Epoch 120, Train Loss: 431.98659, Test Loss: 442.09564\n",
      "Epoch 121, Train Loss: 421.45711, Test Loss: 624.32983\n",
      "Epoch 122, Train Loss: 482.78218, Test Loss: 453.67816\n",
      "Epoch 123, Train Loss: 375.57020, Test Loss: 388.66510\n",
      "Epoch 124, Train Loss: 364.14006, Test Loss: 386.24603\n",
      "Epoch 125, Train Loss: 344.44107, Test Loss: 357.04074\n",
      "Epoch 126, Train Loss: 334.71258, Test Loss: 356.27686\n",
      "Epoch 127, Train Loss: 331.65936, Test Loss: 333.59708\n",
      "Epoch 128, Train Loss: 314.66800, Test Loss: 333.03207\n",
      "Epoch 129, Train Loss: 295.12961, Test Loss: 411.09988\n",
      "Epoch 130, Train Loss: 331.27658, Test Loss: 329.82745\n",
      "Epoch 131, Train Loss: 291.95141, Test Loss: 300.18561\n",
      "Epoch 132, Train Loss: 260.85941, Test Loss: 277.38980\n",
      "Epoch 133, Train Loss: 262.00300, Test Loss: 324.62177\n",
      "Epoch 134, Train Loss: 283.25848, Test Loss: 302.16614\n",
      "Epoch 135, Train Loss: 244.40632, Test Loss: 264.73325\n",
      "Epoch 136, Train Loss: 223.70316, Test Loss: 245.46756\n",
      "Epoch 137, Train Loss: 237.14028, Test Loss: 284.02527\n",
      "Epoch 138, Train Loss: 273.41801, Test Loss: 259.41959\n",
      "Epoch 139, Train Loss: 224.25667, Test Loss: 228.27026\n",
      "Epoch 140, Train Loss: 227.62720, Test Loss: 228.65863\n",
      "Epoch 141, Train Loss: 199.92163, Test Loss: 217.35950\n",
      "Epoch 142, Train Loss: 196.56188, Test Loss: 224.23596\n",
      "Epoch 143, Train Loss: 231.11228, Test Loss: 203.52127\n",
      "Epoch 144, Train Loss: 174.18008, Test Loss: 195.41699\n",
      "Epoch 145, Train Loss: 184.38861, Test Loss: 232.32692\n",
      "Epoch 146, Train Loss: 195.66256, Test Loss: 197.24193\n",
      "Epoch 147, Train Loss: 182.30077, Test Loss: 186.72136\n",
      "Epoch 148, Train Loss: 177.90479, Test Loss: 177.02521\n",
      "Epoch 149, Train Loss: 175.32412, Test Loss: 213.82780\n",
      "Epoch 150, Train Loss: 187.67548, Test Loss: 156.09651\n",
      "Epoch 151, Train Loss: 160.33058, Test Loss: 154.52798\n",
      "Epoch 152, Train Loss: 146.87655, Test Loss: 223.46669\n",
      "Epoch 153, Train Loss: 169.29120, Test Loss: 161.11581\n",
      "Epoch 154, Train Loss: 142.15676, Test Loss: 164.94858\n",
      "Epoch 155, Train Loss: 144.63466, Test Loss: 155.51961\n",
      "Epoch 156, Train Loss: 147.36142, Test Loss: 159.13026\n",
      "Epoch 157, Train Loss: 150.27968, Test Loss: 149.54015\n",
      "Epoch 158, Train Loss: 138.21685, Test Loss: 206.69159\n",
      "Epoch 159, Train Loss: 146.37946, Test Loss: 136.95476\n",
      "Epoch 160, Train Loss: 130.58337, Test Loss: 144.01363\n",
      "Epoch 161, Train Loss: 134.77249, Test Loss: 131.66098\n",
      "Epoch 162, Train Loss: 119.07844, Test Loss: 138.85550\n",
      "Epoch 163, Train Loss: 132.09463, Test Loss: 144.05954\n",
      "Epoch 164, Train Loss: 133.62965, Test Loss: 143.41904\n",
      "Epoch 165, Train Loss: 119.50230, Test Loss: 144.25668\n",
      "Epoch 166, Train Loss: 137.51801, Test Loss: 172.25298\n",
      "Epoch 167, Train Loss: 122.61901, Test Loss: 120.77355\n",
      "Epoch 168, Train Loss: 113.23853, Test Loss: 124.87617\n",
      "Epoch 169, Train Loss: 116.11243, Test Loss: 147.52551\n",
      "Epoch 170, Train Loss: 153.68675, Test Loss: 250.07518\n",
      "Epoch 171, Train Loss: 188.45419, Test Loss: 170.98399\n",
      "Epoch 172, Train Loss: 131.30161, Test Loss: 124.16302\n",
      "Epoch 173, Train Loss: 96.81858, Test Loss: 126.37412\n",
      "Epoch 174, Train Loss: 101.43297, Test Loss: 100.61565\n",
      "Epoch 175, Train Loss: 91.96585, Test Loss: 101.70789\n",
      "Epoch 176, Train Loss: 104.54704, Test Loss: 107.43195\n",
      "Epoch 177, Train Loss: 96.78473, Test Loss: 102.64703\n",
      "Epoch 178, Train Loss: 89.90119, Test Loss: 94.10949\n",
      "Epoch 179, Train Loss: 93.66210, Test Loss: 112.81952\n",
      "Epoch 180, Train Loss: 92.46321, Test Loss: 104.28430\n",
      "Epoch 181, Train Loss: 96.39793, Test Loss: 98.39680\n",
      "Epoch 182, Train Loss: 139.50024, Test Loss: 183.91481\n",
      "Epoch 183, Train Loss: 126.90240, Test Loss: 104.47991\n",
      "Epoch 184, Train Loss: 86.11559, Test Loss: 89.62878\n",
      "Epoch 185, Train Loss: 82.27125, Test Loss: 116.68671\n",
      "Epoch 186, Train Loss: 109.20063, Test Loss: 95.59347\n",
      "Epoch 187, Train Loss: 80.36395, Test Loss: 85.37117\n",
      "Epoch 188, Train Loss: 77.10160, Test Loss: 88.35736\n",
      "Epoch 189, Train Loss: 80.15476, Test Loss: 106.72925\n",
      "Epoch 190, Train Loss: 98.58723, Test Loss: 115.87003\n",
      "Epoch 191, Train Loss: 80.31026, Test Loss: 100.28526\n",
      "Epoch 192, Train Loss: 85.28622, Test Loss: 126.36244\n",
      "Epoch 193, Train Loss: 97.68573, Test Loss: 122.26783\n",
      "Epoch 194, Train Loss: 93.13768, Test Loss: 79.36636\n",
      "Epoch 195, Train Loss: 82.23336, Test Loss: 97.41842\n",
      "Epoch 196, Train Loss: 80.26035, Test Loss: 89.53868\n",
      "Epoch 197, Train Loss: 80.86090, Test Loss: 86.13257\n",
      "Epoch 198, Train Loss: 80.91770, Test Loss: 94.22980\n",
      "Epoch 199, Train Loss: 68.26323, Test Loss: 75.84004\n",
      "Epoch 200, Train Loss: 79.25971, Test Loss: 87.36887\n",
      "Epoch 201, Train Loss: 68.45934, Test Loss: 76.89969\n",
      "Epoch 202, Train Loss: 63.74926, Test Loss: 86.34587\n",
      "Epoch 203, Train Loss: 86.77165, Test Loss: 88.23910\n",
      "Epoch 204, Train Loss: 80.41762, Test Loss: 75.23355\n",
      "Epoch 205, Train Loss: 61.92959, Test Loss: 70.96184\n",
      "Epoch 206, Train Loss: 98.17703, Test Loss: 126.47550\n",
      "Epoch 207, Train Loss: 82.75730, Test Loss: 74.66291\n",
      "Epoch 208, Train Loss: 66.88527, Test Loss: 91.69519\n",
      "Epoch 209, Train Loss: 65.98328, Test Loss: 67.04187\n",
      "Epoch 210, Train Loss: 60.27861, Test Loss: 77.40707\n",
      "Epoch 211, Train Loss: 66.62495, Test Loss: 87.48899\n",
      "Epoch 212, Train Loss: 78.73436, Test Loss: 98.36475\n",
      "Epoch 213, Train Loss: 91.35380, Test Loss: 80.18245\n",
      "Epoch 214, Train Loss: 79.08752, Test Loss: 88.16283\n",
      "Epoch 215, Train Loss: 65.68149, Test Loss: 65.23981\n",
      "Epoch 216, Train Loss: 58.66397, Test Loss: 71.09086\n",
      "Epoch 217, Train Loss: 78.64337, Test Loss: 101.06661\n",
      "Epoch 218, Train Loss: 78.52670, Test Loss: 83.33915\n",
      "Epoch 219, Train Loss: 59.16469, Test Loss: 61.10375\n",
      "Epoch 220, Train Loss: 56.37221, Test Loss: 70.39825\n",
      "Epoch 221, Train Loss: 90.82184, Test Loss: 96.83761\n",
      "Epoch 222, Train Loss: 81.50284, Test Loss: 96.36507\n",
      "Epoch 223, Train Loss: 85.32741, Test Loss: 83.87936\n",
      "Epoch 224, Train Loss: 57.87423, Test Loss: 59.88610\n",
      "Epoch 225, Train Loss: 49.77871, Test Loss: 55.30626\n",
      "Epoch 226, Train Loss: 45.69954, Test Loss: 55.50003\n",
      "Epoch 227, Train Loss: 54.19244, Test Loss: 82.30914\n",
      "Epoch 228, Train Loss: 51.77092, Test Loss: 87.50913\n",
      "Epoch 229, Train Loss: 57.68166, Test Loss: 112.47863\n",
      "Epoch 230, Train Loss: 92.07407, Test Loss: 81.90533\n",
      "Epoch 231, Train Loss: 63.13967, Test Loss: 55.98524\n",
      "Epoch 232, Train Loss: 50.61167, Test Loss: 79.36748\n",
      "Epoch 233, Train Loss: 54.21051, Test Loss: 60.74601\n",
      "Epoch 234, Train Loss: 54.41904, Test Loss: 71.59735\n",
      "Epoch 235, Train Loss: 58.36096, Test Loss: 68.02508\n",
      "Epoch 236, Train Loss: 59.24964, Test Loss: 58.00679\n",
      "Epoch 237, Train Loss: 50.49472, Test Loss: 65.69650\n",
      "Epoch 238, Train Loss: 66.35028, Test Loss: 64.46062\n",
      "Epoch 239, Train Loss: 54.20265, Test Loss: 60.87611\n",
      "Epoch 240, Train Loss: 52.09621, Test Loss: 53.10289\n",
      "Epoch 241, Train Loss: 44.31534, Test Loss: 56.31929\n",
      "Epoch 242, Train Loss: 51.51635, Test Loss: 63.47561\n",
      "Epoch 243, Train Loss: 54.40718, Test Loss: 71.58844\n",
      "Epoch 244, Train Loss: 52.17265, Test Loss: 58.18529\n",
      "Epoch 245, Train Loss: 42.28753, Test Loss: 44.39670\n",
      "Epoch 246, Train Loss: 43.44706, Test Loss: 51.74926\n",
      "Epoch 247, Train Loss: 59.16859, Test Loss: 92.54276\n",
      "Epoch 248, Train Loss: 128.16509, Test Loss: 117.06310\n",
      "Epoch 249, Train Loss: 79.27458, Test Loss: 86.98486\n",
      "Epoch 250, Train Loss: 62.47233, Test Loss: 56.57801\n",
      "Epoch 251, Train Loss: 43.30343, Test Loss: 55.65129\n",
      "Epoch 252, Train Loss: 48.06094, Test Loss: 55.23809\n",
      "Epoch 253, Train Loss: 45.27606, Test Loss: 71.13300\n",
      "Epoch 254, Train Loss: 47.60247, Test Loss: 67.60874\n",
      "Epoch 255, Train Loss: 45.05464, Test Loss: 48.09091\n",
      "Epoch 256, Train Loss: 41.38066, Test Loss: 45.45486\n",
      "Epoch 257, Train Loss: 40.47699, Test Loss: 47.96044\n",
      "Epoch 258, Train Loss: 46.97076, Test Loss: 81.68879\n",
      "Epoch 259, Train Loss: 55.67084, Test Loss: 59.38344\n",
      "Epoch 260, Train Loss: 41.95874, Test Loss: 54.46885\n",
      "Epoch 261, Train Loss: 51.75315, Test Loss: 65.47519\n",
      "Epoch 262, Train Loss: 60.36223, Test Loss: 64.25916\n",
      "Epoch 263, Train Loss: 43.81701, Test Loss: 45.14058\n",
      "Epoch 264, Train Loss: 39.41315, Test Loss: 51.24583\n",
      "Epoch 265, Train Loss: 49.47526, Test Loss: 48.19601\n",
      "Epoch 266, Train Loss: 61.02422, Test Loss: 121.69941\n",
      "Epoch 267, Train Loss: 52.64561, Test Loss: 42.36249\n",
      "Epoch 268, Train Loss: 37.92658, Test Loss: 45.90161\n",
      "Epoch 269, Train Loss: 41.47978, Test Loss: 44.71735\n",
      "Epoch 270, Train Loss: 49.20992, Test Loss: 64.64773\n",
      "Epoch 271, Train Loss: 45.50454, Test Loss: 71.81691\n",
      "Epoch 272, Train Loss: 65.80726, Test Loss: 101.41814\n",
      "Epoch 273, Train Loss: 94.68430, Test Loss: 102.05896\n",
      "Epoch 274, Train Loss: 84.01812, Test Loss: 69.03117\n",
      "Epoch 275, Train Loss: 53.66900, Test Loss: 38.16421\n",
      "Epoch 276, Train Loss: 31.75827, Test Loss: 38.46436\n",
      "Epoch 277, Train Loss: 33.65093, Test Loss: 40.50719\n",
      "Epoch 278, Train Loss: 32.76838, Test Loss: 40.95644\n",
      "Epoch 279, Train Loss: 31.56118, Test Loss: 35.39592\n",
      "Epoch 280, Train Loss: 30.56215, Test Loss: 37.79307\n",
      "Epoch 281, Train Loss: 31.43931, Test Loss: 44.08211\n",
      "Epoch 282, Train Loss: 43.39734, Test Loss: 52.87490\n",
      "Epoch 283, Train Loss: 40.31971, Test Loss: 46.96761\n",
      "Epoch 284, Train Loss: 41.28655, Test Loss: 44.48704\n",
      "Epoch 285, Train Loss: 33.72849, Test Loss: 37.39990\n",
      "Epoch 286, Train Loss: 30.15221, Test Loss: 38.97112\n",
      "Epoch 287, Train Loss: 30.93006, Test Loss: 40.72613\n",
      "Epoch 288, Train Loss: 44.50413, Test Loss: 59.83012\n",
      "Epoch 289, Train Loss: 57.70042, Test Loss: 51.21096\n",
      "Epoch 290, Train Loss: 44.53679, Test Loss: 44.67456\n",
      "Epoch 291, Train Loss: 40.83879, Test Loss: 42.32440\n",
      "Epoch 292, Train Loss: 33.30042, Test Loss: 39.87405\n",
      "Epoch 293, Train Loss: 42.75561, Test Loss: 63.51600\n",
      "Epoch 294, Train Loss: 49.37113, Test Loss: 57.18623\n",
      "Epoch 295, Train Loss: 37.26008, Test Loss: 36.35670\n",
      "Epoch 296, Train Loss: 33.50758, Test Loss: 53.44832\n",
      "Epoch 297, Train Loss: 32.91262, Test Loss: 37.20134\n",
      "Epoch 298, Train Loss: 33.61721, Test Loss: 42.63924\n",
      "Epoch 299, Train Loss: 53.72916, Test Loss: 112.55650\n",
      "Epoch 300, Train Loss: 63.38155, Test Loss: 68.76419\n",
      "Epoch 301, Train Loss: 35.37880, Test Loss: 30.90141\n",
      "Epoch 302, Train Loss: 30.46740, Test Loss: 35.95364\n",
      "Epoch 303, Train Loss: 36.78823, Test Loss: 46.93624\n",
      "Epoch 304, Train Loss: 47.41025, Test Loss: 62.27331\n",
      "Epoch 305, Train Loss: 35.03102, Test Loss: 36.30780\n",
      "Epoch 306, Train Loss: 31.66380, Test Loss: 37.44072\n",
      "Epoch 307, Train Loss: 29.82637, Test Loss: 36.69709\n",
      "Epoch 308, Train Loss: 28.85464, Test Loss: 40.38978\n",
      "Epoch 309, Train Loss: 31.08592, Test Loss: 45.44695\n",
      "Epoch 310, Train Loss: 34.89342, Test Loss: 31.89305\n",
      "Epoch 311, Train Loss: 30.47979, Test Loss: 46.89661\n",
      "Epoch 312, Train Loss: 44.27695, Test Loss: 50.21387\n",
      "Epoch 313, Train Loss: 97.57911, Test Loss: 125.54091\n",
      "Epoch 314, Train Loss: 52.54966, Test Loss: 42.09341\n",
      "Epoch 315, Train Loss: 36.02703, Test Loss: 38.06139\n",
      "Epoch 316, Train Loss: 30.71302, Test Loss: 32.94652\n",
      "Epoch 317, Train Loss: 24.89866, Test Loss: 30.54725\n",
      "Epoch 318, Train Loss: 25.94892, Test Loss: 31.31455\n",
      "Epoch 319, Train Loss: 30.08049, Test Loss: 39.28310\n",
      "Epoch 320, Train Loss: 36.19529, Test Loss: 53.43912\n",
      "Epoch 321, Train Loss: 44.69820, Test Loss: 44.83582\n",
      "Epoch 322, Train Loss: 40.53185, Test Loss: 56.87602\n",
      "Epoch 323, Train Loss: 45.16342, Test Loss: 56.75204\n",
      "Epoch 324, Train Loss: 41.17419, Test Loss: 31.16634\n",
      "Epoch 325, Train Loss: 24.57265, Test Loss: 30.25278\n",
      "Epoch 326, Train Loss: 29.14431, Test Loss: 35.53645\n",
      "Epoch 327, Train Loss: 31.40868, Test Loss: 44.52624\n",
      "Epoch 328, Train Loss: 47.05775, Test Loss: 49.55431\n",
      "Epoch 329, Train Loss: 33.55339, Test Loss: 34.98423\n",
      "Epoch 330, Train Loss: 25.58685, Test Loss: 34.21477\n",
      "Epoch 331, Train Loss: 25.94901, Test Loss: 34.86188\n",
      "Epoch 332, Train Loss: 33.76483, Test Loss: 51.60007\n",
      "Epoch 333, Train Loss: 31.37789, Test Loss: 52.14557\n",
      "Epoch 334, Train Loss: 37.82364, Test Loss: 54.75441\n",
      "Epoch 335, Train Loss: 43.28886, Test Loss: 68.04145\n",
      "Epoch 336, Train Loss: 41.86845, Test Loss: 41.53331\n",
      "Epoch 337, Train Loss: 28.65066, Test Loss: 39.31018\n",
      "Epoch 338, Train Loss: 46.47690, Test Loss: 64.05276\n",
      "Epoch 339, Train Loss: 41.94296, Test Loss: 65.62824\n",
      "Epoch 340, Train Loss: 57.14688, Test Loss: 72.29053\n",
      "Epoch 341, Train Loss: 51.18557, Test Loss: 37.29585\n",
      "Epoch 342, Train Loss: 25.69224, Test Loss: 33.92026\n",
      "Epoch 343, Train Loss: 25.46121, Test Loss: 31.06581\n",
      "Epoch 344, Train Loss: 24.66350, Test Loss: 38.28478\n",
      "Epoch 345, Train Loss: 32.41740, Test Loss: 41.42250\n",
      "Epoch 346, Train Loss: 28.43149, Test Loss: 35.01379\n",
      "Epoch 347, Train Loss: 28.82106, Test Loss: 30.01987\n",
      "Epoch 348, Train Loss: 22.20860, Test Loss: 31.54098\n",
      "Epoch 349, Train Loss: 22.98623, Test Loss: 23.51417\n",
      "Epoch 350, Train Loss: 25.12835, Test Loss: 35.95297\n",
      "Epoch 351, Train Loss: 33.72351, Test Loss: 44.69381\n",
      "Epoch 352, Train Loss: 38.48033, Test Loss: 31.34930\n",
      "Epoch 353, Train Loss: 32.79291, Test Loss: 51.47675\n",
      "Epoch 354, Train Loss: 33.33456, Test Loss: 33.02359\n",
      "Epoch 355, Train Loss: 25.20567, Test Loss: 26.88621\n",
      "Epoch 356, Train Loss: 23.90913, Test Loss: 35.20337\n",
      "Epoch 357, Train Loss: 23.21252, Test Loss: 31.61883\n",
      "Epoch 358, Train Loss: 24.62620, Test Loss: 26.22785\n",
      "Epoch 359, Train Loss: 45.62090, Test Loss: 115.32697\n",
      "Epoch 360, Train Loss: 63.18478, Test Loss: 30.14206\n",
      "Epoch 361, Train Loss: 26.63221, Test Loss: 34.40726\n",
      "Epoch 362, Train Loss: 27.79332, Test Loss: 28.82843\n",
      "Epoch 363, Train Loss: 20.97748, Test Loss: 22.30694\n",
      "Epoch 364, Train Loss: 24.32388, Test Loss: 32.40569\n",
      "Epoch 365, Train Loss: 27.51250, Test Loss: 28.13837\n",
      "Epoch 366, Train Loss: 24.15613, Test Loss: 28.99080\n",
      "Epoch 367, Train Loss: 33.01422, Test Loss: 40.78112\n",
      "Epoch 368, Train Loss: 42.02749, Test Loss: 51.73299\n",
      "Epoch 369, Train Loss: 44.84786, Test Loss: 66.07555\n",
      "Epoch 370, Train Loss: 52.39441, Test Loss: 84.46558\n",
      "Epoch 371, Train Loss: 49.01104, Test Loss: 28.39955\n",
      "Epoch 372, Train Loss: 19.85067, Test Loss: 28.86237\n",
      "Epoch 373, Train Loss: 23.89514, Test Loss: 27.61200\n",
      "Epoch 374, Train Loss: 18.40266, Test Loss: 23.75521\n",
      "Epoch 375, Train Loss: 20.76899, Test Loss: 30.69463\n",
      "Epoch 376, Train Loss: 27.02955, Test Loss: 30.47194\n",
      "Epoch 377, Train Loss: 21.89317, Test Loss: 24.19100\n",
      "Epoch 378, Train Loss: 23.57014, Test Loss: 34.51440\n",
      "Epoch 379, Train Loss: 39.15451, Test Loss: 57.60949\n",
      "Epoch 380, Train Loss: 40.44891, Test Loss: 45.62845\n",
      "Epoch 381, Train Loss: 29.24293, Test Loss: 24.30106\n",
      "Epoch 382, Train Loss: 24.30173, Test Loss: 37.12604\n",
      "Epoch 383, Train Loss: 26.09861, Test Loss: 26.16508\n",
      "Epoch 384, Train Loss: 18.65057, Test Loss: 21.00374\n",
      "Epoch 385, Train Loss: 20.03033, Test Loss: 29.04694\n",
      "Epoch 386, Train Loss: 27.83780, Test Loss: 29.34585\n",
      "Epoch 387, Train Loss: 27.54762, Test Loss: 27.16948\n",
      "Epoch 388, Train Loss: 29.43459, Test Loss: 52.98490\n",
      "Epoch 389, Train Loss: 27.86526, Test Loss: 23.70164\n",
      "Epoch 390, Train Loss: 20.48978, Test Loss: 35.64088\n",
      "Epoch 391, Train Loss: 30.77429, Test Loss: 25.55196\n",
      "Epoch 392, Train Loss: 29.01131, Test Loss: 30.26249\n",
      "Epoch 393, Train Loss: 36.10144, Test Loss: 52.97661\n",
      "Epoch 394, Train Loss: 33.29599, Test Loss: 25.87481\n",
      "Epoch 395, Train Loss: 21.28167, Test Loss: 22.83231\n",
      "Epoch 396, Train Loss: 22.68674, Test Loss: 27.72875\n",
      "Epoch 397, Train Loss: 45.90486, Test Loss: 149.39046\n",
      "Epoch 398, Train Loss: 57.12058, Test Loss: 24.56071\n",
      "Epoch 399, Train Loss: 19.85289, Test Loss: 22.80387\n",
      "Epoch 400, Train Loss: 18.74467, Test Loss: 19.80761\n",
      "Epoch 401, Train Loss: 18.29992, Test Loss: 19.85890\n",
      "Epoch 402, Train Loss: 16.53550, Test Loss: 21.01970\n",
      "Epoch 403, Train Loss: 20.06601, Test Loss: 27.20944\n",
      "Epoch 404, Train Loss: 20.59030, Test Loss: 22.50211\n",
      "Epoch 405, Train Loss: 21.46502, Test Loss: 45.34387\n",
      "Epoch 406, Train Loss: 40.74251, Test Loss: 44.19585\n",
      "Epoch 407, Train Loss: 27.09263, Test Loss: 23.00106\n",
      "Epoch 408, Train Loss: 20.63867, Test Loss: 19.08835\n",
      "Epoch 409, Train Loss: 17.79808, Test Loss: 27.11185\n",
      "Epoch 410, Train Loss: 27.85592, Test Loss: 32.49741\n",
      "Epoch 411, Train Loss: 42.88531, Test Loss: 55.43510\n",
      "Epoch 412, Train Loss: 40.03679, Test Loss: 48.01051\n",
      "Epoch 413, Train Loss: 33.04092, Test Loss: 26.55516\n",
      "Epoch 414, Train Loss: 18.91463, Test Loss: 22.26939\n",
      "Epoch 415, Train Loss: 15.21837, Test Loss: 19.31870\n",
      "Epoch 416, Train Loss: 15.92281, Test Loss: 20.12418\n",
      "Epoch 417, Train Loss: 22.27670, Test Loss: 20.97820\n",
      "Epoch 418, Train Loss: 38.49301, Test Loss: 69.44450\n",
      "Epoch 419, Train Loss: 53.70368, Test Loss: 34.26391\n",
      "Epoch 420, Train Loss: 30.42904, Test Loss: 36.65211\n",
      "Epoch 421, Train Loss: 21.28835, Test Loss: 24.05378\n",
      "Epoch 422, Train Loss: 46.89100, Test Loss: 112.07610\n",
      "Epoch 423, Train Loss: 34.52726, Test Loss: 18.33909\n",
      "Epoch 424, Train Loss: 15.25202, Test Loss: 25.65896\n",
      "Epoch 425, Train Loss: 16.17179, Test Loss: 18.59571\n",
      "Epoch 426, Train Loss: 14.24197, Test Loss: 20.44587\n",
      "Epoch 427, Train Loss: 18.71265, Test Loss: 19.18731\n",
      "Epoch 428, Train Loss: 14.62478, Test Loss: 22.26117\n",
      "Epoch 429, Train Loss: 19.01805, Test Loss: 18.83011\n",
      "Epoch 430, Train Loss: 18.61243, Test Loss: 36.46651\n",
      "Epoch 431, Train Loss: 30.90971, Test Loss: 30.20265\n",
      "Epoch 432, Train Loss: 19.27038, Test Loss: 21.64088\n",
      "Epoch 433, Train Loss: 20.15891, Test Loss: 27.61559\n",
      "Epoch 434, Train Loss: 19.60114, Test Loss: 20.60908\n",
      "Epoch 435, Train Loss: 27.03454, Test Loss: 39.76927\n",
      "Epoch 436, Train Loss: 48.22433, Test Loss: 81.14499\n",
      "Epoch 437, Train Loss: 56.14781, Test Loss: 37.02509\n",
      "Epoch 438, Train Loss: 19.91470, Test Loss: 21.08010\n",
      "Epoch 439, Train Loss: 13.96573, Test Loss: 20.00802\n",
      "Epoch 440, Train Loss: 15.64565, Test Loss: 26.21813\n",
      "Epoch 441, Train Loss: 21.97335, Test Loss: 23.60321\n",
      "Epoch 442, Train Loss: 19.25371, Test Loss: 22.51967\n",
      "Epoch 443, Train Loss: 16.18296, Test Loss: 20.73522\n",
      "Epoch 444, Train Loss: 18.27706, Test Loss: 18.41196\n",
      "Epoch 445, Train Loss: 19.64416, Test Loss: 22.11845\n",
      "Epoch 446, Train Loss: 23.73143, Test Loss: 36.74763\n",
      "Epoch 447, Train Loss: 31.46114, Test Loss: 38.94423\n",
      "Epoch 448, Train Loss: 27.32090, Test Loss: 31.83877\n",
      "Epoch 449, Train Loss: 25.89866, Test Loss: 23.05779\n",
      "Epoch 450, Train Loss: 22.69009, Test Loss: 29.29923\n",
      "Epoch 451, Train Loss: 36.30777, Test Loss: 26.23315\n",
      "Epoch 452, Train Loss: 21.32428, Test Loss: 30.13943\n",
      "Epoch 453, Train Loss: 19.29719, Test Loss: 23.30584\n",
      "Epoch 454, Train Loss: 21.38190, Test Loss: 26.04539\n",
      "Epoch 455, Train Loss: 25.26204, Test Loss: 33.59246\n",
      "Epoch 456, Train Loss: 24.51041, Test Loss: 20.54351\n",
      "Epoch 457, Train Loss: 22.93207, Test Loss: 27.55627\n",
      "Epoch 458, Train Loss: 17.24674, Test Loss: 17.68840\n",
      "Epoch 459, Train Loss: 14.53483, Test Loss: 16.85279\n",
      "Epoch 460, Train Loss: 14.24696, Test Loss: 18.75140\n",
      "Epoch 461, Train Loss: 15.99459, Test Loss: 18.97205\n",
      "Epoch 462, Train Loss: 27.27514, Test Loss: 32.25742\n",
      "Epoch 463, Train Loss: 30.50846, Test Loss: 30.83104\n",
      "Epoch 464, Train Loss: 28.34645, Test Loss: 23.82050\n",
      "Epoch 465, Train Loss: 23.57679, Test Loss: 24.36062\n",
      "Epoch 466, Train Loss: 26.05816, Test Loss: 21.97979\n",
      "Epoch 467, Train Loss: 16.61528, Test Loss: 17.96122\n",
      "Epoch 468, Train Loss: 14.36175, Test Loss: 19.47257\n",
      "Epoch 469, Train Loss: 18.43959, Test Loss: 27.04780\n",
      "Epoch 470, Train Loss: 31.99802, Test Loss: 117.52890\n",
      "Epoch 471, Train Loss: 144.82754, Test Loss: 219.67422\n",
      "Epoch 472, Train Loss: 62.93202, Test Loss: 22.48300\n",
      "Epoch 473, Train Loss: 14.67815, Test Loss: 18.00711\n",
      "Epoch 474, Train Loss: 11.74578, Test Loss: 14.07506\n",
      "Epoch 475, Train Loss: 10.89229, Test Loss: 13.69577\n",
      "Epoch 476, Train Loss: 11.33717, Test Loss: 16.38097\n",
      "Epoch 477, Train Loss: 11.55028, Test Loss: 13.62990\n",
      "Epoch 478, Train Loss: 10.89459, Test Loss: 13.41382\n",
      "Epoch 479, Train Loss: 11.50222, Test Loss: 18.41641\n",
      "Epoch 480, Train Loss: 13.26271, Test Loss: 14.03921\n",
      "Epoch 481, Train Loss: 13.50054, Test Loss: 20.11231\n",
      "Epoch 482, Train Loss: 13.81386, Test Loss: 19.62648\n",
      "Epoch 483, Train Loss: 14.20375, Test Loss: 20.66164\n",
      "Epoch 484, Train Loss: 20.24457, Test Loss: 22.06367\n",
      "Epoch 485, Train Loss: 17.02834, Test Loss: 26.80454\n",
      "Epoch 486, Train Loss: 26.81962, Test Loss: 38.79490\n",
      "Epoch 487, Train Loss: 27.28676, Test Loss: 20.14832\n",
      "Epoch 488, Train Loss: 26.24151, Test Loss: 27.89005\n",
      "Epoch 489, Train Loss: 15.83548, Test Loss: 18.80114\n",
      "Epoch 490, Train Loss: 13.98111, Test Loss: 17.80724\n",
      "Epoch 491, Train Loss: 15.70035, Test Loss: 23.08318\n",
      "Epoch 492, Train Loss: 18.84992, Test Loss: 48.79991\n",
      "Epoch 493, Train Loss: 28.70270, Test Loss: 33.00508\n",
      "Epoch 494, Train Loss: 28.53086, Test Loss: 44.59703\n",
      "Epoch 495, Train Loss: 28.58927, Test Loss: 26.06128\n",
      "Epoch 496, Train Loss: 18.55938, Test Loss: 22.56985\n",
      "Epoch 497, Train Loss: 13.35930, Test Loss: 16.25804\n",
      "Epoch 498, Train Loss: 14.82729, Test Loss: 15.24150\n",
      "Epoch 499, Train Loss: 12.36728, Test Loss: 16.96740\n",
      "Epoch 500, Train Loss: 16.70911, Test Loss: 30.56767\n",
      "Epoch 501, Train Loss: 20.49274, Test Loss: 29.43542\n",
      "Epoch 502, Train Loss: 45.31836, Test Loss: 56.28736\n",
      "Epoch 503, Train Loss: 36.61940, Test Loss: 25.78307\n",
      "Epoch 504, Train Loss: 18.47546, Test Loss: 17.16674\n",
      "Epoch 505, Train Loss: 16.77199, Test Loss: 23.19622\n",
      "Epoch 506, Train Loss: 14.36959, Test Loss: 17.88146\n",
      "Epoch 507, Train Loss: 25.12338, Test Loss: 22.91909\n",
      "Epoch 508, Train Loss: 17.69247, Test Loss: 16.79663\n",
      "Epoch 509, Train Loss: 14.72651, Test Loss: 25.11249\n",
      "Epoch 510, Train Loss: 18.43626, Test Loss: 23.43240\n",
      "Epoch 511, Train Loss: 13.61989, Test Loss: 17.78802\n",
      "Epoch 512, Train Loss: 15.32130, Test Loss: 16.55075\n",
      "Epoch 513, Train Loss: 17.61606, Test Loss: 22.85808\n",
      "Epoch 514, Train Loss: 15.60720, Test Loss: 16.85262\n",
      "Epoch 515, Train Loss: 15.60578, Test Loss: 28.19717\n",
      "Epoch 516, Train Loss: 23.00086, Test Loss: 36.36644\n",
      "Epoch 517, Train Loss: 29.28327, Test Loss: 40.47821\n",
      "Epoch 518, Train Loss: 24.12480, Test Loss: 21.10086\n",
      "Epoch 519, Train Loss: 15.21493, Test Loss: 22.81700\n",
      "Epoch 520, Train Loss: 23.75142, Test Loss: 25.87911\n",
      "Epoch 521, Train Loss: 20.79457, Test Loss: 17.93200\n",
      "Epoch 522, Train Loss: 17.73918, Test Loss: 24.36624\n",
      "Epoch 523, Train Loss: 18.69882, Test Loss: 15.18944\n",
      "Epoch 524, Train Loss: 14.01957, Test Loss: 18.01549\n",
      "Epoch 525, Train Loss: 14.79318, Test Loss: 19.20355\n",
      "Epoch 526, Train Loss: 20.59430, Test Loss: 27.84020\n",
      "Epoch 527, Train Loss: 30.94215, Test Loss: 30.03385\n",
      "Epoch 528, Train Loss: 17.33227, Test Loss: 18.29364\n",
      "Epoch 529, Train Loss: 13.71588, Test Loss: 19.42477\n",
      "Epoch 530, Train Loss: 13.97387, Test Loss: 18.37585\n",
      "Epoch 531, Train Loss: 61.48399, Test Loss: 183.28439\n",
      "Epoch 532, Train Loss: 77.44172, Test Loss: 29.78564\n",
      "Epoch 533, Train Loss: 17.69167, Test Loss: 17.85644\n",
      "Epoch 534, Train Loss: 11.77513, Test Loss: 12.75220\n",
      "Epoch 535, Train Loss: 10.42140, Test Loss: 13.23073\n",
      "Epoch 536, Train Loss: 10.28097, Test Loss: 13.38571\n",
      "Epoch 537, Train Loss: 10.40505, Test Loss: 17.75384\n",
      "Epoch 538, Train Loss: 11.72750, Test Loss: 13.42804\n",
      "Epoch 539, Train Loss: 10.85337, Test Loss: 16.79030\n",
      "Epoch 540, Train Loss: 12.39625, Test Loss: 23.92333\n",
      "Epoch 541, Train Loss: 20.40983, Test Loss: 18.10833\n",
      "Epoch 542, Train Loss: 19.11330, Test Loss: 18.53605\n",
      "Epoch 543, Train Loss: 15.28054, Test Loss: 19.30350\n",
      "Epoch 544, Train Loss: 16.44655, Test Loss: 17.65786\n",
      "Epoch 545, Train Loss: 15.26201, Test Loss: 22.78481\n",
      "Epoch 546, Train Loss: 18.06798, Test Loss: 16.31331\n",
      "Epoch 547, Train Loss: 11.96260, Test Loss: 22.91030\n",
      "Epoch 548, Train Loss: 14.93076, Test Loss: 20.75951\n",
      "Epoch 549, Train Loss: 24.43868, Test Loss: 33.21704\n",
      "Epoch 550, Train Loss: 20.92165, Test Loss: 15.89484\n",
      "Epoch 551, Train Loss: 12.66110, Test Loss: 21.31352\n",
      "Epoch 552, Train Loss: 19.14375, Test Loss: 17.97548\n",
      "Epoch 553, Train Loss: 13.79745, Test Loss: 15.83986\n",
      "Epoch 554, Train Loss: 12.25165, Test Loss: 14.22501\n",
      "Epoch 555, Train Loss: 14.38383, Test Loss: 24.13155\n",
      "Epoch 556, Train Loss: 16.41926, Test Loss: 17.83919\n",
      "Epoch 557, Train Loss: 14.99764, Test Loss: 15.40396\n",
      "Epoch 558, Train Loss: 15.02679, Test Loss: 18.08077\n",
      "Epoch 559, Train Loss: 19.72579, Test Loss: 18.24419\n",
      "Epoch 560, Train Loss: 19.83667, Test Loss: 29.82404\n",
      "Epoch 561, Train Loss: 21.71566, Test Loss: 25.47748\n",
      "Epoch 562, Train Loss: 16.99922, Test Loss: 18.97237\n",
      "Epoch 563, Train Loss: 16.13165, Test Loss: 28.08620\n",
      "Epoch 564, Train Loss: 19.64037, Test Loss: 19.02964\n",
      "Epoch 565, Train Loss: 16.71456, Test Loss: 34.94853\n",
      "Epoch 566, Train Loss: 23.28999, Test Loss: 18.94654\n",
      "Epoch 567, Train Loss: 14.25353, Test Loss: 15.20152\n",
      "Epoch 568, Train Loss: 21.78694, Test Loss: 23.75852\n",
      "Epoch 569, Train Loss: 35.33558, Test Loss: 94.73966\n",
      "Epoch 570, Train Loss: 51.52930, Test Loss: 36.90583\n",
      "Epoch 571, Train Loss: 24.90862, Test Loss: 28.38871\n",
      "Epoch 572, Train Loss: 15.46636, Test Loss: 14.79483\n",
      "Epoch 573, Train Loss: 11.36710, Test Loss: 13.18940\n",
      "Epoch 574, Train Loss: 10.26020, Test Loss: 17.09332\n",
      "Epoch 575, Train Loss: 12.13591, Test Loss: 13.74358\n",
      "Epoch 576, Train Loss: 11.46564, Test Loss: 15.71499\n",
      "Epoch 577, Train Loss: 14.68199, Test Loss: 16.38433\n",
      "Epoch 578, Train Loss: 17.16251, Test Loss: 16.47544\n",
      "Epoch 579, Train Loss: 13.68856, Test Loss: 16.76184\n",
      "Epoch 580, Train Loss: 13.87832, Test Loss: 18.74476\n",
      "Epoch 581, Train Loss: 21.11497, Test Loss: 30.65738\n",
      "Epoch 582, Train Loss: 23.54120, Test Loss: 28.28195\n",
      "Epoch 583, Train Loss: 16.20689, Test Loss: 15.43305\n",
      "Epoch 584, Train Loss: 10.87958, Test Loss: 15.46448\n",
      "Epoch 585, Train Loss: 11.46054, Test Loss: 17.26495\n",
      "Epoch 586, Train Loss: 15.30610, Test Loss: 17.13281\n",
      "Epoch 587, Train Loss: 10.48035, Test Loss: 11.81894\n",
      "Epoch 588, Train Loss: 10.91050, Test Loss: 13.89852\n",
      "Epoch 589, Train Loss: 16.80420, Test Loss: 31.92338\n",
      "Epoch 590, Train Loss: 26.27056, Test Loss: 27.15712\n",
      "Epoch 591, Train Loss: 16.23704, Test Loss: 17.91051\n",
      "Epoch 592, Train Loss: 13.34030, Test Loss: 17.50696\n",
      "Epoch 593, Train Loss: 13.00135, Test Loss: 17.75421\n",
      "Epoch 594, Train Loss: 17.13455, Test Loss: 23.39417\n",
      "Epoch 595, Train Loss: 18.86706, Test Loss: 16.47411\n",
      "Epoch 596, Train Loss: 13.89724, Test Loss: 14.63457\n",
      "Epoch 597, Train Loss: 20.53154, Test Loss: 29.71149\n",
      "Epoch 598, Train Loss: 20.60961, Test Loss: 22.46664\n",
      "Epoch 599, Train Loss: 17.06435, Test Loss: 20.66534\n",
      "Epoch 600, Train Loss: 20.78006, Test Loss: 44.66153\n",
      "Epoch 601, Train Loss: 40.24590, Test Loss: 33.86663\n",
      "Epoch 602, Train Loss: 16.25260, Test Loss: 15.47198\n",
      "Epoch 603, Train Loss: 10.82363, Test Loss: 13.47577\n",
      "Epoch 604, Train Loss: 9.07929, Test Loss: 10.94827\n",
      "Epoch 605, Train Loss: 10.75162, Test Loss: 12.49184\n",
      "Epoch 606, Train Loss: 18.49320, Test Loss: 23.25580\n",
      "Epoch 607, Train Loss: 15.03858, Test Loss: 15.37124\n",
      "Epoch 608, Train Loss: 12.52180, Test Loss: 15.84131\n",
      "Epoch 609, Train Loss: 12.74394, Test Loss: 10.98615\n",
      "Epoch 610, Train Loss: 12.71220, Test Loss: 20.03396\n",
      "Epoch 611, Train Loss: 19.13554, Test Loss: 19.44775\n",
      "Epoch 612, Train Loss: 13.93093, Test Loss: 15.37565\n",
      "Epoch 613, Train Loss: 11.45631, Test Loss: 16.40595\n",
      "Epoch 614, Train Loss: 16.88785, Test Loss: 23.97739\n",
      "Epoch 615, Train Loss: 19.05699, Test Loss: 33.08009\n",
      "Epoch 616, Train Loss: 15.51085, Test Loss: 16.13228\n",
      "Epoch 617, Train Loss: 15.73878, Test Loss: 27.56634\n",
      "Epoch 618, Train Loss: 24.29978, Test Loss: 24.65574\n",
      "Epoch 619, Train Loss: 19.81048, Test Loss: 25.48703\n",
      "Epoch 620, Train Loss: 20.30101, Test Loss: 28.16337\n",
      "Epoch 621, Train Loss: 18.83486, Test Loss: 14.94487\n",
      "Epoch 622, Train Loss: 10.87569, Test Loss: 22.70934\n",
      "Epoch 623, Train Loss: 13.85832, Test Loss: 11.19022\n",
      "Epoch 624, Train Loss: 9.92492, Test Loss: 15.33150\n",
      "Epoch 625, Train Loss: 10.68963, Test Loss: 12.77659\n",
      "Epoch 626, Train Loss: 11.43476, Test Loss: 15.65895\n",
      "Epoch 627, Train Loss: 11.42036, Test Loss: 12.69491\n",
      "Epoch 628, Train Loss: 14.74103, Test Loss: 16.66048\n",
      "Epoch 629, Train Loss: 17.12137, Test Loss: 14.85548\n",
      "Epoch 630, Train Loss: 16.43330, Test Loss: 21.26300\n",
      "Epoch 631, Train Loss: 23.78782, Test Loss: 24.67601\n",
      "Epoch 632, Train Loss: 16.85274, Test Loss: 17.94457\n",
      "Epoch 633, Train Loss: 15.07675, Test Loss: 19.67986\n",
      "Epoch 634, Train Loss: 28.09064, Test Loss: 38.41253\n",
      "Epoch 635, Train Loss: 22.48543, Test Loss: 32.98432\n",
      "Epoch 636, Train Loss: 15.16153, Test Loss: 14.43274\n",
      "Epoch 637, Train Loss: 11.07966, Test Loss: 12.79225\n",
      "Epoch 638, Train Loss: 11.81492, Test Loss: 10.41391\n",
      "Epoch 639, Train Loss: 12.06498, Test Loss: 30.76331\n",
      "Epoch 640, Train Loss: 18.94089, Test Loss: 16.06461\n",
      "Epoch 641, Train Loss: 15.18990, Test Loss: 18.69579\n",
      "Epoch 642, Train Loss: 15.36402, Test Loss: 17.41321\n",
      "Epoch 643, Train Loss: 17.33342, Test Loss: 28.26560\n",
      "Epoch 644, Train Loss: 13.68423, Test Loss: 12.42762\n",
      "Epoch 645, Train Loss: 13.96123, Test Loss: 15.24475\n",
      "Epoch 646, Train Loss: 10.64045, Test Loss: 15.64049\n",
      "Epoch 647, Train Loss: 14.01753, Test Loss: 12.51049\n",
      "Epoch 648, Train Loss: 10.08139, Test Loss: 11.17006\n",
      "Epoch 649, Train Loss: 16.69157, Test Loss: 19.95257\n",
      "Epoch 650, Train Loss: 13.84069, Test Loss: 19.99706\n",
      "Epoch 651, Train Loss: 18.93846, Test Loss: 34.71542\n",
      "Epoch 652, Train Loss: 24.26843, Test Loss: 32.65160\n",
      "Epoch 653, Train Loss: 31.62730, Test Loss: 41.56812\n",
      "Epoch 654, Train Loss: 22.60517, Test Loss: 15.68870\n",
      "Epoch 655, Train Loss: 13.42306, Test Loss: 16.23694\n",
      "Epoch 656, Train Loss: 12.82265, Test Loss: 21.24124\n",
      "Epoch 657, Train Loss: 21.57971, Test Loss: 20.93420\n",
      "Epoch 658, Train Loss: 14.80191, Test Loss: 13.53483\n",
      "Epoch 659, Train Loss: 8.68385, Test Loss: 12.69679\n",
      "Epoch 660, Train Loss: 13.07485, Test Loss: 13.35134\n",
      "Epoch 661, Train Loss: 13.88340, Test Loss: 18.96389\n",
      "Epoch 662, Train Loss: 33.44862, Test Loss: 38.07440\n",
      "Epoch 663, Train Loss: 17.22323, Test Loss: 13.42987\n",
      "Epoch 664, Train Loss: 10.26227, Test Loss: 14.96684\n",
      "Epoch 665, Train Loss: 12.42928, Test Loss: 20.32911\n",
      "Epoch 666, Train Loss: 13.81300, Test Loss: 13.27834\n",
      "Epoch 667, Train Loss: 10.06496, Test Loss: 12.72687\n",
      "Epoch 668, Train Loss: 11.85584, Test Loss: 14.40748\n",
      "Epoch 669, Train Loss: 13.85126, Test Loss: 12.26484\n",
      "Epoch 670, Train Loss: 11.50760, Test Loss: 18.49944\n",
      "Epoch 671, Train Loss: 13.62135, Test Loss: 14.45950\n",
      "Epoch 672, Train Loss: 10.58966, Test Loss: 14.20002\n",
      "Epoch 673, Train Loss: 11.08489, Test Loss: 20.54614\n",
      "Epoch 674, Train Loss: 23.08041, Test Loss: 25.22227\n",
      "Epoch 675, Train Loss: 30.06378, Test Loss: 25.50009\n",
      "Epoch 676, Train Loss: 19.39243, Test Loss: 12.58443\n",
      "Epoch 677, Train Loss: 10.80601, Test Loss: 13.93421\n",
      "Epoch 678, Train Loss: 9.37972, Test Loss: 11.38032\n",
      "Epoch 679, Train Loss: 9.31030, Test Loss: 14.52632\n",
      "Epoch 680, Train Loss: 13.33131, Test Loss: 18.30305\n",
      "Epoch 681, Train Loss: 15.95911, Test Loss: 19.58592\n",
      "Epoch 682, Train Loss: 21.40697, Test Loss: 23.15852\n",
      "Epoch 683, Train Loss: 15.76656, Test Loss: 26.54195\n",
      "Epoch 684, Train Loss: 13.29566, Test Loss: 12.90559\n",
      "Epoch 685, Train Loss: 13.73607, Test Loss: 19.55913\n",
      "Epoch 686, Train Loss: 17.56558, Test Loss: 24.40241\n",
      "Epoch 687, Train Loss: 13.18738, Test Loss: 15.62299\n",
      "Epoch 688, Train Loss: 11.43279, Test Loss: 15.15674\n",
      "Epoch 689, Train Loss: 10.70991, Test Loss: 10.26963\n",
      "Epoch 690, Train Loss: 9.77244, Test Loss: 10.59318\n",
      "Epoch 691, Train Loss: 10.40349, Test Loss: 16.45965\n",
      "Epoch 692, Train Loss: 13.97490, Test Loss: 14.84990\n",
      "Epoch 693, Train Loss: 18.42925, Test Loss: 32.89673\n",
      "Epoch 694, Train Loss: 28.99039, Test Loss: 21.84582\n",
      "Epoch 695, Train Loss: 15.82790, Test Loss: 12.52352\n",
      "Epoch 696, Train Loss: 11.84640, Test Loss: 15.84017\n",
      "Epoch 697, Train Loss: 9.38365, Test Loss: 11.40966\n",
      "Epoch 698, Train Loss: 8.96614, Test Loss: 11.86429\n",
      "Epoch 699, Train Loss: 10.57532, Test Loss: 11.68136\n",
      "Epoch 700, Train Loss: 9.22731, Test Loss: 10.45046\n",
      "Epoch 701, Train Loss: 11.81383, Test Loss: 24.67001\n",
      "Epoch 702, Train Loss: 13.26149, Test Loss: 13.20073\n",
      "Epoch 703, Train Loss: 10.96380, Test Loss: 22.44870\n",
      "Epoch 704, Train Loss: 13.94140, Test Loss: 26.93633\n",
      "Epoch 705, Train Loss: 23.81982, Test Loss: 29.34444\n",
      "Epoch 706, Train Loss: 29.39842, Test Loss: 24.91641\n",
      "Epoch 707, Train Loss: 16.00071, Test Loss: 21.14087\n",
      "Epoch 708, Train Loss: 16.04440, Test Loss: 22.31188\n",
      "Epoch 709, Train Loss: 11.45619, Test Loss: 14.06411\n",
      "Epoch 710, Train Loss: 12.91611, Test Loss: 12.85451\n",
      "Epoch 711, Train Loss: 10.70491, Test Loss: 12.00099\n",
      "Epoch 712, Train Loss: 12.51141, Test Loss: 15.33397\n",
      "Epoch 713, Train Loss: 15.13827, Test Loss: 17.77124\n",
      "Epoch 714, Train Loss: 13.50528, Test Loss: 11.16285\n",
      "Epoch 715, Train Loss: 11.31201, Test Loss: 14.09067\n",
      "Epoch 716, Train Loss: 10.75559, Test Loss: 19.07847\n",
      "Epoch 717, Train Loss: 15.67349, Test Loss: 22.86018\n",
      "Epoch 718, Train Loss: 17.70735, Test Loss: 24.29671\n",
      "Epoch 719, Train Loss: 20.32545, Test Loss: 19.29419\n",
      "Epoch 720, Train Loss: 13.46149, Test Loss: 13.47058\n",
      "Epoch 721, Train Loss: 11.61934, Test Loss: 16.74380\n",
      "Epoch 722, Train Loss: 48.22804, Test Loss: 47.34253\n",
      "Epoch 723, Train Loss: 18.31973, Test Loss: 11.95408\n",
      "Epoch 724, Train Loss: 8.12277, Test Loss: 9.28484\n",
      "Epoch 725, Train Loss: 7.81336, Test Loss: 8.68587\n",
      "Epoch 726, Train Loss: 7.74200, Test Loss: 9.43851\n",
      "Epoch 727, Train Loss: 7.83213, Test Loss: 9.30751\n",
      "Epoch 728, Train Loss: 6.76274, Test Loss: 10.77539\n",
      "Epoch 729, Train Loss: 10.19222, Test Loss: 15.42336\n",
      "Epoch 730, Train Loss: 12.83083, Test Loss: 14.80439\n",
      "Epoch 731, Train Loss: 12.32669, Test Loss: 19.37383\n",
      "Epoch 732, Train Loss: 19.81687, Test Loss: 16.78061\n",
      "Epoch 733, Train Loss: 10.62855, Test Loss: 10.17413\n",
      "Epoch 734, Train Loss: 10.24571, Test Loss: 11.53424\n",
      "Epoch 735, Train Loss: 10.17668, Test Loss: 14.20274\n",
      "Epoch 736, Train Loss: 14.72974, Test Loss: 18.31839\n",
      "Epoch 737, Train Loss: 14.24271, Test Loss: 16.96606\n",
      "Epoch 738, Train Loss: 15.81726, Test Loss: 12.12952\n",
      "Epoch 739, Train Loss: 8.40825, Test Loss: 11.41962\n",
      "Epoch 740, Train Loss: 14.05872, Test Loss: 18.55507\n",
      "Epoch 741, Train Loss: 14.45650, Test Loss: 11.69714\n",
      "Epoch 742, Train Loss: 9.82180, Test Loss: 14.40815\n",
      "Epoch 743, Train Loss: 13.95699, Test Loss: 14.38603\n",
      "Epoch 744, Train Loss: 11.64065, Test Loss: 11.16972\n",
      "Epoch 745, Train Loss: 9.81268, Test Loss: 12.99372\n",
      "Epoch 746, Train Loss: 10.99460, Test Loss: 14.66175\n",
      "Epoch 747, Train Loss: 12.06440, Test Loss: 16.37901\n",
      "Epoch 748, Train Loss: 25.64558, Test Loss: 25.52273\n",
      "Epoch 749, Train Loss: 14.63306, Test Loss: 14.97388\n",
      "Epoch 750, Train Loss: 18.11993, Test Loss: 21.01961\n",
      "Epoch 751, Train Loss: 12.29330, Test Loss: 13.99114\n",
      "Epoch 752, Train Loss: 9.45706, Test Loss: 11.19405\n",
      "Epoch 753, Train Loss: 9.02891, Test Loss: 14.55293\n",
      "Epoch 754, Train Loss: 11.44414, Test Loss: 21.53949\n",
      "Epoch 755, Train Loss: 15.58944, Test Loss: 20.04965\n",
      "Epoch 756, Train Loss: 14.53581, Test Loss: 13.64162\n",
      "Epoch 757, Train Loss: 15.90335, Test Loss: 24.35739\n",
      "Epoch 758, Train Loss: 15.88158, Test Loss: 12.36802\n",
      "Epoch 759, Train Loss: 7.92083, Test Loss: 12.02376\n",
      "Epoch 760, Train Loss: 8.64230, Test Loss: 12.45473\n",
      "Epoch 761, Train Loss: 14.81780, Test Loss: 18.21000\n",
      "Epoch 762, Train Loss: 15.95843, Test Loss: 16.36516\n",
      "Epoch 763, Train Loss: 14.30362, Test Loss: 14.64492\n",
      "Epoch 764, Train Loss: 9.88661, Test Loss: 11.38504\n",
      "Epoch 765, Train Loss: 8.41978, Test Loss: 14.09076\n",
      "Epoch 766, Train Loss: 10.77625, Test Loss: 19.84364\n",
      "Epoch 767, Train Loss: 13.41578, Test Loss: 22.15063\n",
      "Epoch 768, Train Loss: 17.45192, Test Loss: 31.72508\n",
      "Epoch 769, Train Loss: 14.40564, Test Loss: 18.46401\n",
      "Epoch 770, Train Loss: 9.95217, Test Loss: 10.67009\n",
      "Epoch 771, Train Loss: 9.79750, Test Loss: 11.57575\n",
      "Epoch 772, Train Loss: 11.35620, Test Loss: 16.03555\n",
      "Epoch 773, Train Loss: 14.83039, Test Loss: 16.74844\n",
      "Epoch 774, Train Loss: 14.38092, Test Loss: 14.56078\n",
      "Epoch 775, Train Loss: 10.75166, Test Loss: 16.49621\n",
      "Epoch 776, Train Loss: 19.95372, Test Loss: 29.28246\n",
      "Epoch 777, Train Loss: 28.02167, Test Loss: 45.21494\n",
      "Epoch 778, Train Loss: 19.46394, Test Loss: 12.49813\n",
      "Epoch 779, Train Loss: 8.14830, Test Loss: 9.86101\n",
      "Epoch 780, Train Loss: 7.71182, Test Loss: 8.41612\n",
      "Epoch 781, Train Loss: 9.51728, Test Loss: 11.17439\n",
      "Epoch 782, Train Loss: 8.41362, Test Loss: 10.35843\n",
      "Epoch 783, Train Loss: 9.96979, Test Loss: 12.89740\n",
      "Epoch 784, Train Loss: 11.94617, Test Loss: 14.96884\n",
      "Epoch 785, Train Loss: 14.49535, Test Loss: 16.22471\n",
      "Epoch 786, Train Loss: 9.70114, Test Loss: 11.29644\n",
      "Epoch 787, Train Loss: 10.88939, Test Loss: 13.79047\n",
      "Epoch 788, Train Loss: 12.81664, Test Loss: 15.11527\n",
      "Epoch 789, Train Loss: 17.58286, Test Loss: 24.64738\n",
      "Epoch 790, Train Loss: 14.85983, Test Loss: 14.42038\n",
      "Epoch 791, Train Loss: 12.23766, Test Loss: 20.81660\n",
      "Epoch 792, Train Loss: 15.11857, Test Loss: 11.20867\n",
      "Epoch 793, Train Loss: 8.21669, Test Loss: 8.78467\n",
      "Epoch 794, Train Loss: 8.30093, Test Loss: 15.10415\n",
      "Epoch 795, Train Loss: 13.82184, Test Loss: 13.12800\n",
      "Epoch 796, Train Loss: 9.01468, Test Loss: 10.53220\n",
      "Epoch 797, Train Loss: 10.82567, Test Loss: 13.92093\n",
      "Epoch 798, Train Loss: 10.15635, Test Loss: 19.73042\n",
      "Epoch 799, Train Loss: 18.95196, Test Loss: 24.63300\n",
      "Epoch 800, Train Loss: 15.15787, Test Loss: 13.07873\n",
      "Epoch 801, Train Loss: 13.53774, Test Loss: 13.01620\n",
      "Epoch 802, Train Loss: 10.92619, Test Loss: 11.16696\n",
      "Epoch 803, Train Loss: 8.30754, Test Loss: 9.28636\n",
      "Epoch 804, Train Loss: 6.64367, Test Loss: 9.66551\n",
      "Epoch 805, Train Loss: 10.40864, Test Loss: 20.57721\n",
      "Epoch 806, Train Loss: 14.29007, Test Loss: 14.00980\n",
      "Epoch 807, Train Loss: 13.69975, Test Loss: 24.25029\n",
      "Epoch 808, Train Loss: 15.86435, Test Loss: 17.63145\n",
      "Epoch 809, Train Loss: 12.76199, Test Loss: 16.11773\n",
      "Epoch 810, Train Loss: 13.62499, Test Loss: 18.41898\n",
      "Epoch 811, Train Loss: 14.84151, Test Loss: 14.34420\n",
      "Epoch 812, Train Loss: 15.21602, Test Loss: 15.67707\n",
      "Epoch 813, Train Loss: 8.76588, Test Loss: 11.19109\n",
      "Epoch 814, Train Loss: 10.25176, Test Loss: 13.63252\n",
      "Epoch 815, Train Loss: 9.28940, Test Loss: 12.35150\n",
      "Epoch 816, Train Loss: 12.75448, Test Loss: 18.76003\n",
      "Epoch 817, Train Loss: 11.93138, Test Loss: 12.61538\n",
      "Epoch 818, Train Loss: 9.51777, Test Loss: 13.80952\n",
      "Epoch 819, Train Loss: 25.51445, Test Loss: 21.03666\n",
      "Epoch 820, Train Loss: 13.73112, Test Loss: 12.84768\n",
      "Epoch 821, Train Loss: 8.52554, Test Loss: 9.53086\n",
      "Epoch 822, Train Loss: 7.18638, Test Loss: 9.87108\n",
      "Epoch 823, Train Loss: 9.47716, Test Loss: 17.69355\n",
      "Epoch 824, Train Loss: 17.46684, Test Loss: 13.11637\n",
      "Epoch 825, Train Loss: 9.83613, Test Loss: 22.08348\n",
      "Epoch 826, Train Loss: 9.80404, Test Loss: 10.43321\n",
      "Epoch 827, Train Loss: 9.26205, Test Loss: 11.51786\n",
      "Epoch 828, Train Loss: 12.57839, Test Loss: 15.47568\n",
      "Epoch 829, Train Loss: 10.42229, Test Loss: 9.28936\n",
      "Epoch 830, Train Loss: 10.65730, Test Loss: 16.51638\n",
      "Epoch 831, Train Loss: 13.90787, Test Loss: 22.69468\n",
      "Epoch 832, Train Loss: 19.06736, Test Loss: 48.40842\n",
      "Epoch 833, Train Loss: 51.01169, Test Loss: 35.41539\n",
      "Epoch 834, Train Loss: 14.13245, Test Loss: 9.68796\n",
      "Epoch 835, Train Loss: 7.34909, Test Loss: 9.66024\n",
      "Epoch 836, Train Loss: 6.59677, Test Loss: 7.81512\n",
      "Epoch 837, Train Loss: 5.53742, Test Loss: 6.75162\n",
      "Epoch 838, Train Loss: 7.48330, Test Loss: 13.39014\n",
      "Epoch 839, Train Loss: 9.67472, Test Loss: 24.92002\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "# from Models.SeEANet import SeEANet\n",
    "from Models.MyoNet import MyoNet\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "from utils.tools import make_datasets,make_dir\n",
    "\n",
    "# 数据加载\n",
    "data_Dir = \"/home/admin123/SATData/data\"\n",
    "# 实例化数据集\n",
    "emg_data, angle_data = make_datasets(data_Dir, peopleList=['S01'], exp_class=\"MJ\", cluster_num=6, fusionMethod=\"PCA\", windowLength=256, stepLength=1, delta_T=20)\n",
    "\n",
    "semgData = torch.tensor(emg_data, dtype=torch.float32)\n",
    "angleData = torch.tensor(angle_data, dtype=torch.float32)\n",
    "print(\"semg 数据形状为：\", semgData.shape)\n",
    "print(\"angle 数据形状为：\", angleData.shape)\n",
    "\n",
    "dataset = TensorDataset(semgData, angleData)\n",
    "\n",
    "# 定义划分比例\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "# 数据集分割\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 数据加载\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# 初始化模型、优化器和损失函数\n",
    "model = MyoNet(PreNum=3)\n",
    "modelName = \"MyoNet\"\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "formatted_time = datetime.now().strftime(\"%m-%d-%H:%M:%S\")\n",
    "checkpoint_save_Dir = os.path.join(\"/home/admin123/SATData\", \"Run\", modelName, formatted_time)\n",
    "make_dir(checkpoint_save_Dir)\n",
    "checkpoint_save_path = os.path.join(checkpoint_save_Dir,\"best.pth\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "train_running_loss_ls = []\n",
    "test_running_loss_ls = []\n",
    "test_accuracies = []\n",
    "for epoch in range(1, 2000+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_running_loss_ls.append(running_loss)\n",
    "\n",
    "    \n",
    "    # 测试模型\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss\n",
    "    test_running_loss_ls.append(test_loss)\n",
    "    # if epoch % 50 == 0:\n",
    "    print('Epoch %d, Train Loss: %.5f, Test Loss: %.5f'%(epoch, running_loss, test_loss*4))\n",
    "\n",
    "    if running_loss < best_loss:\n",
    "        best_loss = running_loss\n",
    "        torch.save(model.state_dict(), checkpoint_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 文件已保存到: /home/admin123/SATData/Run/MyoNet/04-13-14:24:40/output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# 计算性能指标\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "file_path = os.path.join(checkpoint_save_Dir, \"output.csv\")\n",
    "# 将两个列表写入到 CSV 文件\n",
    "with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    # 写入表头（可选）\n",
    "    writer.writerow([\"train_loss\", \"test_loss\"])\n",
    "    \n",
    "    # 写入数据\n",
    "    for item1, item2 in zip(train_running_loss_ls, test_running_loss_ls):\n",
    "        writer.writerow([item1, item2])\n",
    "print(f\"loss 文件已保存到: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_running_loss_ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
