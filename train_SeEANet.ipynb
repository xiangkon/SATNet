{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semg 数据形状为： torch.Size([26645, 6, 256])\n",
      "angle 数据形状为： torch.Size([26645, 3])\n",
      "learning rate= 0.001\n",
      "Epoch 1, Train Loss: 164574.63312, Test Loss: 33842.92578\n",
      "learning rate= 0.001\n",
      "Epoch 2, Train Loss: 33456.45151, Test Loss: 33272.25000\n",
      "learning rate= 0.001\n",
      "Epoch 3, Train Loss: 33378.67300, Test Loss: 33253.73438\n",
      "learning rate= 0.001\n",
      "Epoch 4, Train Loss: 33472.22009, Test Loss: 33238.89844\n",
      "learning rate= 0.001\n",
      "Epoch 5, Train Loss: 33333.26715, Test Loss: 33222.73047\n",
      "learning rate= 0.001\n",
      "Epoch 6, Train Loss: 33380.66174, Test Loss: 33264.05078\n",
      "learning rate= 0.001\n",
      "Epoch 7, Train Loss: 33517.64697, Test Loss: 33243.63672\n",
      "learning rate= 0.001\n",
      "Epoch 8, Train Loss: 33415.74533, Test Loss: 33252.78906\n",
      "learning rate= 0.001\n",
      "Epoch 9, Train Loss: 33390.44501, Test Loss: 33251.94922\n",
      "learning rate= 0.001\n",
      "Epoch 10, Train Loss: 33459.27817, Test Loss: 33228.26562\n",
      "learning rate= 0.001\n",
      "Epoch 11, Train Loss: 33505.95276, Test Loss: 33250.54688\n",
      "learning rate= 0.001\n",
      "Epoch 12, Train Loss: 33371.96011, Test Loss: 33259.17578\n",
      "learning rate= 0.001\n",
      "Epoch 13, Train Loss: 33414.21661, Test Loss: 33268.39062\n",
      "learning rate= 0.001\n",
      "Epoch 14, Train Loss: 33562.62073, Test Loss: 33257.38672\n",
      "learning rate= 0.001\n",
      "Epoch 15, Train Loss: 33498.56293, Test Loss: 33296.80859\n",
      "learning rate= 0.001\n",
      "Epoch 16, Train Loss: 33399.83820, Test Loss: 33265.86719\n",
      "learning rate= 0.001\n",
      "Epoch 17, Train Loss: 33374.61694, Test Loss: 33140.92188\n",
      "learning rate= 0.001\n",
      "Epoch 18, Train Loss: 30840.81424, Test Loss: 28526.55078\n",
      "learning rate= 0.001\n",
      "Epoch 19, Train Loss: 27124.78821, Test Loss: 25365.83594\n",
      "learning rate= 0.001\n",
      "Epoch 20, Train Loss: 20881.28850, Test Loss: 16457.94336\n",
      "learning rate= 0.001\n",
      "Epoch 21, Train Loss: 14129.68204, Test Loss: 11824.30664\n",
      "learning rate= 0.001\n",
      "Epoch 22, Train Loss: 9445.53796, Test Loss: 6941.43896\n",
      "learning rate= 0.001\n",
      "Epoch 23, Train Loss: 5132.91027, Test Loss: 3456.34375\n",
      "learning rate= 0.001\n",
      "Epoch 24, Train Loss: 2522.97331, Test Loss: 1827.80994\n",
      "learning rate= 0.001\n",
      "Epoch 25, Train Loss: 1690.39714, Test Loss: 1740.88257\n",
      "learning rate= 0.001\n",
      "Epoch 26, Train Loss: 1035.92343, Test Loss: 863.85107\n",
      "learning rate= 0.001\n",
      "Epoch 27, Train Loss: 753.22169, Test Loss: 694.34467\n",
      "learning rate= 0.001\n",
      "Epoch 28, Train Loss: 528.80686, Test Loss: 511.57785\n",
      "learning rate= 0.001\n",
      "Epoch 29, Train Loss: 423.21394, Test Loss: 440.14264\n",
      "learning rate= 0.001\n",
      "Epoch 30, Train Loss: 386.49486, Test Loss: 412.30807\n",
      "learning rate= 0.001\n",
      "Epoch 31, Train Loss: 348.98425, Test Loss: 312.14551\n",
      "learning rate= 0.001\n",
      "Epoch 32, Train Loss: 266.74398, Test Loss: 266.20551\n",
      "learning rate= 0.001\n",
      "Epoch 33, Train Loss: 244.44477, Test Loss: 246.63934\n",
      "learning rate= 0.001\n",
      "Epoch 34, Train Loss: 217.86516, Test Loss: 269.61542\n",
      "learning rate= 0.001\n",
      "Epoch 35, Train Loss: 189.46709, Test Loss: 197.74165\n",
      "learning rate= 0.001\n",
      "Epoch 36, Train Loss: 189.33553, Test Loss: 186.71986\n",
      "learning rate= 0.001\n",
      "Epoch 37, Train Loss: 147.12117, Test Loss: 170.50969\n",
      "learning rate= 0.001\n",
      "Epoch 38, Train Loss: 140.73145, Test Loss: 174.54146\n",
      "learning rate= 0.001\n",
      "Epoch 39, Train Loss: 140.35275, Test Loss: 166.72668\n",
      "learning rate= 0.001\n",
      "Epoch 40, Train Loss: 120.45154, Test Loss: 133.57120\n",
      "learning rate= 0.001\n",
      "Epoch 41, Train Loss: 106.08995, Test Loss: 116.87823\n",
      "learning rate= 0.001\n",
      "Epoch 42, Train Loss: 172.98519, Test Loss: 289.40631\n",
      "learning rate= 0.001\n",
      "Epoch 43, Train Loss: 159.26858, Test Loss: 129.74164\n",
      "learning rate= 0.001\n",
      "Epoch 44, Train Loss: 104.07978, Test Loss: 116.82686\n",
      "learning rate= 0.001\n",
      "Epoch 45, Train Loss: 101.83056, Test Loss: 110.16010\n",
      "learning rate= 0.001\n",
      "Epoch 46, Train Loss: 80.17207, Test Loss: 96.83162\n",
      "learning rate= 0.001\n",
      "Epoch 47, Train Loss: 107.47755, Test Loss: 89.33952\n",
      "learning rate= 0.001\n",
      "Epoch 48, Train Loss: 72.52062, Test Loss: 100.80692\n",
      "learning rate= 0.001\n",
      "Epoch 49, Train Loss: 67.90460, Test Loss: 77.52732\n",
      "learning rate= 0.001\n",
      "Epoch 50, Train Loss: 62.61854, Test Loss: 78.43178\n",
      "learning rate= 0.001\n",
      "Epoch 51, Train Loss: 63.47278, Test Loss: 73.76641\n",
      "learning rate= 0.001\n",
      "Epoch 52, Train Loss: 75.70077, Test Loss: 78.35522\n",
      "learning rate= 0.001\n",
      "Epoch 53, Train Loss: 59.01479, Test Loss: 66.61530\n",
      "learning rate= 0.001\n",
      "Epoch 54, Train Loss: 58.38409, Test Loss: 62.47465\n",
      "learning rate= 0.001\n",
      "Epoch 55, Train Loss: 77.80169, Test Loss: 82.40599\n",
      "learning rate= 0.001\n",
      "Epoch 56, Train Loss: 62.56406, Test Loss: 72.04458\n",
      "learning rate= 0.001\n",
      "Epoch 57, Train Loss: 59.82480, Test Loss: 69.84253\n",
      "learning rate= 0.001\n",
      "Epoch 58, Train Loss: 82.56706, Test Loss: 109.81732\n",
      "learning rate= 0.001\n",
      "Epoch 59, Train Loss: 72.69325, Test Loss: 69.87119\n",
      "learning rate= 0.001\n",
      "Epoch 60, Train Loss: 55.97888, Test Loss: 89.83726\n",
      "learning rate= 0.001\n",
      "Epoch 61, Train Loss: 145.59039, Test Loss: 124.79633\n",
      "learning rate= 0.001\n",
      "Epoch 62, Train Loss: 57.35389, Test Loss: 46.48341\n",
      "learning rate= 0.001\n",
      "Epoch 63, Train Loss: 36.38856, Test Loss: 47.71472\n",
      "learning rate= 0.001\n",
      "Epoch 64, Train Loss: 35.38980, Test Loss: 45.49561\n",
      "learning rate= 0.001\n",
      "Epoch 65, Train Loss: 33.21986, Test Loss: 39.94750\n",
      "learning rate= 0.001\n",
      "Epoch 66, Train Loss: 32.22020, Test Loss: 36.59742\n",
      "learning rate= 0.001\n",
      "Epoch 67, Train Loss: 33.52592, Test Loss: 56.78322\n",
      "learning rate= 0.001\n",
      "Epoch 68, Train Loss: 58.49719, Test Loss: 75.24079\n",
      "learning rate= 0.001\n",
      "Epoch 69, Train Loss: 68.32481, Test Loss: 66.86575\n",
      "learning rate= 0.001\n",
      "Epoch 70, Train Loss: 47.21067, Test Loss: 58.00706\n",
      "learning rate= 0.001\n",
      "Epoch 71, Train Loss: 56.48554, Test Loss: 46.81048\n",
      "learning rate= 0.001\n",
      "Epoch 72, Train Loss: 28.59122, Test Loss: 36.51011\n",
      "learning rate= 0.001\n",
      "Epoch 73, Train Loss: 31.64290, Test Loss: 29.16366\n",
      "learning rate= 0.001\n",
      "Epoch 74, Train Loss: 29.05889, Test Loss: 44.87837\n",
      "learning rate= 0.001\n",
      "Epoch 75, Train Loss: 40.09296, Test Loss: 40.80331\n",
      "learning rate= 0.001\n",
      "Epoch 76, Train Loss: 34.38987, Test Loss: 79.76340\n",
      "learning rate= 0.001\n",
      "Epoch 77, Train Loss: 39.25287, Test Loss: 38.92916\n",
      "learning rate= 0.001\n",
      "Epoch 78, Train Loss: 28.11373, Test Loss: 29.67591\n",
      "learning rate= 0.001\n",
      "Epoch 79, Train Loss: 28.18563, Test Loss: 30.84684\n",
      "learning rate= 0.001\n",
      "Epoch 80, Train Loss: 44.56333, Test Loss: 45.87553\n",
      "learning rate= 0.001\n",
      "Epoch 81, Train Loss: 47.16235, Test Loss: 55.29049\n",
      "learning rate= 0.001\n",
      "Epoch 82, Train Loss: 46.89286, Test Loss: 39.48806\n",
      "learning rate= 0.001\n",
      "Epoch 83, Train Loss: 22.53694, Test Loss: 27.07529\n",
      "learning rate= 0.001\n",
      "Epoch 84, Train Loss: 22.00059, Test Loss: 24.26041\n",
      "learning rate= 0.001\n",
      "Epoch 85, Train Loss: 24.74076, Test Loss: 25.24380\n",
      "learning rate= 0.001\n",
      "Epoch 86, Train Loss: 22.46540, Test Loss: 32.04675\n",
      "learning rate= 0.001\n",
      "Epoch 87, Train Loss: 38.18246, Test Loss: 31.11758\n",
      "learning rate= 0.001\n",
      "Epoch 88, Train Loss: 43.00343, Test Loss: 48.00206\n",
      "learning rate= 0.001\n",
      "Epoch 89, Train Loss: 35.18532, Test Loss: 26.52260\n",
      "learning rate= 0.001\n",
      "Epoch 90, Train Loss: 23.64537, Test Loss: 33.76476\n",
      "learning rate= 0.001\n",
      "Epoch 91, Train Loss: 32.50656, Test Loss: 44.34218\n",
      "learning rate= 0.001\n",
      "Epoch 92, Train Loss: 36.62061, Test Loss: 31.57674\n",
      "learning rate= 0.001\n",
      "Epoch 93, Train Loss: 38.36704, Test Loss: 54.69374\n",
      "learning rate= 0.001\n",
      "Epoch 94, Train Loss: 99.86386, Test Loss: 108.17770\n",
      "learning rate= 0.001\n",
      "Epoch 95, Train Loss: 51.25347, Test Loss: 46.75953\n",
      "learning rate= 0.001\n",
      "Epoch 96, Train Loss: 36.43907, Test Loss: 31.34737\n",
      "learning rate= 0.001\n",
      "Epoch 97, Train Loss: 24.92317, Test Loss: 28.00979\n",
      "learning rate= 0.001\n",
      "Epoch 98, Train Loss: 20.70170, Test Loss: 20.31763\n",
      "learning rate= 0.001\n",
      "Epoch 99, Train Loss: 17.89200, Test Loss: 22.94512\n",
      "learning rate= 0.001\n",
      "Epoch 100, Train Loss: 20.80827, Test Loss: 19.79867\n",
      "learning rate= 0.0005\n",
      "Epoch 101, Train Loss: 10.01439, Test Loss: 10.91522\n",
      "learning rate= 0.0005\n",
      "Epoch 102, Train Loss: 8.28524, Test Loss: 10.78554\n",
      "learning rate= 0.0005\n",
      "Epoch 103, Train Loss: 8.09824, Test Loss: 10.96826\n",
      "learning rate= 0.0005\n",
      "Epoch 104, Train Loss: 8.20920, Test Loss: 10.76613\n",
      "learning rate= 0.0005\n",
      "Epoch 105, Train Loss: 7.94442, Test Loss: 10.91641\n",
      "learning rate= 0.0005\n",
      "Epoch 106, Train Loss: 7.82286, Test Loss: 10.25144\n",
      "learning rate= 0.0005\n",
      "Epoch 107, Train Loss: 7.85143, Test Loss: 10.74291\n",
      "learning rate= 0.0005\n",
      "Epoch 108, Train Loss: 7.75241, Test Loss: 9.72847\n",
      "learning rate= 0.0005\n",
      "Epoch 109, Train Loss: 8.44722, Test Loss: 10.40983\n",
      "learning rate= 0.0005\n",
      "Epoch 110, Train Loss: 8.49063, Test Loss: 11.67286\n",
      "learning rate= 0.0005\n",
      "Epoch 111, Train Loss: 8.30621, Test Loss: 11.11932\n",
      "learning rate= 0.0005\n",
      "Epoch 112, Train Loss: 7.93548, Test Loss: 9.98007\n",
      "learning rate= 0.0005\n",
      "Epoch 113, Train Loss: 8.41371, Test Loss: 10.68142\n",
      "learning rate= 0.0005\n",
      "Epoch 114, Train Loss: 7.60548, Test Loss: 9.69927\n",
      "learning rate= 0.0005\n",
      "Epoch 115, Train Loss: 7.72267, Test Loss: 11.55253\n",
      "learning rate= 0.0005\n",
      "Epoch 116, Train Loss: 8.69132, Test Loss: 9.42753\n",
      "learning rate= 0.0005\n",
      "Epoch 117, Train Loss: 8.17038, Test Loss: 10.42472\n",
      "learning rate= 0.0005\n",
      "Epoch 118, Train Loss: 9.16805, Test Loss: 11.15771\n",
      "learning rate= 0.0005\n",
      "Epoch 119, Train Loss: 9.23789, Test Loss: 10.74995\n",
      "learning rate= 0.0005\n",
      "Epoch 120, Train Loss: 7.90417, Test Loss: 12.14272\n",
      "learning rate= 0.0005\n",
      "Epoch 121, Train Loss: 11.67196, Test Loss: 12.58536\n",
      "learning rate= 0.0005\n",
      "Epoch 122, Train Loss: 8.92026, Test Loss: 9.53511\n",
      "learning rate= 0.0005\n",
      "Epoch 123, Train Loss: 8.38558, Test Loss: 11.93367\n",
      "learning rate= 0.0005\n",
      "Epoch 124, Train Loss: 8.95676, Test Loss: 10.13444\n",
      "learning rate= 0.0005\n",
      "Epoch 125, Train Loss: 7.61449, Test Loss: 10.65216\n",
      "learning rate= 0.0005\n",
      "Epoch 126, Train Loss: 8.98797, Test Loss: 13.86850\n",
      "learning rate= 0.0005\n",
      "Epoch 127, Train Loss: 8.98564, Test Loss: 11.41145\n",
      "learning rate= 0.0005\n",
      "Epoch 128, Train Loss: 9.38699, Test Loss: 8.94342\n",
      "learning rate= 0.0005\n",
      "Epoch 129, Train Loss: 9.73029, Test Loss: 10.87916\n",
      "learning rate= 0.0005\n",
      "Epoch 130, Train Loss: 11.86488, Test Loss: 16.28985\n",
      "learning rate= 0.0005\n",
      "Epoch 131, Train Loss: 11.59449, Test Loss: 10.44513\n",
      "learning rate= 0.0005\n",
      "Epoch 132, Train Loss: 9.79934, Test Loss: 16.26387\n",
      "learning rate= 0.0005\n",
      "Epoch 133, Train Loss: 20.86733, Test Loss: 46.74427\n",
      "learning rate= 0.0005\n",
      "Epoch 134, Train Loss: 25.96875, Test Loss: 18.56716\n",
      "learning rate= 0.0005\n",
      "Epoch 135, Train Loss: 10.55680, Test Loss: 9.55348\n",
      "learning rate= 0.0005\n",
      "Epoch 136, Train Loss: 7.46067, Test Loss: 8.24520\n",
      "learning rate= 0.0005\n",
      "Epoch 137, Train Loss: 6.42024, Test Loss: 8.88488\n",
      "learning rate= 0.0005\n",
      "Epoch 138, Train Loss: 6.17205, Test Loss: 7.71603\n",
      "learning rate= 0.0005\n",
      "Epoch 139, Train Loss: 6.22121, Test Loss: 8.63812\n",
      "learning rate= 0.0005\n",
      "Epoch 140, Train Loss: 7.35207, Test Loss: 10.84796\n",
      "learning rate= 0.0005\n",
      "Epoch 141, Train Loss: 8.67311, Test Loss: 10.37100\n",
      "learning rate= 0.0005\n",
      "Epoch 142, Train Loss: 9.27847, Test Loss: 12.07872\n",
      "learning rate= 0.0005\n",
      "Epoch 143, Train Loss: 9.37245, Test Loss: 14.25986\n",
      "learning rate= 0.0005\n",
      "Epoch 144, Train Loss: 12.79517, Test Loss: 12.63238\n",
      "learning rate= 0.0005\n",
      "Epoch 145, Train Loss: 20.04160, Test Loss: 20.40917\n",
      "learning rate= 0.0005\n",
      "Epoch 146, Train Loss: 24.67066, Test Loss: 28.29050\n",
      "learning rate= 0.0005\n",
      "Epoch 147, Train Loss: 19.27542, Test Loss: 14.10554\n",
      "learning rate= 0.0005\n",
      "Epoch 148, Train Loss: 7.20974, Test Loss: 7.67886\n",
      "learning rate= 0.0005\n",
      "Epoch 149, Train Loss: 6.05067, Test Loss: 6.81009\n",
      "learning rate= 0.0005\n",
      "Epoch 150, Train Loss: 5.66669, Test Loss: 6.40099\n",
      "learning rate= 0.0005\n",
      "Epoch 151, Train Loss: 5.07120, Test Loss: 6.80012\n",
      "learning rate= 0.0005\n",
      "Epoch 152, Train Loss: 5.41130, Test Loss: 6.88222\n",
      "learning rate= 0.0005\n",
      "Epoch 153, Train Loss: 5.53579, Test Loss: 8.09202\n",
      "learning rate= 0.0005\n",
      "Epoch 154, Train Loss: 6.96719, Test Loss: 7.04000\n",
      "learning rate= 0.0005\n",
      "Epoch 155, Train Loss: 6.85369, Test Loss: 7.94917\n",
      "learning rate= 0.0005\n",
      "Epoch 156, Train Loss: 6.98912, Test Loss: 11.52417\n",
      "learning rate= 0.0005\n",
      "Epoch 157, Train Loss: 13.79304, Test Loss: 13.75321\n",
      "learning rate= 0.0005\n",
      "Epoch 158, Train Loss: 12.55119, Test Loss: 18.05363\n",
      "learning rate= 0.0005\n",
      "Epoch 159, Train Loss: 17.09678, Test Loss: 13.98922\n",
      "learning rate= 0.0005\n",
      "Epoch 160, Train Loss: 10.44979, Test Loss: 16.97688\n",
      "learning rate= 0.0005\n",
      "Epoch 161, Train Loss: 10.77813, Test Loss: 7.72948\n",
      "learning rate= 0.0005\n",
      "Epoch 162, Train Loss: 6.27601, Test Loss: 6.61260\n",
      "learning rate= 0.0005\n",
      "Epoch 163, Train Loss: 5.21298, Test Loss: 7.10799\n",
      "learning rate= 0.0005\n",
      "Epoch 164, Train Loss: 6.54845, Test Loss: 7.46856\n",
      "learning rate= 0.0005\n",
      "Epoch 165, Train Loss: 5.33771, Test Loss: 5.51224\n",
      "learning rate= 0.0005\n",
      "Epoch 166, Train Loss: 5.00841, Test Loss: 6.39917\n",
      "learning rate= 0.0005\n",
      "Epoch 167, Train Loss: 5.97967, Test Loss: 7.62257\n",
      "learning rate= 0.0005\n",
      "Epoch 168, Train Loss: 8.31478, Test Loss: 8.29269\n",
      "learning rate= 0.0005\n",
      "Epoch 169, Train Loss: 9.38598, Test Loss: 15.47840\n",
      "learning rate= 0.0005\n",
      "Epoch 170, Train Loss: 22.37701, Test Loss: 26.08556\n",
      "learning rate= 0.0005\n",
      "Epoch 171, Train Loss: 14.52097, Test Loss: 13.61559\n",
      "learning rate= 0.0005\n",
      "Epoch 172, Train Loss: 7.58587, Test Loss: 9.22897\n",
      "learning rate= 0.0005\n",
      "Epoch 173, Train Loss: 5.52134, Test Loss: 7.74220\n",
      "learning rate= 0.0005\n",
      "Epoch 174, Train Loss: 5.86407, Test Loss: 8.51597\n",
      "learning rate= 0.0005\n",
      "Epoch 175, Train Loss: 5.60838, Test Loss: 6.91535\n",
      "learning rate= 0.0005\n",
      "Epoch 176, Train Loss: 4.99304, Test Loss: 6.14188\n",
      "learning rate= 0.0005\n",
      "Epoch 177, Train Loss: 4.61893, Test Loss: 5.64373\n",
      "learning rate= 0.0005\n",
      "Epoch 178, Train Loss: 4.99568, Test Loss: 7.54519\n",
      "learning rate= 0.0005\n",
      "Epoch 179, Train Loss: 7.42523, Test Loss: 9.17320\n",
      "learning rate= 0.0005\n",
      "Epoch 180, Train Loss: 13.16326, Test Loss: 12.88461\n",
      "learning rate= 0.0005\n",
      "Epoch 181, Train Loss: 12.53235, Test Loss: 10.74471\n",
      "learning rate= 0.0005\n",
      "Epoch 182, Train Loss: 6.61104, Test Loss: 8.42101\n",
      "learning rate= 0.0005\n",
      "Epoch 183, Train Loss: 7.01868, Test Loss: 6.24519\n",
      "learning rate= 0.0005\n",
      "Epoch 184, Train Loss: 6.08997, Test Loss: 8.18080\n",
      "learning rate= 0.0005\n",
      "Epoch 185, Train Loss: 6.60934, Test Loss: 10.09349\n",
      "learning rate= 0.0005\n",
      "Epoch 186, Train Loss: 8.93432, Test Loss: 12.23221\n",
      "learning rate= 0.0005\n",
      "Epoch 187, Train Loss: 18.40064, Test Loss: 21.73832\n",
      "learning rate= 0.0005\n",
      "Epoch 188, Train Loss: 12.79019, Test Loss: 9.13956\n",
      "learning rate= 0.0005\n",
      "Epoch 189, Train Loss: 7.13658, Test Loss: 5.93055\n",
      "learning rate= 0.0005\n",
      "Epoch 190, Train Loss: 4.62862, Test Loss: 5.16410\n",
      "learning rate= 0.0005\n",
      "Epoch 191, Train Loss: 3.38398, Test Loss: 4.61656\n",
      "learning rate= 0.0005\n",
      "Epoch 192, Train Loss: 4.54007, Test Loss: 7.74070\n",
      "learning rate= 0.0005\n",
      "Epoch 193, Train Loss: 5.09877, Test Loss: 5.69014\n",
      "learning rate= 0.0005\n",
      "Epoch 194, Train Loss: 3.82694, Test Loss: 4.68770\n",
      "learning rate= 0.0005\n",
      "Epoch 195, Train Loss: 4.06746, Test Loss: 5.53879\n",
      "learning rate= 0.0005\n",
      "Epoch 196, Train Loss: 5.67802, Test Loss: 7.61037\n",
      "learning rate= 0.0005\n",
      "Epoch 197, Train Loss: 6.36144, Test Loss: 7.31511\n",
      "learning rate= 0.0005\n",
      "Epoch 198, Train Loss: 7.47495, Test Loss: 8.12875\n",
      "learning rate= 0.0005\n",
      "Epoch 199, Train Loss: 5.61150, Test Loss: 7.18342\n",
      "learning rate= 0.0005\n",
      "Epoch 200, Train Loss: 8.30768, Test Loss: 12.79677\n",
      "learning rate= 0.00025\n",
      "Epoch 201, Train Loss: 4.30966, Test Loss: 3.78205\n",
      "learning rate= 0.00025\n",
      "Epoch 202, Train Loss: 2.17711, Test Loss: 3.02378\n",
      "learning rate= 0.00025\n",
      "Epoch 203, Train Loss: 1.91560, Test Loss: 3.16355\n",
      "learning rate= 0.00025\n",
      "Epoch 204, Train Loss: 1.86902, Test Loss: 2.94052\n",
      "learning rate= 0.00025\n",
      "Epoch 205, Train Loss: 1.97623, Test Loss: 3.15151\n",
      "learning rate= 0.00025\n",
      "Epoch 206, Train Loss: 1.98247, Test Loss: 3.14396\n",
      "learning rate= 0.00025\n",
      "Epoch 207, Train Loss: 2.02414, Test Loss: 3.11059\n",
      "learning rate= 0.00025\n",
      "Epoch 208, Train Loss: 2.27222, Test Loss: 3.11861\n",
      "learning rate= 0.00025\n",
      "Epoch 209, Train Loss: 2.06472, Test Loss: 3.38038\n",
      "learning rate= 0.00025\n",
      "Epoch 210, Train Loss: 2.15385, Test Loss: 3.65510\n",
      "learning rate= 0.00025\n",
      "Epoch 211, Train Loss: 2.08094, Test Loss: 3.25000\n",
      "learning rate= 0.00025\n",
      "Epoch 212, Train Loss: 2.01087, Test Loss: 3.01143\n",
      "learning rate= 0.00025\n",
      "Epoch 213, Train Loss: 2.16010, Test Loss: 3.38484\n",
      "learning rate= 0.00025\n",
      "Epoch 214, Train Loss: 2.17622, Test Loss: 2.86491\n",
      "learning rate= 0.00025\n",
      "Epoch 215, Train Loss: 2.13989, Test Loss: 3.17513\n",
      "learning rate= 0.00025\n",
      "Epoch 216, Train Loss: 2.17745, Test Loss: 3.16818\n",
      "learning rate= 0.00025\n",
      "Epoch 217, Train Loss: 2.33233, Test Loss: 3.31405\n",
      "learning rate= 0.00025\n",
      "Epoch 218, Train Loss: 2.52111, Test Loss: 3.69351\n",
      "learning rate= 0.00025\n",
      "Epoch 219, Train Loss: 2.57804, Test Loss: 3.47530\n",
      "learning rate= 0.00025\n",
      "Epoch 220, Train Loss: 2.63274, Test Loss: 3.64453\n",
      "learning rate= 0.00025\n",
      "Epoch 221, Train Loss: 3.13808, Test Loss: 4.00602\n",
      "learning rate= 0.00025\n",
      "Epoch 222, Train Loss: 3.30104, Test Loss: 3.91844\n",
      "learning rate= 0.00025\n",
      "Epoch 223, Train Loss: 2.62597, Test Loss: 4.69557\n",
      "learning rate= 0.00025\n",
      "Epoch 224, Train Loss: 3.75358, Test Loss: 4.90084\n",
      "learning rate= 0.00025\n",
      "Epoch 225, Train Loss: 4.53644, Test Loss: 4.87509\n",
      "learning rate= 0.00025\n",
      "Epoch 226, Train Loss: 4.27500, Test Loss: 7.71561\n",
      "learning rate= 0.00025\n",
      "Epoch 227, Train Loss: 5.78778, Test Loss: 4.06540\n",
      "learning rate= 0.00025\n",
      "Epoch 228, Train Loss: 2.45808, Test Loss: 2.73182\n",
      "learning rate= 0.00025\n",
      "Epoch 229, Train Loss: 2.07617, Test Loss: 3.19785\n",
      "learning rate= 0.00025\n",
      "Epoch 230, Train Loss: 2.11533, Test Loss: 3.02340\n",
      "learning rate= 0.00025\n",
      "Epoch 231, Train Loss: 2.12243, Test Loss: 3.09381\n",
      "learning rate= 0.00025\n",
      "Epoch 232, Train Loss: 2.47743, Test Loss: 3.54291\n",
      "learning rate= 0.00025\n",
      "Epoch 233, Train Loss: 2.68353, Test Loss: 3.65702\n",
      "learning rate= 0.00025\n",
      "Epoch 234, Train Loss: 2.38587, Test Loss: 3.17747\n",
      "learning rate= 0.00025\n",
      "Epoch 235, Train Loss: 2.32372, Test Loss: 3.44272\n",
      "learning rate= 0.00025\n",
      "Epoch 236, Train Loss: 2.65180, Test Loss: 3.64188\n",
      "learning rate= 0.00025\n",
      "Epoch 237, Train Loss: 2.49358, Test Loss: 3.21091\n",
      "learning rate= 0.00025\n",
      "Epoch 238, Train Loss: 3.06450, Test Loss: 4.60632\n",
      "learning rate= 0.00025\n",
      "Epoch 239, Train Loss: 2.88352, Test Loss: 3.56694\n",
      "learning rate= 0.00025\n",
      "Epoch 240, Train Loss: 3.49856, Test Loss: 5.01487\n",
      "learning rate= 0.00025\n",
      "Epoch 241, Train Loss: 2.78662, Test Loss: 3.22306\n",
      "learning rate= 0.00025\n",
      "Epoch 242, Train Loss: 2.06152, Test Loss: 3.08442\n",
      "learning rate= 0.00025\n",
      "Epoch 243, Train Loss: 2.32300, Test Loss: 3.35203\n",
      "learning rate= 0.00025\n",
      "Epoch 244, Train Loss: 2.84643, Test Loss: 4.63878\n",
      "learning rate= 0.00025\n",
      "Epoch 245, Train Loss: 2.71347, Test Loss: 3.59046\n",
      "learning rate= 0.00025\n",
      "Epoch 246, Train Loss: 2.53988, Test Loss: 3.14734\n",
      "learning rate= 0.00025\n",
      "Epoch 247, Train Loss: 2.44506, Test Loss: 3.58447\n",
      "learning rate= 0.00025\n",
      "Epoch 248, Train Loss: 2.57744, Test Loss: 4.79047\n",
      "learning rate= 0.00025\n",
      "Epoch 249, Train Loss: 3.37472, Test Loss: 4.53084\n",
      "learning rate= 0.00025\n",
      "Epoch 250, Train Loss: 2.60274, Test Loss: 3.34249\n",
      "learning rate= 0.00025\n",
      "Epoch 251, Train Loss: 2.15713, Test Loss: 2.97737\n",
      "learning rate= 0.00025\n",
      "Epoch 252, Train Loss: 2.79633, Test Loss: 3.80680\n",
      "learning rate= 0.00025\n",
      "Epoch 253, Train Loss: 2.79313, Test Loss: 4.45712\n",
      "learning rate= 0.00025\n",
      "Epoch 254, Train Loss: 3.67148, Test Loss: 3.89496\n",
      "learning rate= 0.00025\n",
      "Epoch 255, Train Loss: 2.96166, Test Loss: 4.53176\n",
      "learning rate= 0.00025\n",
      "Epoch 256, Train Loss: 2.51287, Test Loss: 3.21594\n",
      "learning rate= 0.00025\n",
      "Epoch 257, Train Loss: 2.00712, Test Loss: 3.10535\n",
      "learning rate= 0.00025\n",
      "Epoch 258, Train Loss: 2.26859, Test Loss: 3.19894\n",
      "learning rate= 0.00025\n",
      "Epoch 259, Train Loss: 2.39116, Test Loss: 4.25046\n",
      "learning rate= 0.00025\n",
      "Epoch 260, Train Loss: 2.13890, Test Loss: 3.06730\n",
      "learning rate= 0.00025\n",
      "Epoch 261, Train Loss: 2.12613, Test Loss: 2.73298\n",
      "learning rate= 0.00025\n",
      "Epoch 262, Train Loss: 3.10859, Test Loss: 3.93564\n",
      "learning rate= 0.00025\n",
      "Epoch 263, Train Loss: 3.62038, Test Loss: 3.52748\n",
      "learning rate= 0.00025\n",
      "Epoch 264, Train Loss: 3.89605, Test Loss: 4.77509\n",
      "learning rate= 0.00025\n",
      "Epoch 265, Train Loss: 3.57189, Test Loss: 3.61420\n",
      "learning rate= 0.00025\n",
      "Epoch 266, Train Loss: 2.24133, Test Loss: 3.04281\n",
      "learning rate= 0.00025\n",
      "Epoch 267, Train Loss: 1.95445, Test Loss: 2.83777\n",
      "learning rate= 0.00025\n",
      "Epoch 268, Train Loss: 1.82468, Test Loss: 2.64083\n",
      "learning rate= 0.00025\n",
      "Epoch 269, Train Loss: 1.96252, Test Loss: 3.12768\n",
      "learning rate= 0.00025\n",
      "Epoch 270, Train Loss: 1.97443, Test Loss: 2.69594\n",
      "learning rate= 0.00025\n",
      "Epoch 271, Train Loss: 1.96142, Test Loss: 2.63124\n",
      "learning rate= 0.00025\n",
      "Epoch 272, Train Loss: 1.96243, Test Loss: 2.46568\n",
      "learning rate= 0.00025\n",
      "Epoch 273, Train Loss: 2.07153, Test Loss: 4.04534\n",
      "learning rate= 0.00025\n",
      "Epoch 274, Train Loss: 2.73869, Test Loss: 3.59775\n",
      "learning rate= 0.00025\n",
      "Epoch 275, Train Loss: 2.81765, Test Loss: 4.18999\n",
      "learning rate= 0.00025\n",
      "Epoch 276, Train Loss: 2.88190, Test Loss: 3.69223\n",
      "learning rate= 0.00025\n",
      "Epoch 277, Train Loss: 3.87029, Test Loss: 5.17321\n",
      "learning rate= 0.00025\n",
      "Epoch 278, Train Loss: 4.37959, Test Loss: 4.17581\n",
      "learning rate= 0.00025\n",
      "Epoch 279, Train Loss: 2.39299, Test Loss: 2.61294\n",
      "learning rate= 0.00025\n",
      "Epoch 280, Train Loss: 1.65996, Test Loss: 2.69599\n",
      "learning rate= 0.00025\n",
      "Epoch 281, Train Loss: 1.58396, Test Loss: 3.44919\n",
      "learning rate= 0.00025\n",
      "Epoch 282, Train Loss: 1.78161, Test Loss: 2.72377\n",
      "learning rate= 0.00025\n",
      "Epoch 283, Train Loss: 1.85652, Test Loss: 3.35162\n",
      "learning rate= 0.00025\n",
      "Epoch 284, Train Loss: 1.87762, Test Loss: 3.09367\n",
      "learning rate= 0.00025\n",
      "Epoch 285, Train Loss: 4.81795, Test Loss: 9.48490\n",
      "learning rate= 0.00025\n",
      "Epoch 286, Train Loss: 5.27874, Test Loss: 4.08864\n",
      "learning rate= 0.00025\n",
      "Epoch 287, Train Loss: 3.05468, Test Loss: 3.05122\n",
      "learning rate= 0.00025\n",
      "Epoch 288, Train Loss: 2.33680, Test Loss: 3.29349\n",
      "learning rate= 0.00025\n",
      "Epoch 289, Train Loss: 2.10052, Test Loss: 2.86149\n",
      "learning rate= 0.00025\n",
      "Epoch 290, Train Loss: 3.49237, Test Loss: 4.18212\n",
      "learning rate= 0.00025\n",
      "Epoch 291, Train Loss: 2.35649, Test Loss: 2.43544\n",
      "learning rate= 0.00025\n",
      "Epoch 292, Train Loss: 1.60450, Test Loss: 2.13168\n",
      "learning rate= 0.00025\n",
      "Epoch 293, Train Loss: 1.56043, Test Loss: 2.19035\n",
      "learning rate= 0.00025\n",
      "Epoch 294, Train Loss: 1.52202, Test Loss: 2.57671\n",
      "learning rate= 0.00025\n",
      "Epoch 295, Train Loss: 1.87693, Test Loss: 2.49313\n",
      "learning rate= 0.00025\n",
      "Epoch 296, Train Loss: 1.75186, Test Loss: 2.31520\n",
      "learning rate= 0.00025\n",
      "Epoch 297, Train Loss: 1.61306, Test Loss: 3.61135\n",
      "learning rate= 0.00025\n",
      "Epoch 298, Train Loss: 2.11475, Test Loss: 3.72253\n",
      "learning rate= 0.00025\n",
      "Epoch 299, Train Loss: 2.45323, Test Loss: 3.08107\n",
      "learning rate= 0.00025\n",
      "Epoch 300, Train Loss: 2.72437, Test Loss: 3.63178\n",
      "learning rate= 0.000125\n",
      "Epoch 301, Train Loss: 1.26715, Test Loss: 1.58340\n",
      "learning rate= 0.000125\n",
      "Epoch 302, Train Loss: 0.92451, Test Loss: 1.59480\n",
      "learning rate= 0.000125\n",
      "Epoch 303, Train Loss: 0.90801, Test Loss: 1.61385\n",
      "learning rate= 0.000125\n",
      "Epoch 304, Train Loss: 0.91920, Test Loss: 1.65652\n",
      "learning rate= 0.000125\n",
      "Epoch 305, Train Loss: 0.89227, Test Loss: 1.57315\n",
      "learning rate= 0.000125\n",
      "Epoch 306, Train Loss: 0.90546, Test Loss: 1.61739\n",
      "learning rate= 0.000125\n",
      "Epoch 307, Train Loss: 0.92845, Test Loss: 1.58678\n",
      "learning rate= 0.000125\n",
      "Epoch 308, Train Loss: 0.92512, Test Loss: 1.66000\n",
      "learning rate= 0.000125\n",
      "Epoch 309, Train Loss: 0.95641, Test Loss: 1.60167\n",
      "learning rate= 0.000125\n",
      "Epoch 310, Train Loss: 0.95168, Test Loss: 1.67501\n",
      "learning rate= 0.000125\n",
      "Epoch 311, Train Loss: 0.99298, Test Loss: 1.72693\n",
      "learning rate= 0.000125\n",
      "Epoch 312, Train Loss: 0.97263, Test Loss: 1.60453\n",
      "learning rate= 0.000125\n",
      "Epoch 313, Train Loss: 0.94838, Test Loss: 1.60980\n",
      "learning rate= 0.000125\n",
      "Epoch 314, Train Loss: 0.99399, Test Loss: 1.62181\n",
      "learning rate= 0.000125\n",
      "Epoch 315, Train Loss: 1.03006, Test Loss: 1.77487\n",
      "learning rate= 0.000125\n",
      "Epoch 316, Train Loss: 1.01909, Test Loss: 1.67246\n",
      "learning rate= 0.000125\n",
      "Epoch 317, Train Loss: 1.05195, Test Loss: 1.64613\n",
      "learning rate= 0.000125\n",
      "Epoch 318, Train Loss: 1.03379, Test Loss: 1.71874\n",
      "learning rate= 0.000125\n",
      "Epoch 319, Train Loss: 1.05622, Test Loss: 1.77468\n",
      "learning rate= 0.000125\n",
      "Epoch 320, Train Loss: 1.06481, Test Loss: 1.59226\n",
      "learning rate= 0.000125\n",
      "Epoch 321, Train Loss: 1.15307, Test Loss: 1.82287\n",
      "learning rate= 0.000125\n",
      "Epoch 322, Train Loss: 1.25004, Test Loss: 1.82198\n",
      "learning rate= 0.000125\n",
      "Epoch 323, Train Loss: 1.30197, Test Loss: 1.70794\n",
      "learning rate= 0.000125\n",
      "Epoch 324, Train Loss: 1.02582, Test Loss: 1.60464\n",
      "learning rate= 0.000125\n",
      "Epoch 325, Train Loss: 1.21727, Test Loss: 1.73970\n",
      "learning rate= 0.000125\n",
      "Epoch 326, Train Loss: 1.12307, Test Loss: 1.80272\n",
      "learning rate= 0.000125\n",
      "Epoch 327, Train Loss: 1.13785, Test Loss: 1.68652\n",
      "learning rate= 0.000125\n",
      "Epoch 328, Train Loss: 1.16397, Test Loss: 1.67096\n",
      "learning rate= 0.000125\n",
      "Epoch 329, Train Loss: 1.12305, Test Loss: 1.72159\n",
      "learning rate= 0.000125\n",
      "Epoch 330, Train Loss: 1.07145, Test Loss: 1.79446\n",
      "learning rate= 0.000125\n",
      "Epoch 331, Train Loss: 1.07523, Test Loss: 1.76928\n",
      "learning rate= 0.000125\n",
      "Epoch 332, Train Loss: 1.03699, Test Loss: 1.86424\n",
      "learning rate= 0.000125\n",
      "Epoch 333, Train Loss: 1.14011, Test Loss: 1.95115\n",
      "learning rate= 0.000125\n",
      "Epoch 334, Train Loss: 1.22717, Test Loss: 1.73580\n",
      "learning rate= 0.000125\n",
      "Epoch 335, Train Loss: 1.04906, Test Loss: 1.54833\n",
      "learning rate= 0.000125\n",
      "Epoch 336, Train Loss: 1.18305, Test Loss: 1.67737\n",
      "learning rate= 0.000125\n",
      "Epoch 337, Train Loss: 1.00978, Test Loss: 1.60043\n",
      "learning rate= 0.000125\n",
      "Epoch 338, Train Loss: 1.13581, Test Loss: 1.71086\n",
      "learning rate= 0.000125\n",
      "Epoch 339, Train Loss: 1.46018, Test Loss: 2.24407\n",
      "learning rate= 0.000125\n",
      "Epoch 340, Train Loss: 1.25427, Test Loss: 1.69315\n",
      "learning rate= 0.000125\n",
      "Epoch 341, Train Loss: 1.01704, Test Loss: 1.59881\n",
      "learning rate= 0.000125\n",
      "Epoch 342, Train Loss: 1.00577, Test Loss: 1.73375\n",
      "learning rate= 0.000125\n",
      "Epoch 343, Train Loss: 1.10200, Test Loss: 1.57515\n",
      "learning rate= 0.000125\n",
      "Epoch 344, Train Loss: 1.11323, Test Loss: 1.75778\n",
      "learning rate= 0.000125\n",
      "Epoch 345, Train Loss: 1.16296, Test Loss: 1.80500\n",
      "learning rate= 0.000125\n",
      "Epoch 346, Train Loss: 1.16450, Test Loss: 1.67382\n",
      "learning rate= 0.000125\n",
      "Epoch 347, Train Loss: 1.11242, Test Loss: 1.67319\n",
      "learning rate= 0.000125\n",
      "Epoch 348, Train Loss: 1.15011, Test Loss: 1.77142\n",
      "learning rate= 0.000125\n",
      "Epoch 349, Train Loss: 1.17985, Test Loss: 1.48643\n",
      "learning rate= 0.000125\n",
      "Epoch 350, Train Loss: 0.98770, Test Loss: 1.66615\n",
      "learning rate= 0.000125\n",
      "Epoch 351, Train Loss: 1.03443, Test Loss: 1.50558\n",
      "learning rate= 0.000125\n",
      "Epoch 352, Train Loss: 0.95610, Test Loss: 1.45821\n",
      "learning rate= 0.000125\n",
      "Epoch 353, Train Loss: 0.95982, Test Loss: 1.71122\n",
      "learning rate= 0.000125\n",
      "Epoch 354, Train Loss: 0.95116, Test Loss: 1.97825\n",
      "learning rate= 0.000125\n",
      "Epoch 355, Train Loss: 1.10816, Test Loss: 1.54914\n",
      "learning rate= 0.000125\n",
      "Epoch 356, Train Loss: 0.97986, Test Loss: 1.47866\n",
      "learning rate= 0.000125\n",
      "Epoch 357, Train Loss: 1.12584, Test Loss: 1.80279\n",
      "learning rate= 0.000125\n",
      "Epoch 358, Train Loss: 1.19016, Test Loss: 1.77245\n",
      "learning rate= 0.000125\n",
      "Epoch 359, Train Loss: 1.10123, Test Loss: 1.55097\n",
      "learning rate= 0.000125\n",
      "Epoch 360, Train Loss: 1.00779, Test Loss: 1.51481\n",
      "learning rate= 0.000125\n",
      "Epoch 361, Train Loss: 0.95447, Test Loss: 1.60982\n",
      "learning rate= 0.000125\n",
      "Epoch 362, Train Loss: 1.26359, Test Loss: 1.65623\n",
      "learning rate= 0.000125\n",
      "Epoch 363, Train Loss: 0.98477, Test Loss: 1.54449\n",
      "learning rate= 0.000125\n",
      "Epoch 364, Train Loss: 0.96771, Test Loss: 1.57080\n",
      "learning rate= 0.000125\n",
      "Epoch 365, Train Loss: 1.03601, Test Loss: 1.73434\n",
      "learning rate= 0.000125\n",
      "Epoch 366, Train Loss: 1.05794, Test Loss: 2.53236\n",
      "learning rate= 0.000125\n",
      "Epoch 367, Train Loss: 1.65687, Test Loss: 1.68683\n",
      "learning rate= 0.000125\n",
      "Epoch 368, Train Loss: 1.33222, Test Loss: 1.65532\n",
      "learning rate= 0.000125\n",
      "Epoch 369, Train Loss: 1.14697, Test Loss: 1.47424\n",
      "learning rate= 0.000125\n",
      "Epoch 370, Train Loss: 0.91658, Test Loss: 1.63244\n",
      "learning rate= 0.000125\n",
      "Epoch 371, Train Loss: 0.96054, Test Loss: 1.42736\n",
      "learning rate= 0.000125\n",
      "Epoch 372, Train Loss: 0.86955, Test Loss: 1.37126\n",
      "learning rate= 0.000125\n",
      "Epoch 373, Train Loss: 0.95309, Test Loss: 1.43752\n",
      "learning rate= 0.000125\n",
      "Epoch 374, Train Loss: 1.01016, Test Loss: 2.10270\n",
      "learning rate= 0.000125\n",
      "Epoch 375, Train Loss: 1.17447, Test Loss: 1.58421\n",
      "learning rate= 0.000125\n",
      "Epoch 376, Train Loss: 0.98741, Test Loss: 1.58725\n",
      "learning rate= 0.000125\n",
      "Epoch 377, Train Loss: 0.89803, Test Loss: 1.53748\n",
      "learning rate= 0.000125\n",
      "Epoch 378, Train Loss: 0.84650, Test Loss: 1.54096\n",
      "learning rate= 0.000125\n",
      "Epoch 379, Train Loss: 1.14982, Test Loss: 2.23966\n",
      "learning rate= 0.000125\n",
      "Epoch 380, Train Loss: 1.45626, Test Loss: 1.81270\n",
      "learning rate= 0.000125\n",
      "Epoch 381, Train Loss: 1.15400, Test Loss: 1.73335\n",
      "learning rate= 0.000125\n",
      "Epoch 382, Train Loss: 0.93855, Test Loss: 1.35229\n",
      "learning rate= 0.000125\n",
      "Epoch 383, Train Loss: 0.88785, Test Loss: 1.50299\n",
      "learning rate= 0.000125\n",
      "Epoch 384, Train Loss: 0.92422, Test Loss: 1.75047\n",
      "learning rate= 0.000125\n",
      "Epoch 385, Train Loss: 0.93963, Test Loss: 1.56848\n",
      "learning rate= 0.000125\n",
      "Epoch 386, Train Loss: 0.93211, Test Loss: 1.51857\n",
      "learning rate= 0.000125\n",
      "Epoch 387, Train Loss: 1.08734, Test Loss: 1.57873\n",
      "learning rate= 0.000125\n",
      "Epoch 388, Train Loss: 1.18807, Test Loss: 1.62473\n",
      "learning rate= 0.000125\n",
      "Epoch 389, Train Loss: 0.92327, Test Loss: 1.42035\n",
      "learning rate= 0.000125\n",
      "Epoch 390, Train Loss: 0.91265, Test Loss: 1.51710\n",
      "learning rate= 0.000125\n",
      "Epoch 391, Train Loss: 0.94497, Test Loss: 1.49599\n",
      "learning rate= 0.000125\n",
      "Epoch 392, Train Loss: 0.99025, Test Loss: 1.55152\n",
      "learning rate= 0.000125\n",
      "Epoch 393, Train Loss: 1.04333, Test Loss: 1.77153\n",
      "learning rate= 0.000125\n",
      "Epoch 394, Train Loss: 0.91475, Test Loss: 1.49001\n",
      "learning rate= 0.000125\n",
      "Epoch 395, Train Loss: 0.98622, Test Loss: 1.67588\n",
      "learning rate= 0.000125\n",
      "Epoch 396, Train Loss: 0.85362, Test Loss: 1.56792\n",
      "learning rate= 0.000125\n",
      "Epoch 397, Train Loss: 0.94479, Test Loss: 1.86886\n",
      "learning rate= 0.000125\n",
      "Epoch 398, Train Loss: 1.22714, Test Loss: 1.58387\n",
      "learning rate= 0.000125\n",
      "Epoch 399, Train Loss: 0.85035, Test Loss: 1.28929\n",
      "learning rate= 0.000125\n",
      "Epoch 400, Train Loss: 0.79362, Test Loss: 1.31577\n",
      "learning rate= 6.25e-05\n",
      "Epoch 401, Train Loss: 0.60536, Test Loss: 1.07493\n",
      "learning rate= 6.25e-05\n",
      "Epoch 402, Train Loss: 0.56618, Test Loss: 1.13372\n",
      "learning rate= 6.25e-05\n",
      "Epoch 403, Train Loss: 0.56798, Test Loss: 1.08734\n",
      "learning rate= 6.25e-05\n",
      "Epoch 404, Train Loss: 0.58065, Test Loss: 1.10403\n",
      "learning rate= 6.25e-05\n",
      "Epoch 405, Train Loss: 0.57727, Test Loss: 1.09556\n",
      "learning rate= 6.25e-05\n",
      "Epoch 406, Train Loss: 0.59531, Test Loss: 1.12974\n",
      "learning rate= 6.25e-05\n",
      "Epoch 407, Train Loss: 0.57148, Test Loss: 1.09622\n",
      "learning rate= 6.25e-05\n",
      "Epoch 408, Train Loss: 0.59122, Test Loss: 1.14949\n",
      "learning rate= 6.25e-05\n",
      "Epoch 409, Train Loss: 0.59220, Test Loss: 1.10698\n",
      "learning rate= 6.25e-05\n",
      "Epoch 410, Train Loss: 0.60796, Test Loss: 1.15835\n",
      "learning rate= 6.25e-05\n",
      "Epoch 411, Train Loss: 0.60111, Test Loss: 1.12327\n",
      "learning rate= 6.25e-05\n",
      "Epoch 412, Train Loss: 0.58512, Test Loss: 1.09302\n",
      "learning rate= 6.25e-05\n",
      "Epoch 413, Train Loss: 0.58790, Test Loss: 1.13933\n",
      "learning rate= 6.25e-05\n",
      "Epoch 414, Train Loss: 0.59397, Test Loss: 1.15407\n",
      "learning rate= 6.25e-05\n",
      "Epoch 415, Train Loss: 0.60936, Test Loss: 1.16135\n",
      "learning rate= 6.25e-05\n",
      "Epoch 416, Train Loss: 0.61906, Test Loss: 1.09760\n",
      "learning rate= 6.25e-05\n",
      "Epoch 417, Train Loss: 0.58743, Test Loss: 1.13248\n",
      "learning rate= 6.25e-05\n",
      "Epoch 418, Train Loss: 0.60726, Test Loss: 1.10002\n",
      "learning rate= 6.25e-05\n",
      "Epoch 419, Train Loss: 0.60867, Test Loss: 1.20884\n",
      "learning rate= 6.25e-05\n",
      "Epoch 420, Train Loss: 0.63644, Test Loss: 1.15645\n",
      "learning rate= 6.25e-05\n",
      "Epoch 421, Train Loss: 0.64861, Test Loss: 1.14596\n",
      "learning rate= 6.25e-05\n",
      "Epoch 422, Train Loss: 0.59473, Test Loss: 1.10435\n",
      "learning rate= 6.25e-05\n",
      "Epoch 423, Train Loss: 0.58723, Test Loss: 1.19742\n",
      "learning rate= 6.25e-05\n",
      "Epoch 424, Train Loss: 0.62680, Test Loss: 1.06227\n",
      "learning rate= 6.25e-05\n",
      "Epoch 425, Train Loss: 0.63088, Test Loss: 1.12044\n",
      "learning rate= 6.25e-05\n",
      "Epoch 426, Train Loss: 0.63845, Test Loss: 1.32400\n",
      "learning rate= 6.25e-05\n",
      "Epoch 427, Train Loss: 0.66023, Test Loss: 1.11894\n",
      "learning rate= 6.25e-05\n",
      "Epoch 428, Train Loss: 0.60387, Test Loss: 1.13453\n",
      "learning rate= 6.25e-05\n",
      "Epoch 429, Train Loss: 0.62917, Test Loss: 1.20719\n",
      "learning rate= 6.25e-05\n",
      "Epoch 430, Train Loss: 0.76870, Test Loss: 1.44009\n",
      "learning rate= 6.25e-05\n",
      "Epoch 431, Train Loss: 0.71256, Test Loss: 1.18074\n",
      "learning rate= 6.25e-05\n",
      "Epoch 432, Train Loss: 0.64892, Test Loss: 1.17610\n",
      "learning rate= 6.25e-05\n",
      "Epoch 433, Train Loss: 0.60669, Test Loss: 1.09222\n",
      "learning rate= 6.25e-05\n",
      "Epoch 434, Train Loss: 0.58950, Test Loss: 1.13804\n",
      "learning rate= 6.25e-05\n",
      "Epoch 435, Train Loss: 0.59931, Test Loss: 1.12736\n",
      "learning rate= 6.25e-05\n",
      "Epoch 436, Train Loss: 0.61762, Test Loss: 1.15027\n",
      "learning rate= 6.25e-05\n",
      "Epoch 437, Train Loss: 0.59896, Test Loss: 1.24071\n",
      "learning rate= 6.25e-05\n",
      "Epoch 438, Train Loss: 0.61041, Test Loss: 1.20030\n",
      "learning rate= 6.25e-05\n",
      "Epoch 439, Train Loss: 0.60668, Test Loss: 1.08147\n",
      "learning rate= 6.25e-05\n",
      "Epoch 440, Train Loss: 0.57907, Test Loss: 1.05097\n",
      "learning rate= 6.25e-05\n",
      "Epoch 441, Train Loss: 0.59243, Test Loss: 1.07610\n",
      "learning rate= 6.25e-05\n",
      "Epoch 442, Train Loss: 0.59050, Test Loss: 1.11003\n",
      "learning rate= 6.25e-05\n",
      "Epoch 443, Train Loss: 0.63811, Test Loss: 1.07182\n",
      "learning rate= 6.25e-05\n",
      "Epoch 444, Train Loss: 0.58507, Test Loss: 1.05854\n",
      "learning rate= 6.25e-05\n",
      "Epoch 445, Train Loss: 0.60926, Test Loss: 1.13689\n",
      "learning rate= 6.25e-05\n",
      "Epoch 446, Train Loss: 0.61118, Test Loss: 1.09138\n",
      "learning rate= 6.25e-05\n",
      "Epoch 447, Train Loss: 0.57411, Test Loss: 1.09195\n",
      "learning rate= 6.25e-05\n",
      "Epoch 448, Train Loss: 0.57324, Test Loss: 1.09627\n",
      "learning rate= 6.25e-05\n",
      "Epoch 449, Train Loss: 0.58233, Test Loss: 1.10049\n",
      "learning rate= 6.25e-05\n",
      "Epoch 450, Train Loss: 0.58976, Test Loss: 1.06065\n",
      "learning rate= 6.25e-05\n",
      "Epoch 451, Train Loss: 0.64546, Test Loss: 1.15514\n",
      "learning rate= 6.25e-05\n",
      "Epoch 452, Train Loss: 0.60694, Test Loss: 1.09186\n",
      "learning rate= 6.25e-05\n",
      "Epoch 453, Train Loss: 0.59826, Test Loss: 1.13652\n",
      "learning rate= 6.25e-05\n",
      "Epoch 454, Train Loss: 0.61954, Test Loss: 1.04251\n",
      "learning rate= 6.25e-05\n",
      "Epoch 455, Train Loss: 0.58120, Test Loss: 1.17592\n",
      "learning rate= 6.25e-05\n",
      "Epoch 456, Train Loss: 0.63049, Test Loss: 1.06086\n",
      "learning rate= 6.25e-05\n",
      "Epoch 457, Train Loss: 0.61128, Test Loss: 1.09933\n",
      "learning rate= 6.25e-05\n",
      "Epoch 458, Train Loss: 0.56735, Test Loss: 1.09217\n",
      "learning rate= 6.25e-05\n",
      "Epoch 459, Train Loss: 0.58343, Test Loss: 1.11000\n",
      "learning rate= 6.25e-05\n",
      "Epoch 460, Train Loss: 0.64519, Test Loss: 1.11664\n",
      "learning rate= 6.25e-05\n",
      "Epoch 461, Train Loss: 0.58628, Test Loss: 1.11050\n",
      "learning rate= 6.25e-05\n",
      "Epoch 462, Train Loss: 0.56884, Test Loss: 1.07143\n",
      "learning rate= 6.25e-05\n",
      "Epoch 463, Train Loss: 0.56432, Test Loss: 1.07951\n",
      "learning rate= 6.25e-05\n",
      "Epoch 464, Train Loss: 0.55212, Test Loss: 1.05192\n",
      "learning rate= 6.25e-05\n",
      "Epoch 465, Train Loss: 0.58247, Test Loss: 1.05405\n",
      "learning rate= 6.25e-05\n",
      "Epoch 466, Train Loss: 0.56480, Test Loss: 1.04685\n",
      "learning rate= 6.25e-05\n",
      "Epoch 467, Train Loss: 0.58357, Test Loss: 1.07214\n",
      "learning rate= 6.25e-05\n",
      "Epoch 468, Train Loss: 0.59189, Test Loss: 1.27887\n",
      "learning rate= 6.25e-05\n",
      "Epoch 469, Train Loss: 0.60885, Test Loss: 1.12325\n",
      "learning rate= 6.25e-05\n",
      "Epoch 470, Train Loss: 0.58611, Test Loss: 1.04173\n",
      "learning rate= 6.25e-05\n",
      "Epoch 471, Train Loss: 0.61489, Test Loss: 1.01941\n",
      "learning rate= 6.25e-05\n",
      "Epoch 472, Train Loss: 0.56507, Test Loss: 1.03434\n",
      "learning rate= 6.25e-05\n",
      "Epoch 473, Train Loss: 0.58918, Test Loss: 1.15814\n",
      "learning rate= 6.25e-05\n",
      "Epoch 474, Train Loss: 0.57509, Test Loss: 1.03428\n",
      "learning rate= 6.25e-05\n",
      "Epoch 475, Train Loss: 0.54249, Test Loss: 1.08478\n",
      "learning rate= 6.25e-05\n",
      "Epoch 476, Train Loss: 0.54896, Test Loss: 0.99398\n",
      "learning rate= 6.25e-05\n",
      "Epoch 477, Train Loss: 0.59713, Test Loss: 1.08570\n",
      "learning rate= 6.25e-05\n",
      "Epoch 478, Train Loss: 0.56247, Test Loss: 0.99964\n",
      "learning rate= 6.25e-05\n",
      "Epoch 479, Train Loss: 0.55019, Test Loss: 1.01879\n",
      "learning rate= 6.25e-05\n",
      "Epoch 480, Train Loss: 0.55699, Test Loss: 1.02318\n",
      "learning rate= 6.25e-05\n",
      "Epoch 481, Train Loss: 0.57505, Test Loss: 1.00816\n",
      "learning rate= 6.25e-05\n",
      "Epoch 482, Train Loss: 0.58364, Test Loss: 1.02722\n",
      "learning rate= 6.25e-05\n",
      "Epoch 483, Train Loss: 0.59037, Test Loss: 1.03382\n",
      "learning rate= 6.25e-05\n",
      "Epoch 484, Train Loss: 0.56979, Test Loss: 0.97925\n",
      "learning rate= 6.25e-05\n",
      "Epoch 485, Train Loss: 0.54176, Test Loss: 1.06550\n",
      "learning rate= 6.25e-05\n",
      "Epoch 486, Train Loss: 0.53968, Test Loss: 1.01683\n",
      "learning rate= 6.25e-05\n",
      "Epoch 487, Train Loss: 0.55830, Test Loss: 1.03332\n",
      "learning rate= 6.25e-05\n",
      "Epoch 488, Train Loss: 0.59458, Test Loss: 1.10604\n",
      "learning rate= 6.25e-05\n",
      "Epoch 489, Train Loss: 0.62375, Test Loss: 1.02765\n",
      "learning rate= 6.25e-05\n",
      "Epoch 490, Train Loss: 0.54161, Test Loss: 1.00765\n",
      "learning rate= 6.25e-05\n",
      "Epoch 491, Train Loss: 0.52571, Test Loss: 1.04730\n",
      "learning rate= 6.25e-05\n",
      "Epoch 492, Train Loss: 0.55204, Test Loss: 0.98998\n",
      "learning rate= 6.25e-05\n",
      "Epoch 493, Train Loss: 0.51933, Test Loss: 0.97983\n",
      "learning rate= 6.25e-05\n",
      "Epoch 494, Train Loss: 0.54482, Test Loss: 0.99504\n",
      "learning rate= 6.25e-05\n",
      "Epoch 495, Train Loss: 0.56404, Test Loss: 1.07896\n",
      "learning rate= 6.25e-05\n",
      "Epoch 496, Train Loss: 0.57460, Test Loss: 0.98810\n",
      "learning rate= 6.25e-05\n",
      "Epoch 497, Train Loss: 0.53624, Test Loss: 1.00287\n",
      "learning rate= 6.25e-05\n",
      "Epoch 498, Train Loss: 0.54909, Test Loss: 1.05536\n",
      "learning rate= 6.25e-05\n",
      "Epoch 499, Train Loss: 0.58752, Test Loss: 1.09887\n",
      "learning rate= 6.25e-05\n",
      "Epoch 500, Train Loss: 0.60093, Test Loss: 1.00183\n",
      "learning rate= 3.125e-05\n",
      "Epoch 501, Train Loss: 0.44637, Test Loss: 0.87746\n",
      "learning rate= 3.125e-05\n",
      "Epoch 502, Train Loss: 0.42541, Test Loss: 0.87785\n",
      "learning rate= 3.125e-05\n",
      "Epoch 503, Train Loss: 0.42380, Test Loss: 0.87632\n",
      "learning rate= 3.125e-05\n",
      "Epoch 504, Train Loss: 0.42217, Test Loss: 0.88725\n",
      "learning rate= 3.125e-05\n",
      "Epoch 505, Train Loss: 0.42593, Test Loss: 0.88688\n",
      "learning rate= 3.125e-05\n",
      "Epoch 506, Train Loss: 0.42453, Test Loss: 0.87084\n",
      "learning rate= 3.125e-05\n",
      "Epoch 507, Train Loss: 0.42570, Test Loss: 0.88675\n",
      "learning rate= 3.125e-05\n",
      "Epoch 508, Train Loss: 0.43109, Test Loss: 0.90290\n",
      "learning rate= 3.125e-05\n",
      "Epoch 509, Train Loss: 0.42725, Test Loss: 0.87964\n",
      "learning rate= 3.125e-05\n",
      "Epoch 510, Train Loss: 0.42229, Test Loss: 0.88362\n",
      "learning rate= 3.125e-05\n",
      "Epoch 511, Train Loss: 0.43426, Test Loss: 0.89137\n",
      "learning rate= 3.125e-05\n",
      "Epoch 512, Train Loss: 0.43624, Test Loss: 0.90109\n",
      "learning rate= 3.125e-05\n",
      "Epoch 513, Train Loss: 0.43458, Test Loss: 0.88807\n",
      "learning rate= 3.125e-05\n",
      "Epoch 514, Train Loss: 0.44468, Test Loss: 0.92141\n",
      "learning rate= 3.125e-05\n",
      "Epoch 515, Train Loss: 0.43648, Test Loss: 0.89439\n",
      "learning rate= 3.125e-05\n",
      "Epoch 516, Train Loss: 0.42127, Test Loss: 0.90684\n",
      "learning rate= 3.125e-05\n",
      "Epoch 517, Train Loss: 0.43448, Test Loss: 0.89356\n",
      "learning rate= 3.125e-05\n",
      "Epoch 518, Train Loss: 0.43431, Test Loss: 0.88643\n",
      "learning rate= 3.125e-05\n",
      "Epoch 519, Train Loss: 0.43723, Test Loss: 0.91962\n",
      "learning rate= 3.125e-05\n",
      "Epoch 520, Train Loss: 0.42888, Test Loss: 0.87472\n",
      "learning rate= 3.125e-05\n",
      "Epoch 521, Train Loss: 0.43864, Test Loss: 0.86886\n",
      "learning rate= 3.125e-05\n",
      "Epoch 522, Train Loss: 0.43029, Test Loss: 0.87753\n",
      "learning rate= 3.125e-05\n",
      "Epoch 523, Train Loss: 0.43288, Test Loss: 0.90269\n",
      "learning rate= 3.125e-05\n",
      "Epoch 524, Train Loss: 0.44624, Test Loss: 0.90125\n",
      "learning rate= 3.125e-05\n",
      "Epoch 525, Train Loss: 0.43010, Test Loss: 0.87116\n",
      "learning rate= 3.125e-05\n",
      "Epoch 526, Train Loss: 0.43341, Test Loss: 0.88609\n",
      "learning rate= 3.125e-05\n",
      "Epoch 527, Train Loss: 0.43919, Test Loss: 0.86225\n",
      "learning rate= 3.125e-05\n",
      "Epoch 528, Train Loss: 0.43512, Test Loss: 0.88708\n",
      "learning rate= 3.125e-05\n",
      "Epoch 529, Train Loss: 0.43162, Test Loss: 0.86320\n",
      "learning rate= 3.125e-05\n",
      "Epoch 530, Train Loss: 0.42748, Test Loss: 0.87873\n",
      "learning rate= 3.125e-05\n",
      "Epoch 531, Train Loss: 0.44521, Test Loss: 0.89345\n",
      "learning rate= 3.125e-05\n",
      "Epoch 532, Train Loss: 0.43310, Test Loss: 0.87543\n",
      "learning rate= 3.125e-05\n",
      "Epoch 533, Train Loss: 0.43640, Test Loss: 0.86270\n",
      "learning rate= 3.125e-05\n",
      "Epoch 534, Train Loss: 0.43520, Test Loss: 0.87634\n",
      "learning rate= 3.125e-05\n",
      "Epoch 535, Train Loss: 0.42265, Test Loss: 0.85997\n",
      "learning rate= 3.125e-05\n",
      "Epoch 536, Train Loss: 0.42620, Test Loss: 0.87576\n",
      "learning rate= 3.125e-05\n",
      "Epoch 537, Train Loss: 0.42335, Test Loss: 0.89408\n",
      "learning rate= 3.125e-05\n",
      "Epoch 538, Train Loss: 0.43274, Test Loss: 0.85597\n",
      "learning rate= 3.125e-05\n",
      "Epoch 539, Train Loss: 0.42735, Test Loss: 0.86401\n",
      "learning rate= 3.125e-05\n",
      "Epoch 540, Train Loss: 0.42583, Test Loss: 0.87732\n",
      "learning rate= 3.125e-05\n",
      "Epoch 541, Train Loss: 0.43649, Test Loss: 0.85872\n",
      "learning rate= 3.125e-05\n",
      "Epoch 542, Train Loss: 0.42629, Test Loss: 0.86576\n",
      "learning rate= 3.125e-05\n",
      "Epoch 543, Train Loss: 0.42716, Test Loss: 0.89972\n",
      "learning rate= 3.125e-05\n",
      "Epoch 544, Train Loss: 0.42852, Test Loss: 0.87217\n",
      "learning rate= 3.125e-05\n",
      "Epoch 545, Train Loss: 0.43379, Test Loss: 0.86798\n",
      "learning rate= 3.125e-05\n",
      "Epoch 546, Train Loss: 0.42165, Test Loss: 0.86809\n",
      "learning rate= 3.125e-05\n",
      "Epoch 547, Train Loss: 0.42316, Test Loss: 0.89455\n",
      "learning rate= 3.125e-05\n",
      "Epoch 548, Train Loss: 0.41051, Test Loss: 0.85839\n",
      "learning rate= 3.125e-05\n",
      "Epoch 549, Train Loss: 0.42565, Test Loss: 0.84894\n",
      "learning rate= 3.125e-05\n",
      "Epoch 550, Train Loss: 0.41343, Test Loss: 0.91737\n",
      "learning rate= 3.125e-05\n",
      "Epoch 551, Train Loss: 0.43335, Test Loss: 0.86607\n",
      "learning rate= 3.125e-05\n",
      "Epoch 552, Train Loss: 0.42861, Test Loss: 0.88742\n",
      "learning rate= 3.125e-05\n",
      "Epoch 553, Train Loss: 0.43520, Test Loss: 0.84791\n",
      "learning rate= 3.125e-05\n",
      "Epoch 554, Train Loss: 0.41929, Test Loss: 0.84039\n",
      "learning rate= 3.125e-05\n",
      "Epoch 555, Train Loss: 0.41146, Test Loss: 0.90743\n",
      "learning rate= 3.125e-05\n",
      "Epoch 556, Train Loss: 0.41831, Test Loss: 0.88049\n",
      "learning rate= 3.125e-05\n",
      "Epoch 557, Train Loss: 0.41740, Test Loss: 0.88680\n",
      "learning rate= 3.125e-05\n",
      "Epoch 558, Train Loss: 0.42198, Test Loss: 0.84703\n",
      "learning rate= 3.125e-05\n",
      "Epoch 559, Train Loss: 0.41158, Test Loss: 0.87352\n",
      "learning rate= 3.125e-05\n",
      "Epoch 560, Train Loss: 0.41973, Test Loss: 0.86719\n",
      "learning rate= 3.125e-05\n",
      "Epoch 561, Train Loss: 0.41468, Test Loss: 0.85401\n",
      "learning rate= 3.125e-05\n",
      "Epoch 562, Train Loss: 0.41463, Test Loss: 0.87852\n",
      "learning rate= 3.125e-05\n",
      "Epoch 563, Train Loss: 0.41952, Test Loss: 0.88277\n",
      "learning rate= 3.125e-05\n",
      "Epoch 564, Train Loss: 0.42758, Test Loss: 0.86118\n",
      "learning rate= 3.125e-05\n",
      "Epoch 565, Train Loss: 0.41605, Test Loss: 0.84412\n",
      "learning rate= 3.125e-05\n",
      "Epoch 566, Train Loss: 0.42448, Test Loss: 0.86972\n",
      "learning rate= 3.125e-05\n",
      "Epoch 567, Train Loss: 0.41599, Test Loss: 0.85320\n",
      "learning rate= 3.125e-05\n",
      "Epoch 568, Train Loss: 0.42072, Test Loss: 0.86758\n",
      "learning rate= 3.125e-05\n",
      "Epoch 569, Train Loss: 0.41650, Test Loss: 0.84044\n",
      "learning rate= 3.125e-05\n",
      "Epoch 570, Train Loss: 0.41153, Test Loss: 0.84295\n",
      "learning rate= 3.125e-05\n",
      "Epoch 571, Train Loss: 0.41459, Test Loss: 0.85848\n",
      "learning rate= 3.125e-05\n",
      "Epoch 572, Train Loss: 0.41802, Test Loss: 0.84640\n",
      "learning rate= 3.125e-05\n",
      "Epoch 573, Train Loss: 0.40690, Test Loss: 0.83484\n",
      "learning rate= 3.125e-05\n",
      "Epoch 574, Train Loss: 0.41653, Test Loss: 0.85450\n",
      "learning rate= 3.125e-05\n",
      "Epoch 575, Train Loss: 0.40673, Test Loss: 0.83212\n",
      "learning rate= 3.125e-05\n",
      "Epoch 576, Train Loss: 0.40567, Test Loss: 0.82392\n",
      "learning rate= 3.125e-05\n",
      "Epoch 577, Train Loss: 0.40759, Test Loss: 0.86658\n",
      "learning rate= 3.125e-05\n",
      "Epoch 578, Train Loss: 0.41569, Test Loss: 0.84670\n",
      "learning rate= 3.125e-05\n",
      "Epoch 579, Train Loss: 0.40520, Test Loss: 0.83942\n",
      "learning rate= 3.125e-05\n",
      "Epoch 580, Train Loss: 0.40922, Test Loss: 0.82062\n",
      "learning rate= 3.125e-05\n",
      "Epoch 581, Train Loss: 0.41259, Test Loss: 0.84167\n",
      "learning rate= 3.125e-05\n",
      "Epoch 582, Train Loss: 0.40377, Test Loss: 0.83383\n",
      "learning rate= 3.125e-05\n",
      "Epoch 583, Train Loss: 0.40224, Test Loss: 0.83044\n",
      "learning rate= 3.125e-05\n",
      "Epoch 584, Train Loss: 0.40826, Test Loss: 0.87057\n",
      "learning rate= 3.125e-05\n",
      "Epoch 585, Train Loss: 0.40208, Test Loss: 0.84554\n",
      "learning rate= 3.125e-05\n",
      "Epoch 586, Train Loss: 0.40228, Test Loss: 0.81618\n",
      "learning rate= 3.125e-05\n",
      "Epoch 587, Train Loss: 0.40914, Test Loss: 0.85353\n",
      "learning rate= 3.125e-05\n",
      "Epoch 588, Train Loss: 0.40964, Test Loss: 0.81473\n",
      "learning rate= 3.125e-05\n",
      "Epoch 589, Train Loss: 0.40059, Test Loss: 0.83293\n",
      "learning rate= 3.125e-05\n",
      "Epoch 590, Train Loss: 0.40249, Test Loss: 0.81150\n",
      "learning rate= 3.125e-05\n",
      "Epoch 591, Train Loss: 0.39628, Test Loss: 0.82425\n",
      "learning rate= 3.125e-05\n",
      "Epoch 592, Train Loss: 0.40208, Test Loss: 0.82877\n",
      "learning rate= 3.125e-05\n",
      "Epoch 593, Train Loss: 0.39895, Test Loss: 0.81958\n",
      "learning rate= 3.125e-05\n",
      "Epoch 594, Train Loss: 0.39260, Test Loss: 0.81928\n",
      "learning rate= 3.125e-05\n",
      "Epoch 595, Train Loss: 0.39311, Test Loss: 0.79931\n",
      "learning rate= 3.125e-05\n",
      "Epoch 596, Train Loss: 0.40070, Test Loss: 0.82834\n",
      "learning rate= 3.125e-05\n",
      "Epoch 597, Train Loss: 0.39871, Test Loss: 0.82260\n",
      "learning rate= 3.125e-05\n",
      "Epoch 598, Train Loss: 0.39571, Test Loss: 0.82635\n",
      "learning rate= 3.125e-05\n",
      "Epoch 599, Train Loss: 0.39470, Test Loss: 0.84344\n",
      "learning rate= 3.125e-05\n",
      "Epoch 600, Train Loss: 0.40835, Test Loss: 0.82292\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 601, Train Loss: 0.36086, Test Loss: 0.78733\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 602, Train Loss: 0.34891, Test Loss: 0.78943\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 603, Train Loss: 0.34935, Test Loss: 0.77166\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 604, Train Loss: 0.35315, Test Loss: 0.77051\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 605, Train Loss: 0.35425, Test Loss: 0.77058\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 606, Train Loss: 0.34904, Test Loss: 0.77336\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 607, Train Loss: 0.35219, Test Loss: 0.78659\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 608, Train Loss: 0.35369, Test Loss: 0.77397\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 609, Train Loss: 0.34988, Test Loss: 0.78361\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 610, Train Loss: 0.35293, Test Loss: 0.78943\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 611, Train Loss: 0.34971, Test Loss: 0.77256\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 612, Train Loss: 0.35060, Test Loss: 0.76766\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 613, Train Loss: 0.35342, Test Loss: 0.78181\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 614, Train Loss: 0.35187, Test Loss: 0.77092\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 615, Train Loss: 0.35046, Test Loss: 0.76850\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 616, Train Loss: 0.35102, Test Loss: 0.77029\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 617, Train Loss: 0.34806, Test Loss: 0.76852\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 618, Train Loss: 0.34738, Test Loss: 0.77553\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 619, Train Loss: 0.35024, Test Loss: 0.77651\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 620, Train Loss: 0.35346, Test Loss: 0.77683\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 621, Train Loss: 0.35183, Test Loss: 0.76686\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 622, Train Loss: 0.34832, Test Loss: 0.76891\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 623, Train Loss: 0.35175, Test Loss: 0.76632\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 624, Train Loss: 0.35179, Test Loss: 0.77230\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 625, Train Loss: 0.35129, Test Loss: 0.76853\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 626, Train Loss: 0.35481, Test Loss: 0.76759\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 627, Train Loss: 0.34986, Test Loss: 0.77286\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 628, Train Loss: 0.35176, Test Loss: 0.77461\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 629, Train Loss: 0.34863, Test Loss: 0.77337\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 630, Train Loss: 0.35057, Test Loss: 0.76608\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 631, Train Loss: 0.34731, Test Loss: 0.76751\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 632, Train Loss: 0.34814, Test Loss: 0.75914\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 633, Train Loss: 0.34850, Test Loss: 0.77081\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 634, Train Loss: 0.34614, Test Loss: 0.76431\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 635, Train Loss: 0.35327, Test Loss: 0.77124\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 636, Train Loss: 0.35122, Test Loss: 0.76387\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 637, Train Loss: 0.35087, Test Loss: 0.76741\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 638, Train Loss: 0.35229, Test Loss: 0.76496\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 639, Train Loss: 0.35012, Test Loss: 0.76903\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 640, Train Loss: 0.34660, Test Loss: 0.76738\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 641, Train Loss: 0.35162, Test Loss: 0.77146\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 642, Train Loss: 0.34783, Test Loss: 0.76981\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 643, Train Loss: 0.34462, Test Loss: 0.77037\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 644, Train Loss: 0.34554, Test Loss: 0.75262\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 645, Train Loss: 0.34501, Test Loss: 0.75479\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 646, Train Loss: 0.34198, Test Loss: 0.76612\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 647, Train Loss: 0.34524, Test Loss: 0.76466\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 648, Train Loss: 0.34469, Test Loss: 0.75314\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 649, Train Loss: 0.34300, Test Loss: 0.76101\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 650, Train Loss: 0.34361, Test Loss: 0.75196\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 651, Train Loss: 0.34528, Test Loss: 0.74938\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 652, Train Loss: 0.34346, Test Loss: 0.74584\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 653, Train Loss: 0.34726, Test Loss: 0.77099\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 654, Train Loss: 0.34445, Test Loss: 0.76501\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 655, Train Loss: 0.34641, Test Loss: 0.74999\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 656, Train Loss: 0.34507, Test Loss: 0.75799\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 657, Train Loss: 0.34494, Test Loss: 0.76157\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 658, Train Loss: 0.34302, Test Loss: 0.75802\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 659, Train Loss: 0.34319, Test Loss: 0.75211\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 660, Train Loss: 0.34406, Test Loss: 0.74756\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 661, Train Loss: 0.34369, Test Loss: 0.75234\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 662, Train Loss: 0.34055, Test Loss: 0.75422\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 663, Train Loss: 0.34305, Test Loss: 0.76041\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 664, Train Loss: 0.34542, Test Loss: 0.75484\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 665, Train Loss: 0.34285, Test Loss: 0.76464\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 666, Train Loss: 0.34175, Test Loss: 0.76305\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 667, Train Loss: 0.34160, Test Loss: 0.75202\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 668, Train Loss: 0.34016, Test Loss: 0.75422\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 669, Train Loss: 0.33953, Test Loss: 0.76171\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 670, Train Loss: 0.33751, Test Loss: 0.74779\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 671, Train Loss: 0.33981, Test Loss: 0.74448\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 672, Train Loss: 0.33533, Test Loss: 0.76446\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 673, Train Loss: 0.33621, Test Loss: 0.74941\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 674, Train Loss: 0.34157, Test Loss: 0.74219\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 675, Train Loss: 0.33657, Test Loss: 0.75017\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 676, Train Loss: 0.33882, Test Loss: 0.75307\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 677, Train Loss: 0.33560, Test Loss: 0.76480\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 678, Train Loss: 0.33640, Test Loss: 0.74002\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 679, Train Loss: 0.33661, Test Loss: 0.73616\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 680, Train Loss: 0.33596, Test Loss: 0.73621\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 681, Train Loss: 0.33603, Test Loss: 0.75011\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 682, Train Loss: 0.34272, Test Loss: 0.74875\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 683, Train Loss: 0.34144, Test Loss: 0.75006\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 684, Train Loss: 0.33609, Test Loss: 0.75205\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 685, Train Loss: 0.33406, Test Loss: 0.73958\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 686, Train Loss: 0.33982, Test Loss: 0.75471\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 687, Train Loss: 0.33736, Test Loss: 0.73536\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 688, Train Loss: 0.33531, Test Loss: 0.74234\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 689, Train Loss: 0.33182, Test Loss: 0.73906\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 690, Train Loss: 0.33378, Test Loss: 0.73393\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 691, Train Loss: 0.33275, Test Loss: 0.73943\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 692, Train Loss: 0.33446, Test Loss: 0.74223\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 693, Train Loss: 0.33758, Test Loss: 0.74095\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 694, Train Loss: 0.33573, Test Loss: 0.74138\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 695, Train Loss: 0.33428, Test Loss: 0.75007\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 696, Train Loss: 0.33006, Test Loss: 0.74017\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 697, Train Loss: 0.33452, Test Loss: 0.72814\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 698, Train Loss: 0.32890, Test Loss: 0.74171\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 699, Train Loss: 0.33152, Test Loss: 0.73557\n",
      "learning rate= 1.5625e-05\n",
      "Epoch 700, Train Loss: 0.33230, Test Loss: 0.73366\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 701, Train Loss: 0.31497, Test Loss: 0.72151\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 702, Train Loss: 0.31215, Test Loss: 0.72396\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 703, Train Loss: 0.30976, Test Loss: 0.71672\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 704, Train Loss: 0.31065, Test Loss: 0.71707\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 705, Train Loss: 0.31099, Test Loss: 0.72842\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 706, Train Loss: 0.31103, Test Loss: 0.72267\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 707, Train Loss: 0.31037, Test Loss: 0.71680\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 708, Train Loss: 0.31163, Test Loss: 0.71274\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 709, Train Loss: 0.31053, Test Loss: 0.72034\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 710, Train Loss: 0.31250, Test Loss: 0.71535\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 711, Train Loss: 0.30926, Test Loss: 0.71653\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 712, Train Loss: 0.30984, Test Loss: 0.72177\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 713, Train Loss: 0.31106, Test Loss: 0.71778\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 714, Train Loss: 0.30970, Test Loss: 0.72721\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 715, Train Loss: 0.31269, Test Loss: 0.71565\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 716, Train Loss: 0.31043, Test Loss: 0.71755\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 717, Train Loss: 0.30998, Test Loss: 0.71785\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 718, Train Loss: 0.31082, Test Loss: 0.71493\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 719, Train Loss: 0.30897, Test Loss: 0.71569\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 720, Train Loss: 0.31115, Test Loss: 0.71725\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 721, Train Loss: 0.31009, Test Loss: 0.71922\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 722, Train Loss: 0.30759, Test Loss: 0.71576\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 723, Train Loss: 0.31022, Test Loss: 0.71343\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 724, Train Loss: 0.31034, Test Loss: 0.72015\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 725, Train Loss: 0.31021, Test Loss: 0.71689\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 726, Train Loss: 0.30975, Test Loss: 0.71571\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 727, Train Loss: 0.31120, Test Loss: 0.71144\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 728, Train Loss: 0.31208, Test Loss: 0.72263\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 729, Train Loss: 0.31024, Test Loss: 0.71486\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 730, Train Loss: 0.31007, Test Loss: 0.71437\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 731, Train Loss: 0.30917, Test Loss: 0.70794\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 732, Train Loss: 0.30841, Test Loss: 0.70823\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 733, Train Loss: 0.30916, Test Loss: 0.71158\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 734, Train Loss: 0.30997, Test Loss: 0.71655\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 735, Train Loss: 0.30774, Test Loss: 0.70993\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 736, Train Loss: 0.30830, Test Loss: 0.71209\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 737, Train Loss: 0.30782, Test Loss: 0.71543\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 738, Train Loss: 0.30938, Test Loss: 0.70938\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 739, Train Loss: 0.30881, Test Loss: 0.71398\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 740, Train Loss: 0.30960, Test Loss: 0.70970\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 741, Train Loss: 0.30619, Test Loss: 0.71355\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 742, Train Loss: 0.30821, Test Loss: 0.70901\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 743, Train Loss: 0.30860, Test Loss: 0.71408\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 744, Train Loss: 0.30709, Test Loss: 0.71430\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 745, Train Loss: 0.30788, Test Loss: 0.71378\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 746, Train Loss: 0.30814, Test Loss: 0.71461\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 747, Train Loss: 0.30753, Test Loss: 0.70512\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 748, Train Loss: 0.30697, Test Loss: 0.70452\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 749, Train Loss: 0.30781, Test Loss: 0.70863\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 750, Train Loss: 0.30799, Test Loss: 0.71234\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 751, Train Loss: 0.30724, Test Loss: 0.70177\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 752, Train Loss: 0.30577, Test Loss: 0.70520\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 753, Train Loss: 0.30871, Test Loss: 0.70842\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 754, Train Loss: 0.30785, Test Loss: 0.70523\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 755, Train Loss: 0.30599, Test Loss: 0.70459\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 756, Train Loss: 0.30561, Test Loss: 0.71093\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 757, Train Loss: 0.30553, Test Loss: 0.71032\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 758, Train Loss: 0.30542, Test Loss: 0.70783\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 759, Train Loss: 0.30734, Test Loss: 0.71627\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 760, Train Loss: 0.30489, Test Loss: 0.70468\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 761, Train Loss: 0.30581, Test Loss: 0.71645\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 762, Train Loss: 0.30498, Test Loss: 0.70954\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 763, Train Loss: 0.30473, Test Loss: 0.70307\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 764, Train Loss: 0.30587, Test Loss: 0.70538\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 765, Train Loss: 0.30383, Test Loss: 0.70940\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 766, Train Loss: 0.30467, Test Loss: 0.70374\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 767, Train Loss: 0.30362, Test Loss: 0.71715\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 768, Train Loss: 0.30559, Test Loss: 0.71139\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 769, Train Loss: 0.30476, Test Loss: 0.69969\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 770, Train Loss: 0.30456, Test Loss: 0.70905\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 771, Train Loss: 0.30471, Test Loss: 0.70309\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 772, Train Loss: 0.30206, Test Loss: 0.70935\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 773, Train Loss: 0.30484, Test Loss: 0.70826\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 774, Train Loss: 0.30185, Test Loss: 0.70599\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 775, Train Loss: 0.30264, Test Loss: 0.70269\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 776, Train Loss: 0.30259, Test Loss: 0.69962\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 777, Train Loss: 0.30228, Test Loss: 0.70675\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 778, Train Loss: 0.30223, Test Loss: 0.69577\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 779, Train Loss: 0.30207, Test Loss: 0.70442\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 780, Train Loss: 0.30264, Test Loss: 0.69966\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 781, Train Loss: 0.30454, Test Loss: 0.71073\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 782, Train Loss: 0.30379, Test Loss: 0.69992\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 783, Train Loss: 0.30340, Test Loss: 0.69487\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 784, Train Loss: 0.30263, Test Loss: 0.70652\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 785, Train Loss: 0.30224, Test Loss: 0.69859\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 786, Train Loss: 0.30169, Test Loss: 0.69751\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 787, Train Loss: 0.30305, Test Loss: 0.69899\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 788, Train Loss: 0.30239, Test Loss: 0.69711\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 789, Train Loss: 0.30277, Test Loss: 0.70223\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 790, Train Loss: 0.30112, Test Loss: 0.69864\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 791, Train Loss: 0.30238, Test Loss: 0.69559\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 792, Train Loss: 0.30151, Test Loss: 0.70153\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 793, Train Loss: 0.30121, Test Loss: 0.70359\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 794, Train Loss: 0.29966, Test Loss: 0.70231\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 795, Train Loss: 0.30350, Test Loss: 0.69853\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 796, Train Loss: 0.30071, Test Loss: 0.70074\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 797, Train Loss: 0.30078, Test Loss: 0.69861\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 798, Train Loss: 0.30221, Test Loss: 0.70338\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 799, Train Loss: 0.30014, Test Loss: 0.69898\n",
      "learning rate= 7.8125e-06\n",
      "Epoch 800, Train Loss: 0.30122, Test Loss: 0.69829\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 801, Train Loss: 0.29101, Test Loss: 0.68952\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 802, Train Loss: 0.28965, Test Loss: 0.68819\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 803, Train Loss: 0.29019, Test Loss: 0.68605\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 804, Train Loss: 0.28920, Test Loss: 0.68724\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 805, Train Loss: 0.29069, Test Loss: 0.69058\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 806, Train Loss: 0.29141, Test Loss: 0.69297\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 807, Train Loss: 0.28990, Test Loss: 0.68821\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 808, Train Loss: 0.29043, Test Loss: 0.69120\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 809, Train Loss: 0.29043, Test Loss: 0.69107\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 810, Train Loss: 0.28997, Test Loss: 0.68577\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 811, Train Loss: 0.28986, Test Loss: 0.68881\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 812, Train Loss: 0.28943, Test Loss: 0.68682\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 813, Train Loss: 0.28939, Test Loss: 0.69140\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 814, Train Loss: 0.29054, Test Loss: 0.68858\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 815, Train Loss: 0.28983, Test Loss: 0.68950\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 816, Train Loss: 0.28930, Test Loss: 0.68632\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 817, Train Loss: 0.28881, Test Loss: 0.68695\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 818, Train Loss: 0.28875, Test Loss: 0.69076\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 819, Train Loss: 0.28954, Test Loss: 0.68785\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 820, Train Loss: 0.28961, Test Loss: 0.68870\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 821, Train Loss: 0.28918, Test Loss: 0.68531\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 822, Train Loss: 0.28923, Test Loss: 0.68630\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 823, Train Loss: 0.29008, Test Loss: 0.68552\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 824, Train Loss: 0.28938, Test Loss: 0.68541\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 825, Train Loss: 0.28995, Test Loss: 0.68830\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 826, Train Loss: 0.28925, Test Loss: 0.68349\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 827, Train Loss: 0.28938, Test Loss: 0.68466\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 828, Train Loss: 0.28797, Test Loss: 0.68562\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 829, Train Loss: 0.28861, Test Loss: 0.69167\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 830, Train Loss: 0.28983, Test Loss: 0.68475\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 831, Train Loss: 0.28873, Test Loss: 0.68508\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 832, Train Loss: 0.28931, Test Loss: 0.68617\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 833, Train Loss: 0.28894, Test Loss: 0.68627\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 834, Train Loss: 0.28916, Test Loss: 0.69002\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 835, Train Loss: 0.28849, Test Loss: 0.68753\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 836, Train Loss: 0.28830, Test Loss: 0.68854\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 837, Train Loss: 0.28805, Test Loss: 0.68632\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 838, Train Loss: 0.28996, Test Loss: 0.68826\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 839, Train Loss: 0.28853, Test Loss: 0.68676\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 840, Train Loss: 0.28927, Test Loss: 0.68762\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 841, Train Loss: 0.28718, Test Loss: 0.68469\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 842, Train Loss: 0.28826, Test Loss: 0.68394\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 843, Train Loss: 0.28969, Test Loss: 0.68239\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 844, Train Loss: 0.28763, Test Loss: 0.68378\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 845, Train Loss: 0.28819, Test Loss: 0.68372\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 846, Train Loss: 0.28772, Test Loss: 0.68899\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 847, Train Loss: 0.28882, Test Loss: 0.68922\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 848, Train Loss: 0.28803, Test Loss: 0.68450\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 849, Train Loss: 0.28738, Test Loss: 0.68471\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 850, Train Loss: 0.28821, Test Loss: 0.68456\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 851, Train Loss: 0.28725, Test Loss: 0.68404\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 852, Train Loss: 0.28693, Test Loss: 0.68910\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 853, Train Loss: 0.28661, Test Loss: 0.68456\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 854, Train Loss: 0.28840, Test Loss: 0.68768\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 855, Train Loss: 0.28725, Test Loss: 0.68586\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 856, Train Loss: 0.28620, Test Loss: 0.68068\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 857, Train Loss: 0.28677, Test Loss: 0.68410\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 858, Train Loss: 0.28758, Test Loss: 0.68364\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 859, Train Loss: 0.28737, Test Loss: 0.69533\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 860, Train Loss: 0.28689, Test Loss: 0.67994\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 861, Train Loss: 0.28644, Test Loss: 0.68368\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 862, Train Loss: 0.28699, Test Loss: 0.68209\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 863, Train Loss: 0.28641, Test Loss: 0.68307\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 864, Train Loss: 0.28699, Test Loss: 0.67964\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 865, Train Loss: 0.28638, Test Loss: 0.68043\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 866, Train Loss: 0.28732, Test Loss: 0.68369\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 867, Train Loss: 0.28669, Test Loss: 0.68294\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 868, Train Loss: 0.28628, Test Loss: 0.67954\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 869, Train Loss: 0.28603, Test Loss: 0.68080\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 870, Train Loss: 0.28586, Test Loss: 0.68113\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 871, Train Loss: 0.28770, Test Loss: 0.68328\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 872, Train Loss: 0.28666, Test Loss: 0.68732\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 873, Train Loss: 0.28660, Test Loss: 0.68610\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 874, Train Loss: 0.28834, Test Loss: 0.68293\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 875, Train Loss: 0.28505, Test Loss: 0.68260\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 876, Train Loss: 0.28496, Test Loss: 0.68732\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 877, Train Loss: 0.28557, Test Loss: 0.68371\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 878, Train Loss: 0.28575, Test Loss: 0.68284\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 879, Train Loss: 0.28539, Test Loss: 0.67867\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 880, Train Loss: 0.28469, Test Loss: 0.68772\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 881, Train Loss: 0.28569, Test Loss: 0.67916\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 882, Train Loss: 0.28562, Test Loss: 0.68142\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 883, Train Loss: 0.28560, Test Loss: 0.68271\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 884, Train Loss: 0.28477, Test Loss: 0.68100\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 885, Train Loss: 0.28542, Test Loss: 0.68478\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 886, Train Loss: 0.28569, Test Loss: 0.67801\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 887, Train Loss: 0.28501, Test Loss: 0.67893\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 888, Train Loss: 0.28526, Test Loss: 0.68103\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 889, Train Loss: 0.28453, Test Loss: 0.67691\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 890, Train Loss: 0.28558, Test Loss: 0.67803\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 891, Train Loss: 0.28416, Test Loss: 0.68141\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 892, Train Loss: 0.28446, Test Loss: 0.67993\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 893, Train Loss: 0.28495, Test Loss: 0.67850\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 894, Train Loss: 0.28572, Test Loss: 0.68118\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 895, Train Loss: 0.28490, Test Loss: 0.67960\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 896, Train Loss: 0.28456, Test Loss: 0.68208\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 897, Train Loss: 0.28456, Test Loss: 0.68072\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 898, Train Loss: 0.28442, Test Loss: 0.68410\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 899, Train Loss: 0.28365, Test Loss: 0.67562\n",
      "learning rate= 3.90625e-06\n",
      "Epoch 900, Train Loss: 0.28378, Test Loss: 0.67726\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 901, Train Loss: 0.27949, Test Loss: 0.67459\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 902, Train Loss: 0.27839, Test Loss: 0.67405\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 903, Train Loss: 0.27900, Test Loss: 0.67424\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 904, Train Loss: 0.27873, Test Loss: 0.67434\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 905, Train Loss: 0.27871, Test Loss: 0.67255\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 906, Train Loss: 0.27838, Test Loss: 0.67362\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 907, Train Loss: 0.27778, Test Loss: 0.67309\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 908, Train Loss: 0.27873, Test Loss: 0.67366\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 909, Train Loss: 0.27791, Test Loss: 0.67310\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 910, Train Loss: 0.27873, Test Loss: 0.67499\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 911, Train Loss: 0.27800, Test Loss: 0.67359\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 912, Train Loss: 0.27787, Test Loss: 0.67572\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 913, Train Loss: 0.27777, Test Loss: 0.67144\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 914, Train Loss: 0.27810, Test Loss: 0.67358\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 915, Train Loss: 0.27825, Test Loss: 0.67578\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 916, Train Loss: 0.27813, Test Loss: 0.67254\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 917, Train Loss: 0.27792, Test Loss: 0.67290\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 918, Train Loss: 0.27834, Test Loss: 0.67339\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 919, Train Loss: 0.27812, Test Loss: 0.67169\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 920, Train Loss: 0.27752, Test Loss: 0.67399\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 921, Train Loss: 0.27798, Test Loss: 0.67341\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 922, Train Loss: 0.27782, Test Loss: 0.67280\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 923, Train Loss: 0.27806, Test Loss: 0.67120\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 924, Train Loss: 0.27827, Test Loss: 0.67257\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 925, Train Loss: 0.27858, Test Loss: 0.67251\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 926, Train Loss: 0.27780, Test Loss: 0.67149\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 927, Train Loss: 0.27826, Test Loss: 0.67281\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 928, Train Loss: 0.27793, Test Loss: 0.67266\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 929, Train Loss: 0.27838, Test Loss: 0.67483\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 930, Train Loss: 0.27765, Test Loss: 0.67328\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 931, Train Loss: 0.27821, Test Loss: 0.67185\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 932, Train Loss: 0.27758, Test Loss: 0.67129\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 933, Train Loss: 0.27840, Test Loss: 0.67100\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 934, Train Loss: 0.27820, Test Loss: 0.67414\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 935, Train Loss: 0.27766, Test Loss: 0.67352\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 936, Train Loss: 0.27767, Test Loss: 0.67029\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 937, Train Loss: 0.27826, Test Loss: 0.67338\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 938, Train Loss: 0.27798, Test Loss: 0.67313\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 939, Train Loss: 0.27756, Test Loss: 0.67029\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 940, Train Loss: 0.27718, Test Loss: 0.67126\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 941, Train Loss: 0.27720, Test Loss: 0.67198\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 942, Train Loss: 0.27796, Test Loss: 0.67035\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 943, Train Loss: 0.27766, Test Loss: 0.67241\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 944, Train Loss: 0.27752, Test Loss: 0.67310\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 945, Train Loss: 0.27758, Test Loss: 0.67079\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 946, Train Loss: 0.27703, Test Loss: 0.66968\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 947, Train Loss: 0.27755, Test Loss: 0.67180\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 948, Train Loss: 0.27677, Test Loss: 0.67258\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 949, Train Loss: 0.27723, Test Loss: 0.67090\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 950, Train Loss: 0.27710, Test Loss: 0.67033\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 951, Train Loss: 0.27721, Test Loss: 0.67149\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 952, Train Loss: 0.27773, Test Loss: 0.67058\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 953, Train Loss: 0.27684, Test Loss: 0.67174\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 954, Train Loss: 0.27697, Test Loss: 0.66961\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 955, Train Loss: 0.27650, Test Loss: 0.67099\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 956, Train Loss: 0.27722, Test Loss: 0.67100\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 957, Train Loss: 0.27664, Test Loss: 0.67218\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 958, Train Loss: 0.27676, Test Loss: 0.67051\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 959, Train Loss: 0.27785, Test Loss: 0.67201\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 960, Train Loss: 0.27681, Test Loss: 0.66750\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 961, Train Loss: 0.27667, Test Loss: 0.67033\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 962, Train Loss: 0.27674, Test Loss: 0.67452\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 963, Train Loss: 0.27698, Test Loss: 0.67243\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 964, Train Loss: 0.27684, Test Loss: 0.66997\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 965, Train Loss: 0.27633, Test Loss: 0.66891\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 966, Train Loss: 0.27665, Test Loss: 0.66928\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 967, Train Loss: 0.27668, Test Loss: 0.67022\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 968, Train Loss: 0.27584, Test Loss: 0.67239\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 969, Train Loss: 0.27698, Test Loss: 0.67112\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 970, Train Loss: 0.27573, Test Loss: 0.66864\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 971, Train Loss: 0.27658, Test Loss: 0.67050\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 972, Train Loss: 0.27644, Test Loss: 0.67021\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 973, Train Loss: 0.27667, Test Loss: 0.67284\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 974, Train Loss: 0.27621, Test Loss: 0.66926\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 975, Train Loss: 0.27673, Test Loss: 0.67256\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 976, Train Loss: 0.27668, Test Loss: 0.67006\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 977, Train Loss: 0.27658, Test Loss: 0.66936\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 978, Train Loss: 0.27627, Test Loss: 0.66928\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 979, Train Loss: 0.27626, Test Loss: 0.66915\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 980, Train Loss: 0.27682, Test Loss: 0.66785\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 981, Train Loss: 0.27591, Test Loss: 0.66841\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 982, Train Loss: 0.27578, Test Loss: 0.66897\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 983, Train Loss: 0.27638, Test Loss: 0.67028\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 984, Train Loss: 0.27617, Test Loss: 0.66769\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 985, Train Loss: 0.27604, Test Loss: 0.66768\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 986, Train Loss: 0.27609, Test Loss: 0.66731\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 987, Train Loss: 0.27527, Test Loss: 0.66807\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 988, Train Loss: 0.27621, Test Loss: 0.66777\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 989, Train Loss: 0.27658, Test Loss: 0.66802\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 990, Train Loss: 0.27561, Test Loss: 0.66894\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 991, Train Loss: 0.27532, Test Loss: 0.66855\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 992, Train Loss: 0.27689, Test Loss: 0.66944\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 993, Train Loss: 0.27543, Test Loss: 0.66935\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 994, Train Loss: 0.27546, Test Loss: 0.66892\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 995, Train Loss: 0.27591, Test Loss: 0.66872\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 996, Train Loss: 0.27611, Test Loss: 0.66969\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 997, Train Loss: 0.27590, Test Loss: 0.66873\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 998, Train Loss: 0.27528, Test Loss: 0.66823\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 999, Train Loss: 0.27526, Test Loss: 0.67009\n",
      "learning rate= 1.953125e-06\n",
      "Epoch 1000, Train Loss: 0.27565, Test Loss: 0.67002\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1001, Train Loss: 0.27362, Test Loss: 0.66592\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1002, Train Loss: 0.27232, Test Loss: 0.66550\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1003, Train Loss: 0.27247, Test Loss: 0.66658\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1004, Train Loss: 0.27216, Test Loss: 0.66572\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1005, Train Loss: 0.27253, Test Loss: 0.66575\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1006, Train Loss: 0.27260, Test Loss: 0.66699\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1007, Train Loss: 0.27274, Test Loss: 0.66598\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1008, Train Loss: 0.27210, Test Loss: 0.66698\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1009, Train Loss: 0.27191, Test Loss: 0.66708\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1010, Train Loss: 0.27239, Test Loss: 0.66623\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1011, Train Loss: 0.27203, Test Loss: 0.66690\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1012, Train Loss: 0.27261, Test Loss: 0.66711\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1013, Train Loss: 0.27264, Test Loss: 0.66749\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1014, Train Loss: 0.27227, Test Loss: 0.66597\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1015, Train Loss: 0.27248, Test Loss: 0.66781\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1016, Train Loss: 0.27218, Test Loss: 0.66634\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1017, Train Loss: 0.27276, Test Loss: 0.66580\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1018, Train Loss: 0.27275, Test Loss: 0.66517\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1019, Train Loss: 0.27194, Test Loss: 0.66527\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1020, Train Loss: 0.27229, Test Loss: 0.66699\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1021, Train Loss: 0.27233, Test Loss: 0.66668\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1022, Train Loss: 0.27242, Test Loss: 0.66480\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1023, Train Loss: 0.27182, Test Loss: 0.66534\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1024, Train Loss: 0.27256, Test Loss: 0.66763\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1025, Train Loss: 0.27216, Test Loss: 0.66642\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1026, Train Loss: 0.27246, Test Loss: 0.66631\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1027, Train Loss: 0.27205, Test Loss: 0.66542\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1028, Train Loss: 0.27285, Test Loss: 0.66569\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1029, Train Loss: 0.27203, Test Loss: 0.66628\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1030, Train Loss: 0.27227, Test Loss: 0.66554\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1031, Train Loss: 0.27316, Test Loss: 0.66625\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1032, Train Loss: 0.27193, Test Loss: 0.66602\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1033, Train Loss: 0.27186, Test Loss: 0.66494\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1034, Train Loss: 0.27225, Test Loss: 0.66497\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1035, Train Loss: 0.27241, Test Loss: 0.66629\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1036, Train Loss: 0.27284, Test Loss: 0.66515\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1037, Train Loss: 0.27274, Test Loss: 0.66522\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1038, Train Loss: 0.27175, Test Loss: 0.66569\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1039, Train Loss: 0.27190, Test Loss: 0.66494\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1040, Train Loss: 0.27186, Test Loss: 0.66682\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1041, Train Loss: 0.27185, Test Loss: 0.66641\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1042, Train Loss: 0.27176, Test Loss: 0.66667\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1043, Train Loss: 0.27227, Test Loss: 0.66474\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1044, Train Loss: 0.27233, Test Loss: 0.66555\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1045, Train Loss: 0.27173, Test Loss: 0.66455\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1046, Train Loss: 0.27131, Test Loss: 0.66554\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1047, Train Loss: 0.27170, Test Loss: 0.66526\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1048, Train Loss: 0.27175, Test Loss: 0.66604\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1049, Train Loss: 0.27127, Test Loss: 0.66457\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1050, Train Loss: 0.27138, Test Loss: 0.66524\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1051, Train Loss: 0.27161, Test Loss: 0.66512\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1052, Train Loss: 0.27191, Test Loss: 0.66424\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1053, Train Loss: 0.27164, Test Loss: 0.66615\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1054, Train Loss: 0.27134, Test Loss: 0.66583\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1055, Train Loss: 0.27135, Test Loss: 0.66508\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1056, Train Loss: 0.27149, Test Loss: 0.66676\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1057, Train Loss: 0.27167, Test Loss: 0.66558\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1058, Train Loss: 0.27191, Test Loss: 0.66525\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1059, Train Loss: 0.27177, Test Loss: 0.66399\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1060, Train Loss: 0.27158, Test Loss: 0.66352\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1061, Train Loss: 0.27202, Test Loss: 0.66498\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1062, Train Loss: 0.27178, Test Loss: 0.66526\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1063, Train Loss: 0.27163, Test Loss: 0.66405\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1064, Train Loss: 0.27155, Test Loss: 0.66452\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1065, Train Loss: 0.27178, Test Loss: 0.66439\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1066, Train Loss: 0.27174, Test Loss: 0.66415\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1067, Train Loss: 0.27140, Test Loss: 0.66559\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1068, Train Loss: 0.27180, Test Loss: 0.66396\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1069, Train Loss: 0.27196, Test Loss: 0.66545\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1070, Train Loss: 0.27190, Test Loss: 0.66553\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1071, Train Loss: 0.27175, Test Loss: 0.66455\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1072, Train Loss: 0.27150, Test Loss: 0.66458\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1073, Train Loss: 0.27185, Test Loss: 0.66557\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1074, Train Loss: 0.27128, Test Loss: 0.66423\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1075, Train Loss: 0.27144, Test Loss: 0.66476\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1076, Train Loss: 0.27150, Test Loss: 0.66550\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1077, Train Loss: 0.27161, Test Loss: 0.66526\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1078, Train Loss: 0.27140, Test Loss: 0.66408\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1079, Train Loss: 0.27212, Test Loss: 0.66418\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1080, Train Loss: 0.27155, Test Loss: 0.66468\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1081, Train Loss: 0.27269, Test Loss: 0.66410\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1082, Train Loss: 0.27090, Test Loss: 0.66399\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1083, Train Loss: 0.27171, Test Loss: 0.66317\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1084, Train Loss: 0.27132, Test Loss: 0.66537\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1085, Train Loss: 0.27144, Test Loss: 0.66466\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1086, Train Loss: 0.27152, Test Loss: 0.66490\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1087, Train Loss: 0.27122, Test Loss: 0.66364\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1088, Train Loss: 0.27151, Test Loss: 0.66522\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1089, Train Loss: 0.27151, Test Loss: 0.66430\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1090, Train Loss: 0.27125, Test Loss: 0.66327\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1091, Train Loss: 0.27093, Test Loss: 0.66484\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1092, Train Loss: 0.27158, Test Loss: 0.66426\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1093, Train Loss: 0.27159, Test Loss: 0.66439\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1094, Train Loss: 0.27114, Test Loss: 0.66461\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1095, Train Loss: 0.27096, Test Loss: 0.66346\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1096, Train Loss: 0.27136, Test Loss: 0.66478\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1097, Train Loss: 0.27198, Test Loss: 0.66340\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1098, Train Loss: 0.27118, Test Loss: 0.66469\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1099, Train Loss: 0.27137, Test Loss: 0.66443\n",
      "learning rate= 9.765625e-07\n",
      "Epoch 1100, Train Loss: 0.27103, Test Loss: 0.66240\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1101, Train Loss: 0.26956, Test Loss: 0.66189\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1102, Train Loss: 0.26931, Test Loss: 0.66316\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1103, Train Loss: 0.26925, Test Loss: 0.66177\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1104, Train Loss: 0.26952, Test Loss: 0.66298\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1105, Train Loss: 0.26930, Test Loss: 0.66173\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1106, Train Loss: 0.26984, Test Loss: 0.66327\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1107, Train Loss: 0.26919, Test Loss: 0.66249\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1108, Train Loss: 0.26900, Test Loss: 0.66186\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1109, Train Loss: 0.26934, Test Loss: 0.66198\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1110, Train Loss: 0.26895, Test Loss: 0.66216\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1111, Train Loss: 0.26934, Test Loss: 0.66216\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1112, Train Loss: 0.26949, Test Loss: 0.66219\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1113, Train Loss: 0.26868, Test Loss: 0.66179\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1114, Train Loss: 0.26938, Test Loss: 0.66120\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1115, Train Loss: 0.26901, Test Loss: 0.66185\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1116, Train Loss: 0.26942, Test Loss: 0.66284\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1117, Train Loss: 0.26961, Test Loss: 0.66266\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1118, Train Loss: 0.26908, Test Loss: 0.66254\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1119, Train Loss: 0.26880, Test Loss: 0.66201\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1120, Train Loss: 0.26929, Test Loss: 0.66197\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1121, Train Loss: 0.26911, Test Loss: 0.66173\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1122, Train Loss: 0.26862, Test Loss: 0.66196\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1123, Train Loss: 0.26901, Test Loss: 0.66211\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1124, Train Loss: 0.26907, Test Loss: 0.66218\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1125, Train Loss: 0.26936, Test Loss: 0.66255\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1126, Train Loss: 0.26979, Test Loss: 0.66259\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1127, Train Loss: 0.26958, Test Loss: 0.66231\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1128, Train Loss: 0.26892, Test Loss: 0.66131\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1129, Train Loss: 0.26894, Test Loss: 0.66166\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1130, Train Loss: 0.26870, Test Loss: 0.66234\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1131, Train Loss: 0.26878, Test Loss: 0.66240\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1132, Train Loss: 0.26961, Test Loss: 0.66180\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1133, Train Loss: 0.26947, Test Loss: 0.66216\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1134, Train Loss: 0.26927, Test Loss: 0.66227\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1135, Train Loss: 0.26976, Test Loss: 0.66210\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1136, Train Loss: 0.26899, Test Loss: 0.66136\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1137, Train Loss: 0.26882, Test Loss: 0.66271\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1138, Train Loss: 0.26902, Test Loss: 0.66224\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1139, Train Loss: 0.26923, Test Loss: 0.66286\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1140, Train Loss: 0.26950, Test Loss: 0.66161\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1141, Train Loss: 0.26912, Test Loss: 0.66170\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1142, Train Loss: 0.26893, Test Loss: 0.66105\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1143, Train Loss: 0.26879, Test Loss: 0.66150\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1144, Train Loss: 0.26865, Test Loss: 0.66252\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1145, Train Loss: 0.26931, Test Loss: 0.66217\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1146, Train Loss: 0.26873, Test Loss: 0.66177\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1147, Train Loss: 0.26917, Test Loss: 0.66053\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1148, Train Loss: 0.26896, Test Loss: 0.66195\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1149, Train Loss: 0.26916, Test Loss: 0.66229\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1150, Train Loss: 0.26920, Test Loss: 0.66184\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1151, Train Loss: 0.26897, Test Loss: 0.66178\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1152, Train Loss: 0.26853, Test Loss: 0.66382\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1153, Train Loss: 0.26942, Test Loss: 0.66135\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1154, Train Loss: 0.26911, Test Loss: 0.66216\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1155, Train Loss: 0.26909, Test Loss: 0.66156\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1156, Train Loss: 0.26903, Test Loss: 0.66211\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1157, Train Loss: 0.26859, Test Loss: 0.66097\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1158, Train Loss: 0.26906, Test Loss: 0.66345\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1159, Train Loss: 0.26913, Test Loss: 0.66154\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1160, Train Loss: 0.26911, Test Loss: 0.66160\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1161, Train Loss: 0.26849, Test Loss: 0.66266\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1162, Train Loss: 0.26919, Test Loss: 0.66051\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1163, Train Loss: 0.26887, Test Loss: 0.66249\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1164, Train Loss: 0.26857, Test Loss: 0.66213\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1165, Train Loss: 0.26879, Test Loss: 0.66226\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1166, Train Loss: 0.26902, Test Loss: 0.66250\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1167, Train Loss: 0.26872, Test Loss: 0.66129\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1168, Train Loss: 0.26818, Test Loss: 0.66168\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1169, Train Loss: 0.26847, Test Loss: 0.66184\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1170, Train Loss: 0.26863, Test Loss: 0.66209\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1171, Train Loss: 0.26856, Test Loss: 0.66177\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1172, Train Loss: 0.26848, Test Loss: 0.66222\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1173, Train Loss: 0.26895, Test Loss: 0.66097\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1174, Train Loss: 0.26914, Test Loss: 0.66140\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1175, Train Loss: 0.26828, Test Loss: 0.66137\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1176, Train Loss: 0.26936, Test Loss: 0.66133\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1177, Train Loss: 0.26861, Test Loss: 0.66123\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1178, Train Loss: 0.26854, Test Loss: 0.66224\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1179, Train Loss: 0.26850, Test Loss: 0.66193\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1180, Train Loss: 0.26855, Test Loss: 0.66146\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1181, Train Loss: 0.26806, Test Loss: 0.66203\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1182, Train Loss: 0.26936, Test Loss: 0.66101\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1183, Train Loss: 0.26878, Test Loss: 0.66148\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1184, Train Loss: 0.26817, Test Loss: 0.66190\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1185, Train Loss: 0.26851, Test Loss: 0.66154\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1186, Train Loss: 0.26866, Test Loss: 0.66119\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1187, Train Loss: 0.26810, Test Loss: 0.66180\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1188, Train Loss: 0.26917, Test Loss: 0.66093\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1189, Train Loss: 0.26882, Test Loss: 0.66025\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1190, Train Loss: 0.26875, Test Loss: 0.66090\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1191, Train Loss: 0.26880, Test Loss: 0.66088\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1192, Train Loss: 0.26853, Test Loss: 0.66113\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1193, Train Loss: 0.26864, Test Loss: 0.66046\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1194, Train Loss: 0.26836, Test Loss: 0.66104\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1195, Train Loss: 0.26853, Test Loss: 0.66092\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1196, Train Loss: 0.26859, Test Loss: 0.66095\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1197, Train Loss: 0.26843, Test Loss: 0.66173\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1198, Train Loss: 0.26889, Test Loss: 0.66039\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1199, Train Loss: 0.26863, Test Loss: 0.66096\n",
      "learning rate= 4.8828125e-07\n",
      "Epoch 1200, Train Loss: 0.26823, Test Loss: 0.66105\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1201, Train Loss: 0.26787, Test Loss: 0.66087\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1202, Train Loss: 0.26721, Test Loss: 0.66022\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1203, Train Loss: 0.26798, Test Loss: 0.66021\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1204, Train Loss: 0.26738, Test Loss: 0.66078\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1205, Train Loss: 0.26747, Test Loss: 0.66018\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1206, Train Loss: 0.26794, Test Loss: 0.66036\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1207, Train Loss: 0.26802, Test Loss: 0.65997\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1208, Train Loss: 0.26703, Test Loss: 0.65991\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1209, Train Loss: 0.26783, Test Loss: 0.66051\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1210, Train Loss: 0.26817, Test Loss: 0.66029\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1211, Train Loss: 0.26792, Test Loss: 0.66058\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1212, Train Loss: 0.26756, Test Loss: 0.66116\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1213, Train Loss: 0.26716, Test Loss: 0.66031\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1214, Train Loss: 0.26777, Test Loss: 0.66086\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1215, Train Loss: 0.26814, Test Loss: 0.66035\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1216, Train Loss: 0.26847, Test Loss: 0.66014\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1217, Train Loss: 0.26749, Test Loss: 0.66088\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1218, Train Loss: 0.26760, Test Loss: 0.66083\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1219, Train Loss: 0.26727, Test Loss: 0.66074\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1220, Train Loss: 0.26766, Test Loss: 0.66075\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1221, Train Loss: 0.26734, Test Loss: 0.66079\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1222, Train Loss: 0.26775, Test Loss: 0.66051\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1223, Train Loss: 0.26744, Test Loss: 0.66096\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1224, Train Loss: 0.26775, Test Loss: 0.66102\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1225, Train Loss: 0.26667, Test Loss: 0.66023\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1226, Train Loss: 0.26749, Test Loss: 0.66068\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1227, Train Loss: 0.26800, Test Loss: 0.66019\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1228, Train Loss: 0.26752, Test Loss: 0.66046\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1229, Train Loss: 0.26760, Test Loss: 0.66065\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1230, Train Loss: 0.26771, Test Loss: 0.66038\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1231, Train Loss: 0.26740, Test Loss: 0.66026\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1232, Train Loss: 0.26785, Test Loss: 0.66019\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1233, Train Loss: 0.26751, Test Loss: 0.66025\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1234, Train Loss: 0.26772, Test Loss: 0.65973\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1235, Train Loss: 0.26747, Test Loss: 0.66014\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1236, Train Loss: 0.26704, Test Loss: 0.65997\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1237, Train Loss: 0.26729, Test Loss: 0.65987\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1238, Train Loss: 0.26761, Test Loss: 0.66038\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1239, Train Loss: 0.26751, Test Loss: 0.66021\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1240, Train Loss: 0.26743, Test Loss: 0.65983\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1241, Train Loss: 0.26696, Test Loss: 0.66086\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1242, Train Loss: 0.26765, Test Loss: 0.66035\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1243, Train Loss: 0.26725, Test Loss: 0.66023\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1244, Train Loss: 0.26724, Test Loss: 0.65968\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1245, Train Loss: 0.26744, Test Loss: 0.66028\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1246, Train Loss: 0.26759, Test Loss: 0.65995\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1247, Train Loss: 0.26721, Test Loss: 0.65996\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1248, Train Loss: 0.26736, Test Loss: 0.65976\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1249, Train Loss: 0.26791, Test Loss: 0.65965\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1250, Train Loss: 0.26713, Test Loss: 0.65991\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1251, Train Loss: 0.26741, Test Loss: 0.65972\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1252, Train Loss: 0.26708, Test Loss: 0.66065\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1253, Train Loss: 0.26766, Test Loss: 0.65962\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1254, Train Loss: 0.26699, Test Loss: 0.65946\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1255, Train Loss: 0.26726, Test Loss: 0.65990\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1256, Train Loss: 0.26710, Test Loss: 0.65976\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1257, Train Loss: 0.26679, Test Loss: 0.65936\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1258, Train Loss: 0.26695, Test Loss: 0.65966\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1259, Train Loss: 0.26749, Test Loss: 0.66005\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1260, Train Loss: 0.26788, Test Loss: 0.66038\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1261, Train Loss: 0.26741, Test Loss: 0.65988\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1262, Train Loss: 0.26708, Test Loss: 0.66022\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1263, Train Loss: 0.26743, Test Loss: 0.65981\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1264, Train Loss: 0.26704, Test Loss: 0.65992\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1265, Train Loss: 0.26747, Test Loss: 0.65975\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1266, Train Loss: 0.26737, Test Loss: 0.65923\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1267, Train Loss: 0.26748, Test Loss: 0.65999\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1268, Train Loss: 0.26735, Test Loss: 0.65996\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1269, Train Loss: 0.26738, Test Loss: 0.65966\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1270, Train Loss: 0.26720, Test Loss: 0.65942\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1271, Train Loss: 0.26716, Test Loss: 0.65946\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1272, Train Loss: 0.26776, Test Loss: 0.66070\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1273, Train Loss: 0.26781, Test Loss: 0.66003\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1274, Train Loss: 0.26723, Test Loss: 0.65948\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1275, Train Loss: 0.26689, Test Loss: 0.65986\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1276, Train Loss: 0.26735, Test Loss: 0.66029\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1277, Train Loss: 0.26734, Test Loss: 0.65971\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1278, Train Loss: 0.26730, Test Loss: 0.66025\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1279, Train Loss: 0.26771, Test Loss: 0.65960\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1280, Train Loss: 0.26748, Test Loss: 0.65926\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1281, Train Loss: 0.26723, Test Loss: 0.65993\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1282, Train Loss: 0.26710, Test Loss: 0.65990\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1283, Train Loss: 0.26736, Test Loss: 0.65991\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1284, Train Loss: 0.26748, Test Loss: 0.65858\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1285, Train Loss: 0.26724, Test Loss: 0.65954\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1286, Train Loss: 0.26754, Test Loss: 0.65974\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1287, Train Loss: 0.26784, Test Loss: 0.65970\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1288, Train Loss: 0.26716, Test Loss: 0.65966\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1289, Train Loss: 0.26733, Test Loss: 0.65950\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1290, Train Loss: 0.26685, Test Loss: 0.65926\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1291, Train Loss: 0.26757, Test Loss: 0.65923\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1292, Train Loss: 0.26749, Test Loss: 0.65942\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1293, Train Loss: 0.26716, Test Loss: 0.65971\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1294, Train Loss: 0.26711, Test Loss: 0.65917\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1295, Train Loss: 0.26717, Test Loss: 0.65937\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1296, Train Loss: 0.26756, Test Loss: 0.65978\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1297, Train Loss: 0.26708, Test Loss: 0.66021\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1298, Train Loss: 0.26711, Test Loss: 0.65966\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1299, Train Loss: 0.26743, Test Loss: 0.65905\n",
      "learning rate= 2.44140625e-07\n",
      "Epoch 1300, Train Loss: 0.26695, Test Loss: 0.65955\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1301, Train Loss: 0.26696, Test Loss: 0.66011\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1302, Train Loss: 0.26666, Test Loss: 0.65951\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1303, Train Loss: 0.26729, Test Loss: 0.65978\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1304, Train Loss: 0.26629, Test Loss: 0.65965\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1305, Train Loss: 0.26701, Test Loss: 0.65992\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1306, Train Loss: 0.26645, Test Loss: 0.65960\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1307, Train Loss: 0.26711, Test Loss: 0.65948\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1308, Train Loss: 0.26778, Test Loss: 0.65937\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1309, Train Loss: 0.26679, Test Loss: 0.65932\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1310, Train Loss: 0.26686, Test Loss: 0.65942\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1311, Train Loss: 0.26696, Test Loss: 0.65913\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1312, Train Loss: 0.26665, Test Loss: 0.65914\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1313, Train Loss: 0.26691, Test Loss: 0.65869\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1314, Train Loss: 0.26596, Test Loss: 0.65885\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1315, Train Loss: 0.26701, Test Loss: 0.65933\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1316, Train Loss: 0.26693, Test Loss: 0.65982\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1317, Train Loss: 0.26686, Test Loss: 0.65964\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1318, Train Loss: 0.26635, Test Loss: 0.65949\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1319, Train Loss: 0.26672, Test Loss: 0.65961\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1320, Train Loss: 0.26648, Test Loss: 0.65905\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1321, Train Loss: 0.26616, Test Loss: 0.65936\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1322, Train Loss: 0.26599, Test Loss: 0.66002\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1323, Train Loss: 0.26651, Test Loss: 0.65933\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1324, Train Loss: 0.26662, Test Loss: 0.65963\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1325, Train Loss: 0.26655, Test Loss: 0.65957\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1326, Train Loss: 0.26649, Test Loss: 0.65947\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1327, Train Loss: 0.26643, Test Loss: 0.65971\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1328, Train Loss: 0.26626, Test Loss: 0.65981\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1329, Train Loss: 0.26660, Test Loss: 0.66029\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1330, Train Loss: 0.26684, Test Loss: 0.65935\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1331, Train Loss: 0.26635, Test Loss: 0.66007\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1332, Train Loss: 0.26626, Test Loss: 0.65921\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1333, Train Loss: 0.26674, Test Loss: 0.65923\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1334, Train Loss: 0.26688, Test Loss: 0.66014\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1335, Train Loss: 0.26628, Test Loss: 0.65985\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1336, Train Loss: 0.26620, Test Loss: 0.66004\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1337, Train Loss: 0.26627, Test Loss: 0.65933\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1338, Train Loss: 0.26622, Test Loss: 0.65947\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1339, Train Loss: 0.26670, Test Loss: 0.65927\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1340, Train Loss: 0.26641, Test Loss: 0.66013\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1341, Train Loss: 0.26674, Test Loss: 0.65944\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1342, Train Loss: 0.26613, Test Loss: 0.65984\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1343, Train Loss: 0.26661, Test Loss: 0.65940\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1344, Train Loss: 0.26641, Test Loss: 0.65995\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1345, Train Loss: 0.26640, Test Loss: 0.65935\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1346, Train Loss: 0.26668, Test Loss: 0.65930\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1347, Train Loss: 0.26614, Test Loss: 0.65931\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1348, Train Loss: 0.26619, Test Loss: 0.65887\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1349, Train Loss: 0.26636, Test Loss: 0.65938\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1350, Train Loss: 0.26642, Test Loss: 0.65927\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1351, Train Loss: 0.26673, Test Loss: 0.65937\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1352, Train Loss: 0.26686, Test Loss: 0.65927\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1353, Train Loss: 0.26675, Test Loss: 0.65897\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1354, Train Loss: 0.26660, Test Loss: 0.65927\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1355, Train Loss: 0.26670, Test Loss: 0.65902\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1356, Train Loss: 0.26736, Test Loss: 0.65876\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1357, Train Loss: 0.26731, Test Loss: 0.65894\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1358, Train Loss: 0.26628, Test Loss: 0.65950\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1359, Train Loss: 0.26667, Test Loss: 0.65958\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1360, Train Loss: 0.26612, Test Loss: 0.65934\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1361, Train Loss: 0.26698, Test Loss: 0.65897\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1362, Train Loss: 0.26621, Test Loss: 0.65952\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1363, Train Loss: 0.26601, Test Loss: 0.65908\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1364, Train Loss: 0.26644, Test Loss: 0.65941\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1365, Train Loss: 0.26625, Test Loss: 0.65926\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1366, Train Loss: 0.26629, Test Loss: 0.65903\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1367, Train Loss: 0.26645, Test Loss: 0.65852\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1368, Train Loss: 0.26662, Test Loss: 0.65920\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1369, Train Loss: 0.26627, Test Loss: 0.65886\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1370, Train Loss: 0.26638, Test Loss: 0.65930\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1371, Train Loss: 0.26664, Test Loss: 0.65912\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1372, Train Loss: 0.26666, Test Loss: 0.65981\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1373, Train Loss: 0.26664, Test Loss: 0.65898\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1374, Train Loss: 0.26655, Test Loss: 0.65899\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1375, Train Loss: 0.26656, Test Loss: 0.65833\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1376, Train Loss: 0.26632, Test Loss: 0.65967\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1377, Train Loss: 0.26635, Test Loss: 0.65905\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1378, Train Loss: 0.26717, Test Loss: 0.65867\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1379, Train Loss: 0.26637, Test Loss: 0.65938\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1380, Train Loss: 0.26642, Test Loss: 0.65905\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1381, Train Loss: 0.26724, Test Loss: 0.65923\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1382, Train Loss: 0.26643, Test Loss: 0.65917\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1383, Train Loss: 0.26708, Test Loss: 0.65901\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1384, Train Loss: 0.26673, Test Loss: 0.65902\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1385, Train Loss: 0.26626, Test Loss: 0.65900\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1386, Train Loss: 0.26614, Test Loss: 0.65886\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1387, Train Loss: 0.26670, Test Loss: 0.65953\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1388, Train Loss: 0.26640, Test Loss: 0.65896\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1389, Train Loss: 0.26656, Test Loss: 0.65890\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1390, Train Loss: 0.26701, Test Loss: 0.65935\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1391, Train Loss: 0.26592, Test Loss: 0.65854\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1392, Train Loss: 0.26659, Test Loss: 0.65878\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1393, Train Loss: 0.26736, Test Loss: 0.65889\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1394, Train Loss: 0.26702, Test Loss: 0.65916\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1395, Train Loss: 0.26655, Test Loss: 0.65854\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1396, Train Loss: 0.26643, Test Loss: 0.65845\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1397, Train Loss: 0.26728, Test Loss: 0.65877\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1398, Train Loss: 0.26610, Test Loss: 0.65915\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1399, Train Loss: 0.26638, Test Loss: 0.65852\n",
      "learning rate= 1.220703125e-07\n",
      "Epoch 1400, Train Loss: 0.26622, Test Loss: 0.65913\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1401, Train Loss: 0.26604, Test Loss: 0.65837\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1402, Train Loss: 0.26597, Test Loss: 0.65895\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1403, Train Loss: 0.26636, Test Loss: 0.65875\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1404, Train Loss: 0.26638, Test Loss: 0.65900\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1405, Train Loss: 0.26626, Test Loss: 0.65885\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1406, Train Loss: 0.26583, Test Loss: 0.65855\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1407, Train Loss: 0.26618, Test Loss: 0.65866\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1408, Train Loss: 0.26597, Test Loss: 0.65856\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1409, Train Loss: 0.26639, Test Loss: 0.65850\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1410, Train Loss: 0.26594, Test Loss: 0.65840\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1411, Train Loss: 0.26657, Test Loss: 0.65886\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1412, Train Loss: 0.26621, Test Loss: 0.65897\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1413, Train Loss: 0.26611, Test Loss: 0.65857\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1414, Train Loss: 0.26584, Test Loss: 0.65843\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1415, Train Loss: 0.26591, Test Loss: 0.65892\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1416, Train Loss: 0.26683, Test Loss: 0.65892\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1417, Train Loss: 0.26564, Test Loss: 0.65870\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1418, Train Loss: 0.26573, Test Loss: 0.65872\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1419, Train Loss: 0.26564, Test Loss: 0.65881\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1420, Train Loss: 0.26626, Test Loss: 0.65882\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1421, Train Loss: 0.26624, Test Loss: 0.65845\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1422, Train Loss: 0.26609, Test Loss: 0.65846\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1423, Train Loss: 0.26618, Test Loss: 0.65898\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1424, Train Loss: 0.26592, Test Loss: 0.65843\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1425, Train Loss: 0.26672, Test Loss: 0.65855\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1426, Train Loss: 0.26627, Test Loss: 0.65874\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1427, Train Loss: 0.26635, Test Loss: 0.65868\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1428, Train Loss: 0.26613, Test Loss: 0.65865\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1429, Train Loss: 0.26614, Test Loss: 0.65824\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1430, Train Loss: 0.26613, Test Loss: 0.65902\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1431, Train Loss: 0.26596, Test Loss: 0.65859\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1432, Train Loss: 0.26592, Test Loss: 0.65894\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1433, Train Loss: 0.26673, Test Loss: 0.65874\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1434, Train Loss: 0.26598, Test Loss: 0.65900\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1435, Train Loss: 0.26595, Test Loss: 0.65889\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1436, Train Loss: 0.26599, Test Loss: 0.65866\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1437, Train Loss: 0.26587, Test Loss: 0.65826\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1438, Train Loss: 0.26602, Test Loss: 0.65875\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1439, Train Loss: 0.26599, Test Loss: 0.65854\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1440, Train Loss: 0.26558, Test Loss: 0.65856\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1441, Train Loss: 0.26608, Test Loss: 0.65855\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1442, Train Loss: 0.26607, Test Loss: 0.65811\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1443, Train Loss: 0.26573, Test Loss: 0.65846\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1444, Train Loss: 0.26611, Test Loss: 0.65859\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1445, Train Loss: 0.26647, Test Loss: 0.65863\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1446, Train Loss: 0.26604, Test Loss: 0.65839\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1447, Train Loss: 0.26610, Test Loss: 0.65838\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1448, Train Loss: 0.26612, Test Loss: 0.65811\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1449, Train Loss: 0.26616, Test Loss: 0.65857\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1450, Train Loss: 0.26582, Test Loss: 0.65866\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1451, Train Loss: 0.26623, Test Loss: 0.65839\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1452, Train Loss: 0.26603, Test Loss: 0.65818\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1453, Train Loss: 0.26589, Test Loss: 0.65854\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1454, Train Loss: 0.26612, Test Loss: 0.65849\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1455, Train Loss: 0.26611, Test Loss: 0.65829\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1456, Train Loss: 0.26589, Test Loss: 0.65821\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1457, Train Loss: 0.26602, Test Loss: 0.65899\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1458, Train Loss: 0.26651, Test Loss: 0.65864\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1459, Train Loss: 0.26619, Test Loss: 0.65860\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1460, Train Loss: 0.26607, Test Loss: 0.65875\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1461, Train Loss: 0.26571, Test Loss: 0.65806\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1462, Train Loss: 0.26630, Test Loss: 0.65861\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1463, Train Loss: 0.26584, Test Loss: 0.65875\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1464, Train Loss: 0.26575, Test Loss: 0.65858\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1465, Train Loss: 0.26604, Test Loss: 0.65865\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1466, Train Loss: 0.26565, Test Loss: 0.65862\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1467, Train Loss: 0.26622, Test Loss: 0.65837\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1468, Train Loss: 0.26579, Test Loss: 0.65864\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1469, Train Loss: 0.26572, Test Loss: 0.65855\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1470, Train Loss: 0.26599, Test Loss: 0.65864\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1471, Train Loss: 0.26597, Test Loss: 0.65864\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1472, Train Loss: 0.26618, Test Loss: 0.65825\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1473, Train Loss: 0.26582, Test Loss: 0.65848\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1474, Train Loss: 0.26582, Test Loss: 0.65850\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1475, Train Loss: 0.26590, Test Loss: 0.65848\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1476, Train Loss: 0.26595, Test Loss: 0.65827\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1477, Train Loss: 0.26582, Test Loss: 0.65873\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1478, Train Loss: 0.26614, Test Loss: 0.65846\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1479, Train Loss: 0.26589, Test Loss: 0.65882\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1480, Train Loss: 0.26647, Test Loss: 0.65825\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1481, Train Loss: 0.26597, Test Loss: 0.65798\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1482, Train Loss: 0.26593, Test Loss: 0.65839\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1483, Train Loss: 0.26611, Test Loss: 0.65839\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1484, Train Loss: 0.26555, Test Loss: 0.65867\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1485, Train Loss: 0.26607, Test Loss: 0.65841\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1486, Train Loss: 0.26592, Test Loss: 0.65813\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1487, Train Loss: 0.26582, Test Loss: 0.65808\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1488, Train Loss: 0.26618, Test Loss: 0.65866\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1489, Train Loss: 0.26558, Test Loss: 0.65901\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1490, Train Loss: 0.26552, Test Loss: 0.65833\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1491, Train Loss: 0.26642, Test Loss: 0.65832\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1492, Train Loss: 0.26559, Test Loss: 0.65845\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1493, Train Loss: 0.26651, Test Loss: 0.65813\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1494, Train Loss: 0.26575, Test Loss: 0.65818\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1495, Train Loss: 0.26621, Test Loss: 0.65820\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1496, Train Loss: 0.26597, Test Loss: 0.65856\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1497, Train Loss: 0.26602, Test Loss: 0.65848\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1498, Train Loss: 0.26676, Test Loss: 0.65788\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1499, Train Loss: 0.26598, Test Loss: 0.65795\n",
      "learning rate= 6.103515625e-08\n",
      "Epoch 1500, Train Loss: 0.26589, Test Loss: 0.65811\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1501, Train Loss: 0.26601, Test Loss: 0.65815\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1502, Train Loss: 0.26597, Test Loss: 0.65809\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1503, Train Loss: 0.26590, Test Loss: 0.65816\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1504, Train Loss: 0.26590, Test Loss: 0.65791\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1505, Train Loss: 0.26630, Test Loss: 0.65812\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1506, Train Loss: 0.26573, Test Loss: 0.65798\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1507, Train Loss: 0.26717, Test Loss: 0.65811\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1508, Train Loss: 0.26581, Test Loss: 0.65831\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1509, Train Loss: 0.26583, Test Loss: 0.65843\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1510, Train Loss: 0.26595, Test Loss: 0.65831\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1511, Train Loss: 0.26576, Test Loss: 0.65827\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1512, Train Loss: 0.26585, Test Loss: 0.65839\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1513, Train Loss: 0.26582, Test Loss: 0.65772\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1514, Train Loss: 0.26618, Test Loss: 0.65847\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1515, Train Loss: 0.26555, Test Loss: 0.65819\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1516, Train Loss: 0.26588, Test Loss: 0.65805\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1517, Train Loss: 0.26560, Test Loss: 0.65843\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1518, Train Loss: 0.26628, Test Loss: 0.65846\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1519, Train Loss: 0.26586, Test Loss: 0.65846\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1520, Train Loss: 0.26566, Test Loss: 0.65807\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1521, Train Loss: 0.26593, Test Loss: 0.65839\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1522, Train Loss: 0.26602, Test Loss: 0.65841\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1523, Train Loss: 0.26651, Test Loss: 0.65874\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1524, Train Loss: 0.26585, Test Loss: 0.65834\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1525, Train Loss: 0.26581, Test Loss: 0.65857\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1526, Train Loss: 0.26540, Test Loss: 0.65844\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1527, Train Loss: 0.26656, Test Loss: 0.65827\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1528, Train Loss: 0.26586, Test Loss: 0.65835\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1529, Train Loss: 0.26549, Test Loss: 0.65820\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1530, Train Loss: 0.26577, Test Loss: 0.65822\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1531, Train Loss: 0.26629, Test Loss: 0.65823\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1532, Train Loss: 0.26555, Test Loss: 0.65841\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1533, Train Loss: 0.26616, Test Loss: 0.65862\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1534, Train Loss: 0.26555, Test Loss: 0.65842\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1535, Train Loss: 0.26518, Test Loss: 0.65817\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1536, Train Loss: 0.26595, Test Loss: 0.65822\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1537, Train Loss: 0.26580, Test Loss: 0.65830\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1538, Train Loss: 0.26551, Test Loss: 0.65791\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1539, Train Loss: 0.26605, Test Loss: 0.65859\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1540, Train Loss: 0.26604, Test Loss: 0.65829\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1541, Train Loss: 0.26564, Test Loss: 0.65837\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1542, Train Loss: 0.26585, Test Loss: 0.65850\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1543, Train Loss: 0.26578, Test Loss: 0.65857\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1544, Train Loss: 0.26557, Test Loss: 0.65858\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1545, Train Loss: 0.26632, Test Loss: 0.65849\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1546, Train Loss: 0.26546, Test Loss: 0.65835\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1547, Train Loss: 0.26593, Test Loss: 0.65854\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1548, Train Loss: 0.26569, Test Loss: 0.65871\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1549, Train Loss: 0.26517, Test Loss: 0.65841\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1550, Train Loss: 0.26565, Test Loss: 0.65827\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1551, Train Loss: 0.26619, Test Loss: 0.65832\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1552, Train Loss: 0.26586, Test Loss: 0.65826\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1553, Train Loss: 0.26596, Test Loss: 0.65835\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1554, Train Loss: 0.26530, Test Loss: 0.65805\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1555, Train Loss: 0.26565, Test Loss: 0.65831\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1556, Train Loss: 0.26570, Test Loss: 0.65828\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1557, Train Loss: 0.26556, Test Loss: 0.65820\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1558, Train Loss: 0.26550, Test Loss: 0.65810\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1559, Train Loss: 0.26566, Test Loss: 0.65825\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1560, Train Loss: 0.26596, Test Loss: 0.65823\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1561, Train Loss: 0.26586, Test Loss: 0.65809\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1562, Train Loss: 0.26565, Test Loss: 0.65791\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1563, Train Loss: 0.26572, Test Loss: 0.65830\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1564, Train Loss: 0.26546, Test Loss: 0.65811\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1565, Train Loss: 0.26617, Test Loss: 0.65816\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1566, Train Loss: 0.26582, Test Loss: 0.65814\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1567, Train Loss: 0.26564, Test Loss: 0.65837\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1568, Train Loss: 0.26626, Test Loss: 0.65795\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1569, Train Loss: 0.26659, Test Loss: 0.65822\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1570, Train Loss: 0.26556, Test Loss: 0.65846\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1571, Train Loss: 0.26617, Test Loss: 0.65826\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1572, Train Loss: 0.26559, Test Loss: 0.65816\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1573, Train Loss: 0.26596, Test Loss: 0.65820\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1574, Train Loss: 0.26567, Test Loss: 0.65813\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1575, Train Loss: 0.26573, Test Loss: 0.65832\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1576, Train Loss: 0.26570, Test Loss: 0.65824\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1577, Train Loss: 0.26528, Test Loss: 0.65836\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1578, Train Loss: 0.26551, Test Loss: 0.65847\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1579, Train Loss: 0.26624, Test Loss: 0.65831\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1580, Train Loss: 0.26590, Test Loss: 0.65835\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1581, Train Loss: 0.26539, Test Loss: 0.65834\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1582, Train Loss: 0.26578, Test Loss: 0.65812\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1583, Train Loss: 0.26585, Test Loss: 0.65808\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1584, Train Loss: 0.26523, Test Loss: 0.65846\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1585, Train Loss: 0.26613, Test Loss: 0.65812\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1586, Train Loss: 0.26566, Test Loss: 0.65830\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1587, Train Loss: 0.26606, Test Loss: 0.65807\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1588, Train Loss: 0.26555, Test Loss: 0.65816\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1589, Train Loss: 0.26606, Test Loss: 0.65799\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1590, Train Loss: 0.26551, Test Loss: 0.65809\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1591, Train Loss: 0.26565, Test Loss: 0.65802\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1592, Train Loss: 0.26556, Test Loss: 0.65814\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1593, Train Loss: 0.26575, Test Loss: 0.65791\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1594, Train Loss: 0.26612, Test Loss: 0.65813\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1595, Train Loss: 0.26526, Test Loss: 0.65832\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1596, Train Loss: 0.26596, Test Loss: 0.65803\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1597, Train Loss: 0.26548, Test Loss: 0.65801\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1598, Train Loss: 0.26566, Test Loss: 0.65805\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1599, Train Loss: 0.26621, Test Loss: 0.65832\n",
      "learning rate= 3.0517578125e-08\n",
      "Epoch 1600, Train Loss: 0.26609, Test Loss: 0.65817\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1601, Train Loss: 0.26551, Test Loss: 0.65802\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1602, Train Loss: 0.26592, Test Loss: 0.65804\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1603, Train Loss: 0.26572, Test Loss: 0.65815\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1604, Train Loss: 0.26553, Test Loss: 0.65833\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1605, Train Loss: 0.26557, Test Loss: 0.65819\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1606, Train Loss: 0.26546, Test Loss: 0.65808\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1607, Train Loss: 0.26568, Test Loss: 0.65819\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1608, Train Loss: 0.26547, Test Loss: 0.65820\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1609, Train Loss: 0.26530, Test Loss: 0.65791\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1610, Train Loss: 0.26566, Test Loss: 0.65820\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1611, Train Loss: 0.26525, Test Loss: 0.65819\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1612, Train Loss: 0.26619, Test Loss: 0.65803\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1613, Train Loss: 0.26554, Test Loss: 0.65816\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1614, Train Loss: 0.26567, Test Loss: 0.65805\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1615, Train Loss: 0.26587, Test Loss: 0.65817\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1616, Train Loss: 0.26565, Test Loss: 0.65809\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1617, Train Loss: 0.26546, Test Loss: 0.65808\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1618, Train Loss: 0.26597, Test Loss: 0.65792\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1619, Train Loss: 0.26543, Test Loss: 0.65813\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1620, Train Loss: 0.26574, Test Loss: 0.65796\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1621, Train Loss: 0.26578, Test Loss: 0.65814\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1622, Train Loss: 0.26583, Test Loss: 0.65800\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1623, Train Loss: 0.26591, Test Loss: 0.65819\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1624, Train Loss: 0.26560, Test Loss: 0.65802\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1625, Train Loss: 0.26638, Test Loss: 0.65790\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1626, Train Loss: 0.26529, Test Loss: 0.65787\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1627, Train Loss: 0.26568, Test Loss: 0.65819\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1628, Train Loss: 0.26584, Test Loss: 0.65804\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1629, Train Loss: 0.26533, Test Loss: 0.65853\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1630, Train Loss: 0.26597, Test Loss: 0.65823\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1631, Train Loss: 0.26516, Test Loss: 0.65853\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1632, Train Loss: 0.26600, Test Loss: 0.65812\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1633, Train Loss: 0.26563, Test Loss: 0.65811\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1634, Train Loss: 0.26538, Test Loss: 0.65831\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1635, Train Loss: 0.26572, Test Loss: 0.65802\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1636, Train Loss: 0.26616, Test Loss: 0.65824\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1637, Train Loss: 0.26538, Test Loss: 0.65825\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1638, Train Loss: 0.26538, Test Loss: 0.65797\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1639, Train Loss: 0.26550, Test Loss: 0.65803\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1640, Train Loss: 0.26606, Test Loss: 0.65804\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1641, Train Loss: 0.26615, Test Loss: 0.65811\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1642, Train Loss: 0.26547, Test Loss: 0.65808\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1643, Train Loss: 0.26590, Test Loss: 0.65815\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1644, Train Loss: 0.26538, Test Loss: 0.65824\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1645, Train Loss: 0.26543, Test Loss: 0.65811\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1646, Train Loss: 0.26570, Test Loss: 0.65795\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1647, Train Loss: 0.26520, Test Loss: 0.65811\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1648, Train Loss: 0.26567, Test Loss: 0.65802\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1649, Train Loss: 0.26526, Test Loss: 0.65806\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1650, Train Loss: 0.26610, Test Loss: 0.65788\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1651, Train Loss: 0.26591, Test Loss: 0.65793\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1652, Train Loss: 0.26590, Test Loss: 0.65820\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1653, Train Loss: 0.26566, Test Loss: 0.65812\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1654, Train Loss: 0.26548, Test Loss: 0.65786\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1655, Train Loss: 0.26590, Test Loss: 0.65821\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1656, Train Loss: 0.26602, Test Loss: 0.65820\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1657, Train Loss: 0.26588, Test Loss: 0.65812\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1658, Train Loss: 0.26601, Test Loss: 0.65816\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1659, Train Loss: 0.26545, Test Loss: 0.65804\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1660, Train Loss: 0.26586, Test Loss: 0.65796\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1661, Train Loss: 0.26583, Test Loss: 0.65794\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1662, Train Loss: 0.26553, Test Loss: 0.65798\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1663, Train Loss: 0.26573, Test Loss: 0.65786\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1664, Train Loss: 0.26492, Test Loss: 0.65776\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1665, Train Loss: 0.26574, Test Loss: 0.65797\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1666, Train Loss: 0.26530, Test Loss: 0.65811\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1667, Train Loss: 0.26506, Test Loss: 0.65793\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1668, Train Loss: 0.26562, Test Loss: 0.65792\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1669, Train Loss: 0.26519, Test Loss: 0.65795\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1670, Train Loss: 0.26547, Test Loss: 0.65811\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1671, Train Loss: 0.26541, Test Loss: 0.65783\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1672, Train Loss: 0.26578, Test Loss: 0.65800\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1673, Train Loss: 0.26596, Test Loss: 0.65800\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1674, Train Loss: 0.26522, Test Loss: 0.65790\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1675, Train Loss: 0.26584, Test Loss: 0.65785\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1676, Train Loss: 0.26607, Test Loss: 0.65794\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1677, Train Loss: 0.26622, Test Loss: 0.65779\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1678, Train Loss: 0.26524, Test Loss: 0.65789\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1679, Train Loss: 0.26562, Test Loss: 0.65808\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1680, Train Loss: 0.26576, Test Loss: 0.65799\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1681, Train Loss: 0.26552, Test Loss: 0.65797\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1682, Train Loss: 0.26585, Test Loss: 0.65800\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1683, Train Loss: 0.26560, Test Loss: 0.65808\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1684, Train Loss: 0.26596, Test Loss: 0.65816\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1685, Train Loss: 0.26569, Test Loss: 0.65807\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1686, Train Loss: 0.26567, Test Loss: 0.65806\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1687, Train Loss: 0.26594, Test Loss: 0.65803\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1688, Train Loss: 0.26579, Test Loss: 0.65814\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1689, Train Loss: 0.26555, Test Loss: 0.65814\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1690, Train Loss: 0.26599, Test Loss: 0.65799\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1691, Train Loss: 0.26544, Test Loss: 0.65817\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1692, Train Loss: 0.26538, Test Loss: 0.65811\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1693, Train Loss: 0.26548, Test Loss: 0.65820\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1694, Train Loss: 0.26535, Test Loss: 0.65817\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1695, Train Loss: 0.26556, Test Loss: 0.65825\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1696, Train Loss: 0.26577, Test Loss: 0.65821\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1697, Train Loss: 0.26591, Test Loss: 0.65815\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1698, Train Loss: 0.26574, Test Loss: 0.65831\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1699, Train Loss: 0.26558, Test Loss: 0.65819\n",
      "learning rate= 1.52587890625e-08\n",
      "Epoch 1700, Train Loss: 0.26511, Test Loss: 0.65810\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1701, Train Loss: 0.26562, Test Loss: 0.65807\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1702, Train Loss: 0.26637, Test Loss: 0.65814\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1703, Train Loss: 0.26583, Test Loss: 0.65816\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1704, Train Loss: 0.26573, Test Loss: 0.65825\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1705, Train Loss: 0.26553, Test Loss: 0.65824\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1706, Train Loss: 0.26547, Test Loss: 0.65820\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1707, Train Loss: 0.26562, Test Loss: 0.65833\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1708, Train Loss: 0.26530, Test Loss: 0.65817\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1709, Train Loss: 0.26545, Test Loss: 0.65823\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1710, Train Loss: 0.26556, Test Loss: 0.65828\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1711, Train Loss: 0.26569, Test Loss: 0.65803\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1712, Train Loss: 0.26541, Test Loss: 0.65819\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1713, Train Loss: 0.26553, Test Loss: 0.65814\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1714, Train Loss: 0.26570, Test Loss: 0.65811\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1715, Train Loss: 0.26605, Test Loss: 0.65806\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1716, Train Loss: 0.26568, Test Loss: 0.65807\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1717, Train Loss: 0.26583, Test Loss: 0.65823\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1718, Train Loss: 0.26533, Test Loss: 0.65817\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1719, Train Loss: 0.26536, Test Loss: 0.65819\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1720, Train Loss: 0.26523, Test Loss: 0.65810\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1721, Train Loss: 0.26666, Test Loss: 0.65810\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1722, Train Loss: 0.26519, Test Loss: 0.65811\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1723, Train Loss: 0.26551, Test Loss: 0.65810\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1724, Train Loss: 0.26544, Test Loss: 0.65811\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1725, Train Loss: 0.26554, Test Loss: 0.65814\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1726, Train Loss: 0.26577, Test Loss: 0.65799\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1727, Train Loss: 0.26582, Test Loss: 0.65815\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1728, Train Loss: 0.26508, Test Loss: 0.65818\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1729, Train Loss: 0.26519, Test Loss: 0.65808\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1730, Train Loss: 0.26534, Test Loss: 0.65809\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1731, Train Loss: 0.26546, Test Loss: 0.65810\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1732, Train Loss: 0.26565, Test Loss: 0.65800\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1733, Train Loss: 0.26602, Test Loss: 0.65797\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1734, Train Loss: 0.26607, Test Loss: 0.65808\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1735, Train Loss: 0.26598, Test Loss: 0.65810\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1736, Train Loss: 0.26645, Test Loss: 0.65798\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1737, Train Loss: 0.26602, Test Loss: 0.65799\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1738, Train Loss: 0.26554, Test Loss: 0.65793\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1739, Train Loss: 0.26577, Test Loss: 0.65782\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1740, Train Loss: 0.26541, Test Loss: 0.65787\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1741, Train Loss: 0.26563, Test Loss: 0.65791\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1742, Train Loss: 0.26518, Test Loss: 0.65793\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1743, Train Loss: 0.26566, Test Loss: 0.65793\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1744, Train Loss: 0.26544, Test Loss: 0.65788\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1745, Train Loss: 0.26607, Test Loss: 0.65792\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1746, Train Loss: 0.26573, Test Loss: 0.65786\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1747, Train Loss: 0.26503, Test Loss: 0.65798\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1748, Train Loss: 0.26568, Test Loss: 0.65788\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1749, Train Loss: 0.26562, Test Loss: 0.65803\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1750, Train Loss: 0.26532, Test Loss: 0.65801\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1751, Train Loss: 0.26524, Test Loss: 0.65806\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1752, Train Loss: 0.26561, Test Loss: 0.65805\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1753, Train Loss: 0.26516, Test Loss: 0.65810\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1754, Train Loss: 0.26540, Test Loss: 0.65808\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1755, Train Loss: 0.26564, Test Loss: 0.65796\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1756, Train Loss: 0.26558, Test Loss: 0.65796\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1757, Train Loss: 0.26521, Test Loss: 0.65805\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1758, Train Loss: 0.26588, Test Loss: 0.65793\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1759, Train Loss: 0.26532, Test Loss: 0.65801\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1760, Train Loss: 0.26619, Test Loss: 0.65817\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1761, Train Loss: 0.26586, Test Loss: 0.65816\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1762, Train Loss: 0.26554, Test Loss: 0.65813\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1763, Train Loss: 0.26534, Test Loss: 0.65812\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1764, Train Loss: 0.26539, Test Loss: 0.65826\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1765, Train Loss: 0.26572, Test Loss: 0.65812\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1766, Train Loss: 0.26640, Test Loss: 0.65810\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1767, Train Loss: 0.26629, Test Loss: 0.65806\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1768, Train Loss: 0.26574, Test Loss: 0.65807\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1769, Train Loss: 0.26562, Test Loss: 0.65799\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1770, Train Loss: 0.26510, Test Loss: 0.65822\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1771, Train Loss: 0.26541, Test Loss: 0.65834\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1772, Train Loss: 0.26521, Test Loss: 0.65811\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1773, Train Loss: 0.26519, Test Loss: 0.65816\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1774, Train Loss: 0.26588, Test Loss: 0.65817\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1775, Train Loss: 0.26518, Test Loss: 0.65810\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1776, Train Loss: 0.26521, Test Loss: 0.65818\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1777, Train Loss: 0.26593, Test Loss: 0.65806\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1778, Train Loss: 0.26549, Test Loss: 0.65802\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1779, Train Loss: 0.26568, Test Loss: 0.65824\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1780, Train Loss: 0.26623, Test Loss: 0.65825\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1781, Train Loss: 0.26604, Test Loss: 0.65822\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1782, Train Loss: 0.26550, Test Loss: 0.65816\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1783, Train Loss: 0.26505, Test Loss: 0.65822\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1784, Train Loss: 0.26553, Test Loss: 0.65819\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1785, Train Loss: 0.26524, Test Loss: 0.65833\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1786, Train Loss: 0.26583, Test Loss: 0.65825\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1787, Train Loss: 0.26527, Test Loss: 0.65823\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1788, Train Loss: 0.26542, Test Loss: 0.65825\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1789, Train Loss: 0.26550, Test Loss: 0.65824\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1790, Train Loss: 0.26568, Test Loss: 0.65837\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1791, Train Loss: 0.26604, Test Loss: 0.65838\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1792, Train Loss: 0.26515, Test Loss: 0.65827\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1793, Train Loss: 0.26588, Test Loss: 0.65840\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1794, Train Loss: 0.26533, Test Loss: 0.65819\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1795, Train Loss: 0.26525, Test Loss: 0.65824\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1796, Train Loss: 0.26513, Test Loss: 0.65831\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1797, Train Loss: 0.26567, Test Loss: 0.65829\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1798, Train Loss: 0.26597, Test Loss: 0.65816\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1799, Train Loss: 0.26567, Test Loss: 0.65817\n",
      "learning rate= 7.62939453125e-09\n",
      "Epoch 1800, Train Loss: 0.26549, Test Loss: 0.65802\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1801, Train Loss: 0.26575, Test Loss: 0.65802\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1802, Train Loss: 0.26588, Test Loss: 0.65805\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1803, Train Loss: 0.26545, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1804, Train Loss: 0.26500, Test Loss: 0.65808\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1805, Train Loss: 0.26542, Test Loss: 0.65810\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1806, Train Loss: 0.26632, Test Loss: 0.65812\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1807, Train Loss: 0.26557, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1808, Train Loss: 0.26583, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1809, Train Loss: 0.26573, Test Loss: 0.65808\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1810, Train Loss: 0.26533, Test Loss: 0.65814\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1811, Train Loss: 0.26575, Test Loss: 0.65810\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1812, Train Loss: 0.26587, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1813, Train Loss: 0.26503, Test Loss: 0.65816\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1814, Train Loss: 0.26533, Test Loss: 0.65817\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1815, Train Loss: 0.26545, Test Loss: 0.65812\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1816, Train Loss: 0.26541, Test Loss: 0.65815\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1817, Train Loss: 0.26530, Test Loss: 0.65816\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1818, Train Loss: 0.26587, Test Loss: 0.65815\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1819, Train Loss: 0.26580, Test Loss: 0.65819\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1820, Train Loss: 0.26540, Test Loss: 0.65816\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1821, Train Loss: 0.26543, Test Loss: 0.65821\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1822, Train Loss: 0.26546, Test Loss: 0.65816\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1823, Train Loss: 0.26571, Test Loss: 0.65819\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1824, Train Loss: 0.26580, Test Loss: 0.65812\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1825, Train Loss: 0.26576, Test Loss: 0.65815\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1826, Train Loss: 0.26553, Test Loss: 0.65819\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1827, Train Loss: 0.26520, Test Loss: 0.65819\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1828, Train Loss: 0.26562, Test Loss: 0.65824\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1829, Train Loss: 0.26554, Test Loss: 0.65822\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1830, Train Loss: 0.26554, Test Loss: 0.65820\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1831, Train Loss: 0.26552, Test Loss: 0.65819\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1832, Train Loss: 0.26540, Test Loss: 0.65817\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1833, Train Loss: 0.26566, Test Loss: 0.65809\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1834, Train Loss: 0.26513, Test Loss: 0.65812\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1835, Train Loss: 0.26617, Test Loss: 0.65817\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1836, Train Loss: 0.26534, Test Loss: 0.65816\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1837, Train Loss: 0.26562, Test Loss: 0.65808\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1838, Train Loss: 0.26533, Test Loss: 0.65805\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1839, Train Loss: 0.26612, Test Loss: 0.65810\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1840, Train Loss: 0.26569, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1841, Train Loss: 0.26550, Test Loss: 0.65812\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1842, Train Loss: 0.26564, Test Loss: 0.65812\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1843, Train Loss: 0.26558, Test Loss: 0.65808\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1844, Train Loss: 0.26563, Test Loss: 0.65806\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1845, Train Loss: 0.26536, Test Loss: 0.65808\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1846, Train Loss: 0.26557, Test Loss: 0.65815\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1847, Train Loss: 0.26556, Test Loss: 0.65812\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1848, Train Loss: 0.26579, Test Loss: 0.65811\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1849, Train Loss: 0.26540, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1850, Train Loss: 0.26527, Test Loss: 0.65815\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1851, Train Loss: 0.26555, Test Loss: 0.65816\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1852, Train Loss: 0.26593, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1853, Train Loss: 0.26539, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1854, Train Loss: 0.26562, Test Loss: 0.65815\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1855, Train Loss: 0.26518, Test Loss: 0.65805\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1856, Train Loss: 0.26595, Test Loss: 0.65803\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1857, Train Loss: 0.26522, Test Loss: 0.65808\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1858, Train Loss: 0.26557, Test Loss: 0.65806\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1859, Train Loss: 0.26569, Test Loss: 0.65806\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1860, Train Loss: 0.26603, Test Loss: 0.65800\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1861, Train Loss: 0.26566, Test Loss: 0.65814\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1862, Train Loss: 0.26566, Test Loss: 0.65811\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1863, Train Loss: 0.26489, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1864, Train Loss: 0.26577, Test Loss: 0.65810\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1865, Train Loss: 0.26533, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1866, Train Loss: 0.26514, Test Loss: 0.65812\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1867, Train Loss: 0.26582, Test Loss: 0.65812\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1868, Train Loss: 0.26538, Test Loss: 0.65806\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1869, Train Loss: 0.26506, Test Loss: 0.65810\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1870, Train Loss: 0.26569, Test Loss: 0.65811\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1871, Train Loss: 0.26564, Test Loss: 0.65807\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1872, Train Loss: 0.26583, Test Loss: 0.65805\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1873, Train Loss: 0.26551, Test Loss: 0.65806\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1874, Train Loss: 0.26553, Test Loss: 0.65807\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1875, Train Loss: 0.26544, Test Loss: 0.65807\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1876, Train Loss: 0.26565, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1877, Train Loss: 0.26604, Test Loss: 0.65813\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1878, Train Loss: 0.26544, Test Loss: 0.65810\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1879, Train Loss: 0.26568, Test Loss: 0.65807\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1880, Train Loss: 0.26571, Test Loss: 0.65815\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1881, Train Loss: 0.26642, Test Loss: 0.65811\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1882, Train Loss: 0.26534, Test Loss: 0.65810\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1883, Train Loss: 0.26579, Test Loss: 0.65810\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1884, Train Loss: 0.26563, Test Loss: 0.65818\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1885, Train Loss: 0.26585, Test Loss: 0.65814\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1886, Train Loss: 0.26537, Test Loss: 0.65809\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1887, Train Loss: 0.26525, Test Loss: 0.65816\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1888, Train Loss: 0.26567, Test Loss: 0.65816\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1889, Train Loss: 0.26493, Test Loss: 0.65809\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1890, Train Loss: 0.26515, Test Loss: 0.65810\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1891, Train Loss: 0.26519, Test Loss: 0.65815\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1892, Train Loss: 0.26523, Test Loss: 0.65817\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1893, Train Loss: 0.26542, Test Loss: 0.65811\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1894, Train Loss: 0.26557, Test Loss: 0.65811\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1895, Train Loss: 0.26547, Test Loss: 0.65809\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1896, Train Loss: 0.26627, Test Loss: 0.65806\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1897, Train Loss: 0.26586, Test Loss: 0.65808\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1898, Train Loss: 0.26568, Test Loss: 0.65805\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1899, Train Loss: 0.26559, Test Loss: 0.65802\n",
      "learning rate= 3.814697265625e-09\n",
      "Epoch 1900, Train Loss: 0.26605, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1901, Train Loss: 0.26509, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1902, Train Loss: 0.26532, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1903, Train Loss: 0.26638, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1904, Train Loss: 0.26524, Test Loss: 0.65807\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1905, Train Loss: 0.26522, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1906, Train Loss: 0.26542, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1907, Train Loss: 0.26528, Test Loss: 0.65805\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1908, Train Loss: 0.26536, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1909, Train Loss: 0.26531, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1910, Train Loss: 0.26553, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1911, Train Loss: 0.26547, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1912, Train Loss: 0.26517, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1913, Train Loss: 0.26593, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1914, Train Loss: 0.26545, Test Loss: 0.65809\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1915, Train Loss: 0.26547, Test Loss: 0.65806\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1916, Train Loss: 0.26576, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1917, Train Loss: 0.26491, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1918, Train Loss: 0.26563, Test Loss: 0.65805\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1919, Train Loss: 0.26560, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1920, Train Loss: 0.26597, Test Loss: 0.65806\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1921, Train Loss: 0.26530, Test Loss: 0.65806\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1922, Train Loss: 0.26585, Test Loss: 0.65800\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1923, Train Loss: 0.26584, Test Loss: 0.65805\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1924, Train Loss: 0.26546, Test Loss: 0.65798\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1925, Train Loss: 0.26591, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1926, Train Loss: 0.26550, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1927, Train Loss: 0.26523, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1928, Train Loss: 0.26538, Test Loss: 0.65805\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1929, Train Loss: 0.26512, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1930, Train Loss: 0.26511, Test Loss: 0.65805\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1931, Train Loss: 0.26554, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1932, Train Loss: 0.26615, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1933, Train Loss: 0.26525, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1934, Train Loss: 0.26555, Test Loss: 0.65807\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1935, Train Loss: 0.26576, Test Loss: 0.65810\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1936, Train Loss: 0.26527, Test Loss: 0.65811\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1937, Train Loss: 0.26528, Test Loss: 0.65809\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1938, Train Loss: 0.26610, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1939, Train Loss: 0.26546, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1940, Train Loss: 0.26520, Test Loss: 0.65807\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1941, Train Loss: 0.26570, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1942, Train Loss: 0.26556, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1943, Train Loss: 0.26507, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1944, Train Loss: 0.26547, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1945, Train Loss: 0.26538, Test Loss: 0.65805\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1946, Train Loss: 0.26562, Test Loss: 0.65806\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1947, Train Loss: 0.26535, Test Loss: 0.65807\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1948, Train Loss: 0.26511, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1949, Train Loss: 0.26562, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1950, Train Loss: 0.26578, Test Loss: 0.65806\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1951, Train Loss: 0.26543, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1952, Train Loss: 0.26612, Test Loss: 0.65809\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1953, Train Loss: 0.26550, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1954, Train Loss: 0.26533, Test Loss: 0.65807\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1955, Train Loss: 0.26564, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1956, Train Loss: 0.26569, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1957, Train Loss: 0.26559, Test Loss: 0.65814\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1958, Train Loss: 0.26544, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1959, Train Loss: 0.26550, Test Loss: 0.65807\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1960, Train Loss: 0.26548, Test Loss: 0.65810\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1961, Train Loss: 0.26571, Test Loss: 0.65810\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1962, Train Loss: 0.26554, Test Loss: 0.65809\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1963, Train Loss: 0.26559, Test Loss: 0.65808\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1964, Train Loss: 0.26571, Test Loss: 0.65805\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1965, Train Loss: 0.26534, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1966, Train Loss: 0.26554, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1967, Train Loss: 0.26528, Test Loss: 0.65806\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1968, Train Loss: 0.26534, Test Loss: 0.65806\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1969, Train Loss: 0.26554, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1970, Train Loss: 0.26535, Test Loss: 0.65805\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1971, Train Loss: 0.26559, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1972, Train Loss: 0.26544, Test Loss: 0.65801\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1973, Train Loss: 0.26572, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1974, Train Loss: 0.26543, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1975, Train Loss: 0.26555, Test Loss: 0.65805\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1976, Train Loss: 0.26619, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1977, Train Loss: 0.26559, Test Loss: 0.65801\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1978, Train Loss: 0.26532, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1979, Train Loss: 0.26497, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1980, Train Loss: 0.26532, Test Loss: 0.65806\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1981, Train Loss: 0.26601, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1982, Train Loss: 0.26568, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1983, Train Loss: 0.26564, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1984, Train Loss: 0.26548, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1985, Train Loss: 0.26542, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1986, Train Loss: 0.26572, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1987, Train Loss: 0.26579, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1988, Train Loss: 0.26523, Test Loss: 0.65800\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1989, Train Loss: 0.26520, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1990, Train Loss: 0.26554, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1991, Train Loss: 0.26554, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1992, Train Loss: 0.26488, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1993, Train Loss: 0.26531, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1994, Train Loss: 0.26538, Test Loss: 0.65802\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1995, Train Loss: 0.26571, Test Loss: 0.65800\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1996, Train Loss: 0.26574, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1997, Train Loss: 0.26658, Test Loss: 0.65803\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1998, Train Loss: 0.26551, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 1999, Train Loss: 0.26534, Test Loss: 0.65804\n",
      "learning rate= 1.9073486328125e-09\n",
      "Epoch 2000, Train Loss: 0.26571, Test Loss: 0.65806\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "from Models.SeEANet import SeEANet\n",
    "from Models.MyoNet import MyoNet\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "from utils.tools import make_datasets,make_dir\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "\n",
    "# 数据加载\n",
    "data_Dir = \"/home/admin123/SATData/data\"\n",
    "# 实例化数据集\n",
    "emg_data, angle_data = make_datasets(data_Dir, peopleList=['S01'], exp_class=\"MJ\", cluster_num=6, fusionMethod=\"PCA\", windowLength=256, stepLength=1, delta_T=20)\n",
    "\n",
    "semgData = torch.tensor(emg_data, dtype=torch.float32)\n",
    "angleData = torch.tensor(angle_data, dtype=torch.float32)\n",
    "print(\"semg 数据形状为：\", semgData.shape)\n",
    "print(\"angle 数据形状为：\", angleData.shape)\n",
    "\n",
    "dataset = TensorDataset(semgData, angleData)\n",
    "\n",
    "# 定义划分比例\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "# 数据集分割\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 数据加载\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# 初始化模型、优化器和损失函数\n",
    "model = SeEANet(PreNum=3)\n",
    "modelName = \"SeEANet\"\n",
    "model = model.cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=100, gamma=0.5)\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "formatted_time = datetime.now().strftime(\"%m-%d-%H:%M:%S\")\n",
    "checkpoint_save_Dir = os.path.join(\"/home/admin123/SATData\", \"Run\", modelName, formatted_time)\n",
    "make_dir(checkpoint_save_Dir)\n",
    "checkpoint_save_path = os.path.join(checkpoint_save_Dir,\"best.pth\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "train_running_loss_ls = []\n",
    "test_running_loss_ls = []\n",
    "test_accuracies = []\n",
    "for epoch in range(1, 2000+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(\"learning rate=\", optimizer.param_groups[0]['lr'])\n",
    "    scheduler.step()\n",
    "    train_running_loss_ls.append(running_loss)\n",
    "\n",
    "    \n",
    "    # 测试模型\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss\n",
    "    test_running_loss_ls.append(test_loss)\n",
    "    # if epoch % 50 == 0:\n",
    "    print('Epoch %d, Train Loss: %.5f, Test Loss: %.5f'%(epoch, running_loss, test_loss*4))\n",
    "\n",
    "    if running_loss < best_loss:\n",
    "        best_loss = running_loss\n",
    "        torch.save(model.state_dict(), checkpoint_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 文件已保存到: /home/admin123/SATData/Run/SeEANet/04-13-16:55:51/output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# 计算性能指标\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "file_path = os.path.join(checkpoint_save_Dir, \"output.csv\")\n",
    "# 将两个列表写入到 CSV 文件\n",
    "with open(file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    \n",
    "    # 写入表头（可选）\n",
    "    writer.writerow([\"train_loss\", \"test_loss\"])\n",
    "    \n",
    "    # 写入数据\n",
    "    for item1, item2 in zip(train_running_loss_ls, test_running_loss_ls):\n",
    "        writer.writerow([item1, item2])\n",
    "print(f\"loss 文件已保存到: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SeEANet:\n\tMissing key(s) in state_dict: \"InceptionBlk1.b1_conv1.weight\", \"InceptionBlk1.b1_conv1.bias\", \"InceptionBlk1.b2_conv1.weight\", \"InceptionBlk1.b2_conv1.bias\", \"InceptionBlk1.b2_conv2.weight\", \"InceptionBlk1.b2_conv2.bias\", \"InceptionBlk1.b3_conv1.weight\", \"InceptionBlk1.b3_conv1.bias\", \"InceptionBlk1.b3_conv2.weight\", \"InceptionBlk1.b3_conv2.bias\", \"InceptionBlk1.b4_conv1.weight\", \"InceptionBlk1.b4_conv1.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 初始化模型结构\u001b[39;00m\n\u001b[1;32m      7\u001b[0m model_eval \u001b[38;5;241m=\u001b[39m SeEANet(PreNum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel_eval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_save_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model_eval \u001b[38;5;241m=\u001b[39m model_eval\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model_eval = model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SeEANet:\n\tMissing key(s) in state_dict: \"InceptionBlk1.b1_conv1.weight\", \"InceptionBlk1.b1_conv1.bias\", \"InceptionBlk1.b2_conv1.weight\", \"InceptionBlk1.b2_conv1.bias\", \"InceptionBlk1.b2_conv2.weight\", \"InceptionBlk1.b2_conv2.bias\", \"InceptionBlk1.b3_conv1.weight\", \"InceptionBlk1.b3_conv1.bias\", \"InceptionBlk1.b3_conv2.weight\", \"InceptionBlk1.b3_conv2.bias\", \"InceptionBlk1.b4_conv1.weight\", \"InceptionBlk1.b4_conv1.bias\". "
     ]
    }
   ],
   "source": [
    "# 计算性能指标\n",
    "import matplotlib.pyplot as plt\n",
    "from Models.SeEANet import SeEANet\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "# 初始化模型结构\n",
    "model_eval = SeEANet(PreNum=3)\n",
    "model_eval.load_state_dict(torch.load(checkpoint_save_path, weights_only=False))\n",
    "model_eval = model_eval.cuda()\n",
    "# model_eval = model\n",
    "\n",
    "model_eval.eval()\n",
    "truthAnglesList = []\n",
    "preAnglesList = []\n",
    "for inputs, labels in test_loader:\n",
    "    truthAnglesList.append(labels)\n",
    "    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    outputs = model_eval(inputs)\n",
    "    preAnglesList.append(outputs.cpu().detach().numpy())\n",
    "\n",
    "truthAngles = np.concatenate(truthAnglesList, axis=0)\n",
    "preAngles = np.concatenate(preAnglesList, axis=0)\n",
    "\n",
    "tru_angle_1 = truthAngles[:,0].T\n",
    "tru_angle_2 = truthAngles[:,1].T\n",
    "tru_angle_3 = truthAngles[:,2].T\n",
    "\n",
    "pre_angle_1 = preAngles[:,0].T\n",
    "pre_angle_2 = preAngles[:,1].T\n",
    "pre_angle_3 = preAngles[:,2].T\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(np.abs(tru_angle_1-pre_angle_1), bins=100, alpha=0.8, label=\"elv_angle\")\n",
    "plt.hist(np.abs(tru_angle_2-pre_angle_2), bins=100, alpha=0.8, label=\"shoulder_elv\")\n",
    "plt.hist(np.abs(tru_angle_3-pre_angle_3), bins=100, alpha=0.8, label=\"elbow_flexion\")\n",
    "plt.xlim((0, 2))\n",
    "plt.legend()\n",
    "plt.title('Histogram Example')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
