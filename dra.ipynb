{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型结构:\n",
      "SingleLayerNet(\n",
      "  (linear): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n",
      "\n",
      "输入形状: torch.Size([3, 10, 20])\n",
      "输出形状: torch.Size([3, 10, 5])\n",
      "\n",
      "前3个样本的输出示例:\n",
      "tensor([[[-0.0756,  0.7473,  1.0564,  0.2725,  0.8614],\n",
      "         [ 0.1039,  0.8008,  0.9195,  1.0703,  0.9789],\n",
      "         [ 0.3596, -0.3920,  0.0724, -0.3199, -0.2100],\n",
      "         [ 0.2259,  0.1826,  0.3812, -0.0281,  0.2861],\n",
      "         [-0.1274,  0.5550,  0.4859,  0.1172,  0.3532],\n",
      "         [-0.4267,  0.5544,  0.3043,  0.0673,  0.6002],\n",
      "         [ 0.3817,  0.4111,  0.0077, -0.1533, -0.0500],\n",
      "         [ 0.3110,  0.3331, -0.3116,  0.4077,  0.9141],\n",
      "         [ 0.1817,  0.9595,  0.2959,  0.2309,  0.6635],\n",
      "         [-0.7728, -1.2180,  0.2332, -0.1493,  0.1157]],\n",
      "\n",
      "        [[-0.3624, -0.1314,  1.9791, -0.0159,  0.6907],\n",
      "         [-0.3848,  0.4734,  1.5797, -0.3050,  0.6885],\n",
      "         [-0.5546, -0.7310,  0.0978,  0.4279, -0.2886],\n",
      "         [-0.1615, -0.2032, -0.1288,  1.0454,  0.0314],\n",
      "         [ 0.1704,  0.5333,  1.6015,  0.4394,  0.5292],\n",
      "         [-0.5032,  0.1130,  0.6732, -0.1713, -0.4470],\n",
      "         [-0.7069,  0.8421,  0.4413,  0.3364,  0.5252],\n",
      "         [ 0.5930,  0.1308, -1.0457, -0.0890,  1.1037],\n",
      "         [-0.6212, -0.5297, -0.4567,  0.6303,  0.5242],\n",
      "         [ 0.3125,  0.2133,  0.7977, -0.9073,  0.3234]],\n",
      "\n",
      "        [[ 0.3927, -0.0224, -0.0668, -0.0891, -0.2365],\n",
      "         [ 0.8692,  0.5208,  0.5019,  0.0096,  1.4701],\n",
      "         [-0.3417,  0.3895,  0.2997,  0.3067,  0.5383],\n",
      "         [-0.0590,  0.5925,  0.9550,  0.0945,  0.0968],\n",
      "         [ 0.0914,  0.7415,  1.3109, -0.8635,  1.1530],\n",
      "         [-0.2700, -0.2831,  0.0333,  0.0721,  0.0898],\n",
      "         [-0.0640, -0.5410, -0.3327, -0.3577,  1.8268],\n",
      "         [-0.9802,  0.5482, -0.1678,  0.0532, -1.2531],\n",
      "         [-0.1197,  0.0082,  0.2936, -0.0503,  0.0271],\n",
      "         [-0.0576, -0.5354,  0.0953, -0.6065,  0.8821]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义神经网络\n",
    "class SingleLayerNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SingleLayerNet, self).__init__()\n",
    "        # 定义单个线性层\n",
    "        self.linear = nn.Linear(in_features=input_dim, \n",
    "                               out_features=output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 前向传播：通过线性层\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# --------------------------\n",
    "# 示例用法\n",
    "# --------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 设置参数\n",
    "    input_dim = 10   # 输入特征维度\n",
    "    output_dim = 5   # 输出特征维度\n",
    "    batch_size = 3   # 批大小\n",
    "\n",
    "    # 创建模型实例\n",
    "    model = SingleLayerNet(20, output_dim)\n",
    "    \n",
    "    # 打印模型结构\n",
    "    print(\"模型结构:\")\n",
    "    print(model)\n",
    "    \n",
    "    # 生成随机输入数据\n",
    "    x = torch.randn(batch_size, input_dim, 20)  # shape: [3, 10]\n",
    "    \n",
    "    # 前向传播\n",
    "    output = model(x)\n",
    "    \n",
    "    # 打印输出结果\n",
    "    print(\"\\n输入形状:\", x.shape)\n",
    "    print(\"输出形状:\", output.shape)\n",
    "    print(\"\\n前3个样本的输出示例:\")\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_259965/3560933107.py:93: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return (torch.tensor(X).float().unsqueeze(-1),  # 形状：(样本数, lookback, 1)\n",
      "/home/admin123/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerPredictor(\n",
      "  (embedding): Linear(in_features=1, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 001, Loss: 76.5870\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 002, Loss: 41.6784\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 003, Loss: 37.5518\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 004, Loss: 41.8427\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 005, Loss: 31.1289\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 006, Loss: 22.4238\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 007, Loss: 40.3901\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 008, Loss: 37.9271\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 009, Loss: 35.2371\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 010, Loss: 33.9103\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 011, Loss: 29.0680\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 012, Loss: 28.9737\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 013, Loss: 35.0446\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 014, Loss: 21.7840\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 015, Loss: 21.7277\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 016, Loss: 18.6637\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 017, Loss: 16.0329\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 018, Loss: 11.5073\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 019, Loss: 13.5899\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 020, Loss: 17.7288\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 021, Loss: 12.8476\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([9, 15, 1])\n",
      "torch.Size([9])\n",
      "Epoch 022, Loss: 9.8179\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n",
      "torch.Size([16, 15, 1])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 119\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n\u001b[0;32m--> 119\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 115\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(output, y_batch)\n\u001b[1;32m    114\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 115\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/optim/adam.py:425\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    423\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m device_beta1)\n\u001b[0;32m--> 425\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    428\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, TensorDataset\n",
    "from Models.SeEANet import SeEANet\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "from utils.tools import make_datasets,make_dir\n",
    "from torchsummary import summary\n",
    "\n",
    "# 生成含趋势、周期和噪声的合成数据\n",
    "def generate_finance_data(seq_length=200, noise_level=0.5):\n",
    "    t = np.linspace(0, 20, seq_length)\n",
    "    trend = 0.05 * t ** 2  # 二次趋势项\n",
    "    seasonality = 2 * np.sin(0.5 * t) + 1.5 * np.cos(1.2 * t)  # 多频率周期项\n",
    "    y_true = trend + seasonality\n",
    "    noise = noise_level * np.random.randn(seq_length)\n",
    "    y_noisy = y_true + noise\n",
    "    return t, y_true, y_noisy\n",
    " \n",
    "t, y_true, y_noisy = generate_finance_data()\n",
    "\n",
    "def kalman_filter_1d(y_noisy, Q=1e-4, R=0.1):\n",
    "    n = len(y_noisy)\n",
    "    x_est = np.zeros(n)  # 状态估计序列\n",
    "    P = np.zeros(n)  # 协方差序列\n",
    "    x_est[0] = y_noisy[0]  # 初始状态：第一个观测值\n",
    "    P[0] = 1.0  # 初始协方差：假设初始估计不确定性大\n",
    "    \n",
    "    for k in range(1, n):\n",
    "        # 预测阶段：基于前一时刻估计值外推\n",
    "        x_pred = x_est[k-1]  # 状态预测（假设A=1，无控制输入）\n",
    "        P_pred = P[k-1] + Q  # 协方差预测：累积过程噪声Q\n",
    "        \n",
    "        # 更新阶段：用当前观测值修正预测\n",
    "        K = P_pred / (P_pred + R)  # 卡尔曼增益：噪声越小，K越接近1\n",
    "        x_est[k] = x_pred + K * (y_noisy[k] - x_pred)  # 状态更新：融合预测与观测\n",
    "        P[k] = (1 - K) * P_pred  # 协方差更新：观测降低不确定性\n",
    "    return x_est\n",
    " \n",
    "y_kalman = kalman_filter_1d(y_noisy)\n",
    "\n",
    "class TransformerPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=1, d_model=64, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Linear(input_dim, d_model)  # 输入特征映射\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)  # 位置编码\n",
    "        # Transformer编码器层（含因果掩码，避免未来信息泄露）\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward=256, dropout=dropout, batch_first=True,\n",
    "            norm_first=True  # 先归一化，再残差连接，提升稳定性\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(d_model, 1)  # 输出层\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 输入形状：(batch_size, seq_length, input_dim)\n",
    "        x = self.embedding(x) * np.sqrt(self.d_model)  # 缩放嵌入，稳定梯度\n",
    "        x = self.pos_encoder(x)  # 注入位置信息\n",
    "        # 因果掩码：确保第i步只能看到前i步的信息\n",
    "        seq_len = x.size(1)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1)\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=None, mask=mask)\n",
    "        output = self.fc(x)  # 输出形状：(batch_size, seq_length, 1)\n",
    "        return output\n",
    " \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 预计算位置编码，避免重复计算\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x形状：(batch_size, seq_length, d_model)\n",
    "        x = x + self.pe[:x.size(1), :]  # 叠加位置编码\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "def create_dataset(y, lookback=15):\n",
    "    \"\"\"将序列转换为滑动窗口样本：输入为前lookback步，标签为第lookback+1步\"\"\"\n",
    "    X, y_label = [], []\n",
    "    for i in range(lookback, len(y)):\n",
    "        X.append(y[i-lookback:i])\n",
    "        y_label.append(y[i])\n",
    "    return (torch.tensor(X).float().unsqueeze(-1),  # 形状：(样本数, lookback, 1)\n",
    "            torch.tensor(y_label).float())\n",
    " \n",
    "X_train, y_train = create_dataset(y_kalman)  # 输入为卡尔曼滤波后的数据\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    " \n",
    "model = TransformerPredictor()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    " \n",
    "def train_model(model, dataloader, epochs=80):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for X_batch, y_batch in dataloader:\n",
    "            print(X_batch.shape)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)[:, -1, :].squeeze()  # 取最后一步预测\n",
    "            print(output.shape)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1:03d}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "print(model)\n",
    "train_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200555/2721485015.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
      "  return (torch.tensor(X).float().unsqueeze(-1),  # 形状：(样本数, lookback, 1)\n",
      "/home/admin123/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerPredictor(\n",
      "  (embedding): Linear(in_features=1, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 001, Loss: 486.5779\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 002, Loss: 95.5694\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 003, Loss: 39.7645\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 004, Loss: 48.3676\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 005, Loss: 23.7659\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 006, Loss: 32.2928\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 007, Loss: 25.7448\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 008, Loss: 26.5949\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 009, Loss: 34.2277\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 010, Loss: 31.2752\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 011, Loss: 29.4633\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 012, Loss: 23.5470\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 013, Loss: 32.8014\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 014, Loss: 19.0443\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 015, Loss: 19.8375\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 016, Loss: 26.1151\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 017, Loss: 17.7546\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 018, Loss: 18.6386\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 019, Loss: 19.8758\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 020, Loss: 31.5493\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 021, Loss: 12.6839\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 022, Loss: 7.9433\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 023, Loss: 15.5670\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 024, Loss: 9.4076\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 025, Loss: 16.5654\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 026, Loss: 9.6252\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 027, Loss: 11.2999\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 028, Loss: 10.1947\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 029, Loss: 8.4089\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 030, Loss: 11.1996\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 031, Loss: 6.6719\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 032, Loss: 7.3102\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 033, Loss: 10.8810\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 034, Loss: 7.8253\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 035, Loss: 6.1977\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 036, Loss: 6.4395\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 037, Loss: 5.5906\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 038, Loss: 6.5786\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 039, Loss: 5.1032\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 040, Loss: 4.8064\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 041, Loss: 5.8934\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 042, Loss: 3.3362\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 043, Loss: 6.9762\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 044, Loss: 3.4017\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 045, Loss: 2.7087\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 046, Loss: 2.3071\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 047, Loss: 2.4238\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 048, Loss: 2.2689\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 049, Loss: 2.2925\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 050, Loss: 1.8588\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 051, Loss: 2.0302\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 052, Loss: 1.9143\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 053, Loss: 1.7745\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 054, Loss: 2.3466\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 055, Loss: 2.0011\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 056, Loss: 1.6173\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 057, Loss: 1.9474\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 058, Loss: 2.5385\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 059, Loss: 1.9409\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 060, Loss: 1.2423\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 061, Loss: 1.2994\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 062, Loss: 1.3093\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 063, Loss: 1.7209\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 064, Loss: 1.3402\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 065, Loss: 0.9718\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 066, Loss: 1.1447\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 067, Loss: 1.7072\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 068, Loss: 1.0220\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 069, Loss: 1.0736\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 070, Loss: 1.7884\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 071, Loss: 1.3660\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 072, Loss: 0.9947\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 073, Loss: 0.9795\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 074, Loss: 1.0543\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 075, Loss: 1.0340\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 076, Loss: 0.5277\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 077, Loss: 1.2967\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 078, Loss: 0.8254\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 079, Loss: 0.7248\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([16])\n",
      "torch.Size([9])\n",
      "Epoch 080, Loss: 0.7053\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results():\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # 原始数据与真实信号：噪声显著干扰观测\n",
    "    axes[0, 0].plot(t, y_noisy, 'r.', alpha=0.6, label='Noisy Data')\n",
    "    axes[0, 0].plot(t, y_true, 'g-', label='True Signal', linewidth=2)\n",
    "    axes[0, 0].set_title('1. 原始数据：噪声掩盖真实趋势与周期')\n",
    "    \n",
    "    # 卡尔曼滤波结果：噪声被有效抑制\n",
    "    axes[0, 1].plot(t, y_kalman, 'b-', linewidth=2, label='Kalman Filtered')\n",
    "    axes[0, 1].set_title('2. 卡尔曼滤波：去除高频噪声，保留低频趋势')\n",
    "    \n",
    "    # Transformer预测结果：基于去噪数据的长程依赖建模\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_train).squeeze().numpy()\n",
    "    predict_t = t[15:]  # lookback=15，预测从第15步开始\n",
    "    axes[1, 0].plot(predict_t, y_pred, 'm-', label='Transformer Prediction')\n",
    "    axes[1, 0].set_title('3. Transformer预测：捕捉多尺度依赖（趋势+周期）')\n",
    "    \n",
    "    # 真实值与预测值对比：融合后误差显著降低\n",
    "    axes[1, 1].plot(predict_t, y_true[15:], 'g-', label='True Signal')\n",
    "    axes[1, 1].plot(predict_t, y_pred, 'm--', label='Prediction', alpha=0.8)\n",
    "    axes[1, 1].set_title('4. 融合效果：预测曲线紧密贴合真实信号')\n",
    "    \n",
    "    for ax in axes.flatten():\n",
    "        ax.legend()\n",
    "        ax.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    " \n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from Models.SeEANet import SeEANet\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/admin123/SATNet/Run/SAT/04-12-16:51:47.pth'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelName = \"SAT\"\n",
    "formatted_time = datetime.now().strftime(\"%m-%d-%H:%M:%S\")\n",
    "checkpoint_save_path = os.path.join(os.getcwd(), \"Run\", modelName,formatted_time + \".pth\")\n",
    "checkpoint_save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# 可视化卷积核\n",
    "from Models.SeEANet import SeEANet\n",
    "\n",
    "# 定义模型\n",
    "model = SeEANet()\n",
    "# 创建虚拟输入数据\n",
    "dummy_input = torch.randn(1, 6, 256)  # 根据你的模型输入形状调整\n",
    "\n",
    "# 前向传播以初始化模型参数\n",
    "model(dummy_input)\n",
    "\n",
    "# 现在可以访问卷积核了\n",
    "filters = model.conv1.weight.detach().numpy()\n",
    "\n",
    "# 可视化卷积核\n",
    "for i in range(filters.shape[0]):\n",
    "    # 确保卷积核是 2D 的\n",
    "    kernel = filters[i, 0]\n",
    "    if kernel.ndim == 1:\n",
    "        # 如果是 1D，可以将其转换为 2D\n",
    "        kernel = kernel.reshape(1, -1)\n",
    "    plt.imshow(kernel, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(f\"filter_{i}.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3])\n",
      "加权MSE损失: 2.647390127182007\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设输出形状为 (batch_size, 3)\n",
    "batch_size = 32\n",
    "outputs = torch.randn(batch_size, 3)  # 网络输出\n",
    "labels = torch.randn(batch_size, 3)   # 真实值\n",
    "\n",
    "# 定义权重\n",
    "weights = torch.tensor([0.5, 1.0, 2.0], dtype=torch.float32)  # 权重可以是任意正数\n",
    "\n",
    "# 计算加权的均方误差\n",
    "print((weights *(outputs - labels) ** 2).shape)\n",
    "mse_loss = torch.mean(weights * (outputs - labels) ** 2)\n",
    "\n",
    "print(f\"加权MSE损失: {mse_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 生成计算图\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m dot \u001b[38;5;241m=\u001b[39m make_dot(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_parameters()))\n\u001b[1;32m     13\u001b[0m dot\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_visualization\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 保存为 PDF 文件\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code-V5.0/Models/SeEANet.py:217\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdp3 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout1d(\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLazyLinear(PreLen\u001b[38;5;241m*\u001b[39mPreNum)\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    218\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mFirConv(x)\n\u001b[1;32m    219\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mInceptionBlk(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Code-V5.0/Models/SeEANet.py:71\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     70\u001b[0m     first_conv_outs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 71\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m     72\u001b[0m         x_input \u001b[38;5;241m=\u001b[39m x[:, i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Slice operation\u001b[39;00m\n\u001b[1;32m     73\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_conv_blocks[i](x_input)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "from Models.SeEANet import SeEANet\n",
    "\n",
    "# 定义模型\n",
    "model = SeEANet()\n",
    "\n",
    "# 创建虚拟输入\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "# 生成计算图\n",
    "dot = make_dot(model(x), params=dict(model.named_parameters()))\n",
    "dot.render(\"model_visualization\", format=\"pdf\")  # 保存为 PDF 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outpus's shape : torch.Size([24, 1])\n",
      "Type 'Tuple[int, int, int]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
      "Error occurs, No graph saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Type 'Tuple[int, int, int]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutpus\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms shape :\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m writer\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/tensorboardX/writer.py:940\u001b[0m, in \u001b[0;36mSummaryWriter.add_graph\u001b[0;34m(self, model, input_to_model, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add graph data to summary. The graph is actually processed by `torch.utils.tensorboard.add_graph()`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \n\u001b[1;32m    930\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m \u001b[38;5;124;03m        record your mutable container types (list, dict)\u001b[39;00m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytorch_graph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m graph\n\u001b[0;32m--> 940\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_graph(\u001b[43mgraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_to_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_strict_trace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py:332\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError occurs, No graph saved\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 332\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28mprint\u001b[39m(graph)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/utils/tensorboard/_pytorch_graph.py:326\u001b[0m, in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose, use_strict_trace)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _set_model_to_eval(model):\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 326\u001b[0m         trace \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_strict_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m         graph \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mgraph\n\u001b[1;32m    328\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_jit_pass_inline(graph)\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/jit/_trace.py:1000\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    987\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`optimize` is deprecated and has no effect. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `with torch.jit.optimized_execution()` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    991\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    992\u001b[0m     )\n\u001b[1;32m    994\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    995\u001b[0m     check_if_torch_exportable,\n\u001b[1;32m    996\u001b[0m     log_torch_jit_trace_exportability,\n\u001b[1;32m    997\u001b[0m     log_torchscript_usage,\n\u001b[1;32m    998\u001b[0m )\n\u001b[0;32m-> 1000\u001b[0m traced_func \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_compilation_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m log_torchscript_usage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_id\u001b[38;5;241m=\u001b[39m_get_model_id(traced_func))\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_if_torch_exportable():\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/jit/_trace.py:696\u001b[0m, in \u001b[0;36m_trace_impl\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_kwarg_inputs, _store_inputs)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_kwarg_inputs should be a dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrap_check_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_tolerance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_module_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs_is_kwarg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample_kwarg_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule)\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m ):\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pt/lib/python3.9/site-packages/torch/jit/_trace.py:1276\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit, example_inputs_is_kwarg, _store_inputs)\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1275\u001b[0m     example_inputs \u001b[38;5;241m=\u001b[39m make_tuple(example_inputs)\n\u001b[0;32m-> 1276\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_method_from_trace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvar_lookup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43margument_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_store_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1287\u001b[0m check_trace_method \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_c\u001b[38;5;241m.\u001b[39m_get_method(method_name)\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;66;03m# Check the trace against new traces created from user-specified inputs\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Type 'Tuple[int, int, int]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "model = SeEANet()\n",
    "model = model.cuda()\n",
    "inputs = torch.randn(24, 6, 256)\n",
    "inputs = inputs.cuda()\n",
    "# summary(model, input_size=(6, 256))\n",
    "outputs = model(inputs)\n",
    "print(\"outpus's shape :\", outputs.shape)\n",
    "\n",
    "writer.add_graph(model, input_to_model=(24, 6, 256))\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
